{"cpu_usage_count": [19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 19.191988249, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675, 27.281609675], "time_cpu_usage_count": [1711329838.0, 1711329839.0, 1711329840.0, 1711329841.0, 1711329842.0, 1711329843.0, 1711329844.0, 1711329845.0, 1711329846.0, 1711329847.0, 1711329848.0, 1711329849.0, 1711329850.0, 1711329851.0, 1711329852.0, 1711329853.0, 1711329854.0, 1711329855.0, 1711329856.0, 1711329857.0, 1711329858.0, 1711329859.0, 1711329860.0, 1711329861.0, 1711329862.0, 1711329863.0, 1711329864.0, 1711329865.0, 1711329866.0, 1711329867.0, 1711329868.0, 1711329869.0, 1711329870.0, 1711329871.0, 1711329872.0, 1711329873.0, 1711329874.0, 1711329875.0, 1711329876.0, 1711329877.0, 1711329878.0, 1711329879.0, 1711329880.0, 1711329881.0, 1711329882.0, 1711329883.0, 1711329884.0, 1711329885.0, 1711329886.0, 1711329887.0, 1711329888.0, 1711329889.0], "cpu_usage_rate": [0.18162786301649603, 0.1855933637155156, 0.1895588644145352, 0.19352436511355484, 0.1974898658125744, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333, 0.2696540475333333], "time_cpu_usage_rate": [1711329868.0, 1711329869.0, 1711329870.0, 1711329871.0, 1711329872.0, 1711329873.0, 1711329874.0, 1711329875.0, 1711329876.0, 1711329877.0, 1711329878.0, 1711329879.0, 1711329880.0, 1711329881.0, 1711329882.0, 1711329883.0, 1711329884.0, 1711329885.0, 1711329886.0, 1711329887.0, 1711329888.0, 1711329889.0], "cpu_throttled_count": [20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348, 20.26002348], "time_cpu_throttled_count": [1711329838.0, 1711329839.0, 1711329840.0, 1711329841.0, 1711329842.0, 1711329843.0, 1711329844.0, 1711329845.0, 1711329846.0, 1711329847.0, 1711329848.0, 1711329849.0, 1711329850.0, 1711329851.0, 1711329852.0, 1711329853.0, 1711329854.0, 1711329855.0, 1711329856.0, 1711329857.0, 1711329858.0, 1711329859.0, 1711329860.0, 1711329861.0, 1711329862.0, 1711329863.0, 1711329864.0, 1711329865.0, 1711329866.0, 1711329867.0, 1711329868.0, 1711329869.0, 1711329870.0, 1711329871.0, 1711329872.0, 1711329873.0, 1711329874.0, 1711329875.0, 1711329876.0, 1711329877.0, 1711329878.0, 1711329879.0, 1711329880.0, 1711329881.0, 1711329882.0, 1711329883.0, 1711329884.0, 1711329885.0, 1711329886.0, 1711329887.0, 1711329888.0, 1711329889.0], "cpu_throttled_rate": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "time_cpu_throttled_rate": [1711329868.0, 1711329869.0, 1711329870.0, 1711329871.0, 1711329872.0, 1711329873.0, 1711329874.0, 1711329875.0, 1711329876.0, 1711329877.0, 1711329878.0, 1711329879.0, 1711329880.0, 1711329881.0, 1711329882.0, 1711329883.0, 1711329884.0, 1711329885.0, 1711329886.0, 1711329887.0, 1711329888.0, 1711329889.0], "memory_usage": [2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2246316032.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0, 2449272832.0], "time_memory_usage": [1711329838.0, 1711329839.0, 1711329840.0, 1711329841.0, 1711329842.0, 1711329843.0, 1711329844.0, 1711329845.0, 1711329846.0, 1711329847.0, 1711329848.0, 1711329849.0, 1711329850.0, 1711329851.0, 1711329852.0, 1711329853.0, 1711329854.0, 1711329855.0, 1711329856.0, 1711329857.0, 1711329858.0, 1711329859.0, 1711329860.0, 1711329861.0, 1711329862.0, 1711329863.0, 1711329864.0, 1711329865.0, 1711329866.0, 1711329867.0, 1711329868.0, 1711329869.0, 1711329870.0, 1711329871.0, 1711329872.0, 1711329873.0, 1711329874.0, 1711329875.0, 1711329876.0, 1711329877.0, 1711329878.0, 1711329879.0, 1711329880.0, 1711329881.0, 1711329882.0, 1711329883.0, 1711329884.0, 1711329885.0, 1711329886.0, 1711329887.0, 1711329888.0, 1711329889.0], "throughput": [1.0415686274509803, 1.2376470588235293, 1.712929411764706, 1.9570470588235291, 1.8198529411764703, 2.497117647058823, 3.3884248366013066, 3.792117647058824, 4.020898395721924, 4.376372549019608, 4.653334841628959, 4.681705882352941, 4.975709803921569, 5.501511029411765, 5.794882352941177, 5.857797385620915, 6.15184520123839, 6.7213529411764705, 7.0149019607843135, 7.278005347593583, 7.3280971867007665, 7.622171568627452, 8.174369411764706, 8.210330316742082, 8.50441394335512, 9.040617647058824, 9.092588235294118, 9.583029411764707, 9.952409867172676, 10.216128676470587, 10.49497504456328, 10.834083044982698, 11.09790588235294, 11.091101307189541, 11.430322734499205, 12.009814241486067, 12.273746606334843, 12.32765588235294, 12.89170731707317, 13.15570588235294, 13.20996716826265, 13.773663101604278, 14.037717647058823, 14.09228388746803, 14.655669586983727, 14.934726715686274, 14.974605042016806, 15.268712941176469, 15.831741637831602, 16.095898190045247, 16.15103884572697, 16.44514814814815, 16.933256684491976, 17.033367647058824, 17.327477812177502, 17.651411764705884, 17.64734995014955, 17.881901960784315, 17.87805207328833, 17.874326375711576, 17.870718954248364, 17.86722426470588, 17.8638371040724, 17.86055258467023], "time_throughput": [1711329826.0, 1711329827.0, 1711329828.0, 1711329829.0, 1711329830.0, 1711329831.0, 1711329832.0, 1711329833.0, 1711329834.0, 1711329835.0, 1711329836.0, 1711329837.0, 1711329838.0, 1711329839.0, 1711329840.0, 1711329841.0, 1711329842.0, 1711329843.0, 1711329844.0, 1711329845.0, 1711329846.0, 1711329847.0, 1711329848.0, 1711329849.0, 1711329850.0, 1711329851.0, 1711329852.0, 1711329853.0, 1711329854.0, 1711329855.0, 1711329856.0, 1711329857.0, 1711329858.0, 1711329859.0, 1711329860.0, 1711329861.0, 1711329862.0, 1711329863.0, 1711329864.0, 1711329865.0, 1711329866.0, 1711329867.0, 1711329868.0, 1711329869.0, 1711329870.0, 1711329871.0, 1711329872.0, 1711329873.0, 1711329874.0, 1711329875.0, 1711329876.0, 1711329877.0, 1711329878.0, 1711329879.0, 1711329880.0, 1711329881.0, 1711329882.0, 1711329883.0, 1711329884.0, 1711329885.0, 1711329886.0, 1711329887.0, 1711329888.0, 1711329889.0], "responses": [[{"times": {"request": {"sending": 1711329820.4232395, "arrival": 1711329822.748641}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4051728, "arrival": 1711329822.7407064}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.420339, "arrival": 1711329822.7061865}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4057276, "arrival": 1711329822.7597158}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4051976, "arrival": 1711329821.111553}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.468699, "arrival": 1711329822.7649074}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4327314, "arrival": 1711329822.725371}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4050684, "arrival": 1711329822.739287}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4322786, "arrival": 1711329821.1353886}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4232755, "arrival": 1711329821.1341233}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432168, "arrival": 1711329821.1350973}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4050395, "arrival": 1711329822.7148635}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.43264, "arrival": 1711329822.7174737}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4203622, "arrival": 1711329822.6900158}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.405096, "arrival": 1711329821.1298409}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4323192, "arrival": 1711329822.7142725}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4324396, "arrival": 1711329822.6941645}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4049385, "arrival": 1711329822.6788995}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4056094, "arrival": 1711329821.1311567}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4324663, "arrival": 1711329822.7526202}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4056811, "arrival": 1711329822.7056644}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4202313, "arrival": 1711329822.7412906}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4326792, "arrival": 1711329822.7543175}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4050052, "arrival": 1711329822.6970928}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4229932, "arrival": 1711329822.7426836}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4326146, "arrival": 1711329822.7515118}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4322917, "arrival": 1711329822.7482252}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4329293, "arrival": 1711329822.6951714}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4331815, "arrival": 1711329821.1394925}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4057746, "arrival": 1711329821.114301}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4329667, "arrival": 1711329821.1259005}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432534, "arrival": 1711329822.7156408}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432916, "arrival": 1711329822.7182233}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4330306, "arrival": 1711329822.6954143}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4321513, "arrival": 1711329821.1182766}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4323049, "arrival": 1711329822.7235248}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4330437, "arrival": 1711329822.767992}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4331057, "arrival": 1711329822.7421188}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.405122, "arrival": 1711329821.0892215}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4324005, "arrival": 1711329822.7484155}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4056575, "arrival": 1711329822.7188714}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4052222, "arrival": 1711329821.130365}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4319303, "arrival": 1711329821.1344898}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.423111, "arrival": 1711329821.1158369}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4327445, "arrival": 1711329822.7180119}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4322119, "arrival": 1711329822.713977}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4319756, "arrival": 1711329822.713367}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4330688, "arrival": 1711329821.1262798}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4324532, "arrival": 1711329822.7657359}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4320192, "arrival": 1711329822.7490573}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4229472, "arrival": 1711329821.1152222}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4320612, "arrival": 1711329822.747797}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4323874, "arrival": 1711329821.1356773}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432507, "arrival": 1711329822.750053}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4053438, "arrival": 1711329822.7545466}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4672515, "arrival": 1711329822.6958766}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4054632, "arrival": 1711329822.7186594}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4329422, "arrival": 1711329822.767785}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.405513, "arrival": 1711329822.6803493}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4326925, "arrival": 1711329821.125008}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4326537, "arrival": 1711329822.6946836}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4203131, "arrival": 1711329822.720629}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4230535, "arrival": 1711329822.6913607}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4321988, "arrival": 1711329822.7228863}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.405271, "arrival": 1711329822.7177715}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4323742, "arrival": 1711329821.1214852}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.405751, "arrival": 1711329822.7438061}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4321842, "arrival": 1711329822.748021}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4328902, "arrival": 1711329822.7522285}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4053676, "arrival": 1711329822.741561}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.433132, "arrival": 1711329822.6956527}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4051476, "arrival": 1711329822.7541096}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4319613, "arrival": 1711329822.7219536}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4318783, "arrival": 1711329822.7608905}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4057047, "arrival": 1711329822.6807091}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4321337, "arrival": 1711329822.749268}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4055371, "arrival": 1711329822.7547448}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4329925, "arrival": 1711329822.7524214}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4325476, "arrival": 1711329822.6944299}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4327059, "arrival": 1711329821.138164}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4319456, "arrival": 1711329822.7474976}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4232025, "arrival": 1711329822.691604}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4054155, "arrival": 1711329821.1307774}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4053917, "arrival": 1711329821.112463}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4324265, "arrival": 1711329822.7145717}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.423034, "arrival": 1711329822.7119114}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4052947, "arrival": 1711329822.6973658}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4320047, "arrival": 1711329822.7644885}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4054394, "arrival": 1711329822.7401195}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4053192, "arrival": 1711329822.6799965}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4331195, "arrival": 1711329822.7211225}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4052463, "arrival": 1711329822.7397726}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4329793, "arrival": 1711329821.1389318}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4323335, "arrival": 1711329822.6939092}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4232936, "arrival": 1711329822.7432556}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4056334, "arrival": 1711329822.7404284}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4321184, "arrival": 1711329822.765128}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432758, "arrival": 1711329822.6949217}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432587, "arrival": 1711329821.1222548}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.423073, "arrival": 1711329822.7604768}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4330053, "arrival": 1711329822.7418358}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4323611, "arrival": 1711329822.751779}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4233298, "arrival": 1711329822.713051}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4055607, "arrival": 1711329822.7435205}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4672184, "arrival": 1711329822.7215502}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4318278, "arrival": 1711329822.6918638}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.405488, "arrival": 1711329822.6975768}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4330566, "arrival": 1711329822.7642322}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.433081, "arrival": 1711329821.1392312}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4319143, "arrival": 1711329821.1170306}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4323475, "arrival": 1711329822.7655308}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4671779, "arrival": 1711329822.7423787}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4319904, "arrival": 1711329822.692123}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4330938, "arrival": 1711329822.7600574}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4203835, "arrival": 1711329822.7602725}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4231846, "arrival": 1711329822.7127101}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4324932, "arrival": 1711329821.1359508}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4327185, "arrival": 1711329822.7519944}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432666, "arrival": 1711329822.767308}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4233117, "arrival": 1711329822.7217507}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4330182, "arrival": 1711329822.7184467}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4327714, "arrival": 1711329822.7675638}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4329538, "arrival": 1711329822.7621996}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4232213, "arrival": 1711329822.7606926}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432903, "arrival": 1711329822.7409906}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432047, "arrival": 1711329821.1348047}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4322383, "arrival": 1711329822.7653348}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4331446, "arrival": 1711329822.768201}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4229715, "arrival": 1711329821.1318045}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432089, "arrival": 1711329822.7136712}, "models": {"yolo": {"arrival": 1711329820.7499957, "serving": 1711329822.6335964}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4055848, "arrival": 1711329821.1133978}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.422893, "arrival": 1711329822.7446651}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4320753, "arrival": 1711329822.7226431}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4231665, "arrival": 1711329822.7213414}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4232578, "arrival": 1711329821.1164405}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4328632, "arrival": 1711329821.125508}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4322252, "arrival": 1711329822.6936555}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.433169, "arrival": 1711329821.1266673}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432601, "arrival": 1711329821.1377563}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4231477, "arrival": 1711329822.7429683}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4670622, "arrival": 1711329822.761738}, "models": {"yolo": {"arrival": 1711329820.8536704, "serving": 1711329822.6791444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432103, "arrival": 1711329822.6923854}, "models": {"yolo": {"arrival": 1711329820.582788, "serving": 1711329822.5862901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432033, "arrival": 1711329821.1176367}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4231296, "arrival": 1711329821.1320653}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4325202, "arrival": 1711329822.7244585}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4331572, "arrival": 1711329822.7647083}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4230142, "arrival": 1711329822.7209032}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432561, "arrival": 1711329822.7659419}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4324133, "arrival": 1711329822.7242181}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432849, "arrival": 1711329822.7619946}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.468651, "arrival": 1711329822.768395}, "models": {"yolo": {"arrival": 1711329820.7875504, "serving": 1711329822.665763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4230914, "arrival": 1711329822.7449973}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4318979, "arrival": 1711329822.7488582}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4328763, "arrival": 1711329821.138554}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432265, "arrival": 1711329821.1209414}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432252, "arrival": 1711329822.7494593}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4325738, "arrival": 1711329822.7538548}, "models": {"yolo": {"arrival": 1711329820.7758818, "serving": 1711329822.6694598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4057975, "arrival": 1711329821.1315339}, "models": {"yolo": {"arrival": 1711329820.5808518, "serving": 1711329821.0715544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.432627, "arrival": 1711329822.7251139}, "models": {"yolo": {"arrival": 1711329820.673017, "serving": 1711329822.642937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329820.4324799, "arrival": 1711329821.1218765}, "models": {"yolo": {"arrival": 1711329820.6406105, "serving": 1711329821.077564}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329821.444672, "arrival": 1711329825.4158125}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4113016, "arrival": 1711329825.367227}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4448311, "arrival": 1711329822.682748}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4452124, "arrival": 1711329825.3985684}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4111843, "arrival": 1711329822.6347506}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4715264, "arrival": 1711329825.4577153}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4109032, "arrival": 1711329825.3296785}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4107366, "arrival": 1711329825.3895216}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4117203, "arrival": 1711329825.3417096}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4108233, "arrival": 1711329825.366925}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4108644, "arrival": 1711329825.3322074}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4105709, "arrival": 1711329822.6372352}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4441805, "arrival": 1711329825.384155}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4115682, "arrival": 1711329825.3147445}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4446309, "arrival": 1711329822.6926453}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4445188, "arrival": 1711329822.691098}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4114153, "arrival": 1711329825.3410995}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.411378, "arrival": 1711329825.330633}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4111006, "arrival": 1711329825.340515}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4107819, "arrival": 1711329825.3106153}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4450781, "arrival": 1711329825.413929}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4300766, "arrival": 1711329825.3646305}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.430246, "arrival": 1711329825.4125173}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4293094, "arrival": 1711329822.644029}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4743333, "arrival": 1711329825.5954185}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4295514, "arrival": 1711329825.3637965}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4752896, "arrival": 1711329825.4613903}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4117966, "arrival": 1711329822.6777568}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444534, "arrival": 1711329825.459136}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4114544, "arrival": 1711329822.6420574}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4114923, "arrival": 1711329822.635302}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4450495, "arrival": 1711329825.4132147}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.445117, "arrival": 1711329825.227338}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444359, "arrival": 1711329825.3734531}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4445617, "arrival": 1711329825.4155898}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4112241, "arrival": 1711329825.390086}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4441452, "arrival": 1711329825.3955824}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4111445, "arrival": 1711329822.6380289}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4112635, "arrival": 1711329825.3135667}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4447777, "arrival": 1711329825.4167385}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4106836, "arrival": 1711329822.6332343}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4442465, "arrival": 1711329822.690581}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4451296, "arrival": 1711329825.195066}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444161, "arrival": 1711329825.4146736}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4451036, "arrival": 1711329825.3981502}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4447653, "arrival": 1711329825.4127495}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4113405, "arrival": 1711329825.337785}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4231787, "arrival": 1711329822.6437328}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444232, "arrival": 1711329822.677393}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4451845, "arrival": 1711329825.4141762}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.443979, "arrival": 1711329825.3650692}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4443293, "arrival": 1711329825.384773}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444752, "arrival": 1711329825.4622688}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4297364, "arrival": 1711329825.3662486}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444422, "arrival": 1711329825.4589112}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4299734, "arrival": 1711329825.4122782}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4300416, "arrival": 1711329825.38839}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.430145, "arrival": 1711329825.3611088}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.471509, "arrival": 1711329825.4144368}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4713585, "arrival": 1711329825.1962605}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4442608, "arrival": 1711329825.4586883}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4444354, "arrival": 1711329825.396383}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444686, "arrival": 1711329825.3866303}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4302802, "arrival": 1711329825.3834977}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4449935, "arrival": 1711329825.3977435}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4450912, "arrival": 1711329825.411474}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444462, "arrival": 1711329825.3853807}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4445753, "arrival": 1711329825.3859777}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.429614, "arrival": 1711329825.3601987}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4115303, "arrival": 1711329825.3946393}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4444065, "arrival": 1711329822.6908517}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.429871, "arrival": 1711329825.360654}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444872, "arrival": 1711329825.4129705}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4116812, "arrival": 1711329825.3314416}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4296443, "arrival": 1711329822.6756327}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4445477, "arrival": 1711329825.3967755}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4230032, "arrival": 1711329825.3398879}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4294167, "arrival": 1711329822.6785188}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4300075, "arrival": 1711329825.366618}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4298027, "arrival": 1711329825.364224}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4116428, "arrival": 1711329825.3389807}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4451985, "arrival": 1711329825.4183805}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4743555, "arrival": 1711329825.4595976}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4451442, "arrival": 1711329825.4658582}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.471473, "arrival": 1711329825.45745}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4753575, "arrival": 1711329825.4579518}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4742637, "arrival": 1711329825.2170827}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4449127, "arrival": 1711329825.411225}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4715657, "arrival": 1711329825.2285464}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.471492, "arrival": 1711329825.4176404}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4117591, "arrival": 1711329822.6433783}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.429705, "arrival": 1711329825.4120288}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.423348, "arrival": 1711329825.362453}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4446173, "arrival": 1711329822.6814277}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4448848, "arrival": 1711329825.4169726}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.423374, "arrival": 1711329825.3588161}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4301112, "arrival": 1711329825.371442}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4443119, "arrival": 1711329825.4151335}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4441986, "arrival": 1711329825.3727467}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444375, "arrival": 1711329825.3716736}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4447258, "arrival": 1711329822.68233}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4448166, "arrival": 1711329825.3971643}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.42332, "arrival": 1711329825.3595734}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4116058, "arrival": 1711329825.3686013}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4447908, "arrival": 1711329825.4106946}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444589, "arrival": 1711329825.400341}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4295192, "arrival": 1711329825.3872206}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4450212, "arrival": 1711329822.6934018}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4445043, "arrival": 1711329822.681063}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444859, "arrival": 1711329825.4624975}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4118721, "arrival": 1711329825.315402}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4714468, "arrival": 1711329825.46661}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4232368, "arrival": 1711329825.4100666}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.445242, "arrival": 1711329825.2279441}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4753747, "arrival": 1711329825.414902}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4446578, "arrival": 1711329825.410386}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4450345, "arrival": 1711329825.4627209}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4451704, "arrival": 1711329825.4174118}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.429837, "arrival": 1711329825.3633437}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4294877, "arrival": 1711329825.3658643}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4450645, "arrival": 1711329825.4171956}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4447126, "arrival": 1711329825.3723376}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4440982, "arrival": 1711329822.6770198}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.41191, "arrival": 1711329825.3689606}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4444895, "arrival": 1711329825.371905}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.411835, "arrival": 1711329825.395175}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4303129, "arrival": 1711329825.3889453}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4444761, "arrival": 1711329825.373733}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444129, "arrival": 1711329825.4570193}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444699, "arrival": 1711329825.4095283}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.423109, "arrival": 1711329825.362029}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.445157, "arrival": 1711329825.4134626}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4442155, "arrival": 1711329825.3711352}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4294548, "arrival": 1711329825.4117968}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4446447, "arrival": 1711329825.4593642}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4441137, "arrival": 1711329822.689587}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.429674, "arrival": 1711329822.6796372}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.423265, "arrival": 1711329825.3654726}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4753375, "arrival": 1711329825.4605153}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.429941, "arrival": 1711329822.6818385}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.423292, "arrival": 1711329825.382633}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4297667, "arrival": 1711329825.3878157}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4448457, "arrival": 1711329822.6931617}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4440813, "arrival": 1711329825.361601}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4302113, "arrival": 1711329822.6831057}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4295824, "arrival": 1711329825.3628876}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4448986, "arrival": 1711329825.4136913}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4715474, "arrival": 1711329825.3991323}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4442942, "arrival": 1711329825.395994}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4232085, "arrival": 1711329822.6781175}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4444485, "arrival": 1711329825.4153576}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.430177, "arrival": 1711329822.6766384}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444804, "arrival": 1711329825.4109492}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4450078, "arrival": 1711329822.6902962}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4299064, "arrival": 1711329822.676228}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444391, "arrival": 1711329822.6792731}, "models": {"yolo": {"arrival": 1711329821.5502741, "serving": 1711329822.6255796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4231472, "arrival": 1711329825.3423285}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.444739, "arrival": 1711329822.6929002}, "models": {"yolo": {"arrival": 1711329821.5503573, "serving": 1711329822.6212974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4440596, "arrival": 1711329825.3725467}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329821.4446023, "arrival": 1711329825.3721259}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329822.4515302, "arrival": 1711329825.6171796}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4511583, "arrival": 1711329825.6115267}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4103286, "arrival": 1711329825.4162784}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4111435, "arrival": 1711329825.6008675}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4386768, "arrival": 1711329825.5939767}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4513342, "arrival": 1711329825.253583}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4515588, "arrival": 1711329825.6148396}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.409614, "arrival": 1711329825.22894}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.451074, "arrival": 1711329825.605789}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.450854, "arrival": 1711329825.599281}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4512947, "arrival": 1711329825.6062322}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4516606, "arrival": 1711329825.6151984}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4512184, "arrival": 1711329825.2533002}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4504988, "arrival": 1711329825.59979}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4343152, "arrival": 1711329825.4668174}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.451086, "arrival": 1711329825.5917382}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4511228, "arrival": 1711329825.248714}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4515169, "arrival": 1711329825.616456}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4386168, "arrival": 1711329825.226729}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4097247, "arrival": 1711329825.2179904}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4509127, "arrival": 1711329825.2413473}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4508662, "arrival": 1711329825.6068587}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4509273, "arrival": 1711329825.6135514}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4103646, "arrival": 1711329825.2335062}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4490047, "arrival": 1711329825.235168}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4383001, "arrival": 1711329825.6075993}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4386308, "arrival": 1711329825.601742}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.438646, "arrival": 1711329825.5915082}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4510067, "arrival": 1711329825.2503736}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4505332, "arrival": 1711329825.5911689}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4108875, "arrival": 1711329825.5928435}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4504802, "arrival": 1711329825.5986197}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4513073, "arrival": 1711329825.6104598}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4511106, "arrival": 1711329825.2505941}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4097881, "arrival": 1711329825.5956519}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4514253, "arrival": 1711329825.6045542}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.410026, "arrival": 1711329825.4160395}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4101758, "arrival": 1711329825.4600313}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4106596, "arrival": 1711329825.466381}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.451031, "arrival": 1711329825.6157098}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4489045, "arrival": 1711329825.4893954}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4385204, "arrival": 1711329825.593531}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4515846, "arrival": 1711329825.6169658}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.410962, "arrival": 1711329825.4883647}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4098961, "arrival": 1711329825.4616082}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4099495, "arrival": 1711329825.4607399}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4098456, "arrival": 1711329825.459808}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4505594, "arrival": 1711329825.2410834}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4509573, "arrival": 1711329825.609864}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4104671, "arrival": 1711329825.5962467}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4099889, "arrival": 1711329825.4581838}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.410063, "arrival": 1711329825.232521}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4505851, "arrival": 1711329825.6128943}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4343524, "arrival": 1711329825.2254717}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4508295, "arrival": 1711329825.609563}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4387336, "arrival": 1711329825.2330976}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.450619, "arrival": 1711329825.6000078}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4513843, "arrival": 1711329825.612062}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4511342, "arrival": 1711329825.6159291}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4100997, "arrival": 1711329825.2186947}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4105175, "arrival": 1711329825.4602518}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4386003, "arrival": 1711329825.2402596}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4342752, "arrival": 1711329825.6073673}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4385042, "arrival": 1711329825.6013825}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4383366, "arrival": 1711329825.4670248}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4512794, "arrival": 1711329825.611759}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4512336, "arrival": 1711329825.2492242}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4104195, "arrival": 1711329825.219332}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.450778, "arrival": 1711329825.5967321}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4108434, "arrival": 1711329825.5965054}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4385529, "arrival": 1711329825.5937526}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4101384, "arrival": 1711329825.5959313}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.451413, "arrival": 1711329825.6140065}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.450649, "arrival": 1711329825.6089742}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4382763, "arrival": 1711329825.5944982}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.438801, "arrival": 1711329825.590315}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.45044, "arrival": 1711329825.6126666}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4109983, "arrival": 1711329825.5076938}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4512646, "arrival": 1711329825.6055105}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4507084, "arrival": 1711329825.6131191}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4107518, "arrival": 1711329825.2339082}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4507902, "arrival": 1711329825.2499385}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4510183, "arrival": 1711329825.2416108}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4515426, "arrival": 1711329825.612336}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.438202, "arrival": 1711329825.5933056}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4508142, "arrival": 1711329825.6133356}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4506028, "arrival": 1711329825.598845}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4511466, "arrival": 1711329825.6052678}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4102132, "arrival": 1711329825.461827}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4509432, "arrival": 1711329825.6047938}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4102917, "arrival": 1711329825.45845}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4386911, "arrival": 1711329825.5976377}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4102552, "arrival": 1711329825.460956}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4506936, "arrival": 1711329825.236017}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.450725, "arrival": 1711329825.5990667}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4106138, "arrival": 1711329825.4611714}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4342482, "arrival": 1711329825.5942636}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4110699, "arrival": 1711329825.234308}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4510982, "arrival": 1711329825.6022227}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4105635, "arrival": 1711329825.4620483}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4385831, "arrival": 1711329825.4672327}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4388175, "arrival": 1711329825.597865}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4513466, "arrival": 1711329825.249496}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4508822, "arrival": 1711329825.597017}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.450679, "arrival": 1711329825.2497208}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4515047, "arrival": 1711329825.2508132}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4107978, "arrival": 1711329825.2199385}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4516232, "arrival": 1711329825.6161458}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4343696, "arrival": 1711329825.601159}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4507535, "arrival": 1711329825.6086867}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4516745, "arrival": 1711329825.6145494}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4489818, "arrival": 1711329825.2408223}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4343348, "arrival": 1711329825.2347548}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4383729, "arrival": 1711329825.2260957}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4342957, "arrival": 1711329825.591966}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.411034, "arrival": 1711329825.466165}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4505138, "arrival": 1711329825.6080706}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4386597, "arrival": 1711329825.5949473}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4516354, "arrival": 1711329825.613767}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4512033, "arrival": 1711329825.6039333}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4509704, "arrival": 1711329825.5995638}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4107058, "arrival": 1711329825.4165177}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.450634, "arrival": 1711329825.6083868}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4516473, "arrival": 1711329825.615483}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4109254, "arrival": 1711329825.4888885}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4513586, "arrival": 1711329825.616672}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4387052, "arrival": 1711329825.4674354}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.450982, "arrival": 1711329825.6071537}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4509945, "arrival": 1711329825.5973475}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4516096, "arrival": 1711329825.2510252}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4515715, "arrival": 1711329825.6142256}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.438751, "arrival": 1711329825.6019993}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4387193, "arrival": 1711329825.2405534}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4508975, "arrival": 1711329825.2501516}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.451597, "arrival": 1711329825.254042}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4111066, "arrival": 1711329825.2246559}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4514918, "arrival": 1711329825.2538064}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4513702, "arrival": 1711329825.6100872}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4513988, "arrival": 1711329825.6065621}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4508421, "arrival": 1711329825.6005726}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4513218, "arrival": 1711329825.6042638}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4510574, "arrival": 1711329825.611293}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4511878, "arrival": 1711329825.609284}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4387822, "arrival": 1711329825.5951953}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4507658, "arrival": 1711329825.598078}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4511724, "arrival": 1711329825.6060066}, "models": {"yolo": {"arrival": 1711329822.987353, "serving": 1711329825.3019772}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4506645, "arrival": 1711329825.607843}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4385347, "arrival": 1711329825.5947227}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4507406, "arrival": 1711329825.6002908}, "models": {"yolo": {"arrival": 1711329823.042905, "serving": 1711329825.3123}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.451043, "arrival": 1711329825.6050246}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4385674, "arrival": 1711329825.5925817}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4383209, "arrival": 1711329825.592345}, "models": {"yolo": {"arrival": 1711329822.994655, "serving": 1711329825.298604}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4383512, "arrival": 1711329825.239977}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4505725, "arrival": 1711329825.235582}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4505467, "arrival": 1711329825.5909026}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4341655, "arrival": 1711329825.5930748}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.450802, "arrival": 1711329825.2395756}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4512494, "arrival": 1711329825.6107538}, "models": {"yolo": {"arrival": 1711329823.0816243, "serving": 1711329825.323726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.4387674, "arrival": 1711329825.598362}, "models": {"yolo": {"arrival": 1711329822.9323804, "serving": 1711329825.292586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329822.480171, "arrival": 1711329825.610987}, "models": {"yolo": {"arrival": 1711329823.092615, "serving": 1711329825.3013892}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329823.4305236, "arrival": 1711329828.132274}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4306862, "arrival": 1711329828.0492725}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.430219, "arrival": 1711329828.0904145}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4301293, "arrival": 1711329828.045431}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4315236, "arrival": 1711329828.0538635}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4125977, "arrival": 1711329828.0070472}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.425373, "arrival": 1711329828.1133003}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4236834, "arrival": 1711329828.0883827}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4312706, "arrival": 1711329828.1149328}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4126933, "arrival": 1711329828.0685258}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4312496, "arrival": 1711329828.2162762}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.412631, "arrival": 1711329828.0924916}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.431479, "arrival": 1711329828.21427}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4123359, "arrival": 1711329825.2542572}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4301507, "arrival": 1711329828.1306725}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.431225, "arrival": 1711329828.3007262}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.430458, "arrival": 1711329828.135408}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4125552, "arrival": 1711329825.2512336}, "models": {"yolo": {"arrival": 1711329823.4273293, "serving": 1711329825.1817446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4250588, "arrival": 1711329828.094044}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4566593, "arrival": 1711329828.2148077}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4252598, "arrival": 1711329828.0942743}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4130857, "arrival": 1711329828.0243223}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4313767, "arrival": 1711329828.213305}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4253263, "arrival": 1711329828.048189}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4313567, "arrival": 1711329828.053302}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.42991, "arrival": 1711329828.1161227}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4306607, "arrival": 1711329828.1991205}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4127843, "arrival": 1711329825.2544727}, "models": {"yolo": {"arrival": 1711329823.4419532, "serving": 1711329825.188991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4601524, "arrival": 1711329828.197432}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4306114, "arrival": 1711329828.2999601}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.425144, "arrival": 1711329828.0476902}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4128728, "arrival": 1711329828.0928392}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4313967, "arrival": 1711329828.3009636}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.425236, "arrival": 1711329828.0313904}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.41296, "arrival": 1711329828.0278502}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4299953, "arrival": 1711329828.195789}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4126625, "arrival": 1711329828.0970175}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4303038, "arrival": 1711329828.0460231}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4128141, "arrival": 1711329828.0911005}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4128435, "arrival": 1711329828.0082119}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.412902, "arrival": 1711329828.0972886}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4255385, "arrival": 1711329828.0486479}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4298155, "arrival": 1711329828.1968772}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.431163, "arrival": 1711329828.200023}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4315436, "arrival": 1711329828.213662}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.425091, "arrival": 1711329828.1693897}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4298387, "arrival": 1711329828.1181166}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4255164, "arrival": 1711329828.1176043}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4301078, "arrival": 1711329828.1744125}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.425396, "arrival": 1711329828.118339}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4307117, "arrival": 1711329828.1326265}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4251888, "arrival": 1711329828.0895538}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4133193, "arrival": 1711329828.0256639}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4302611, "arrival": 1711329828.13158}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4311411, "arrival": 1711329828.1377473}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4600523, "arrival": 1711329828.2221096}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4254675, "arrival": 1711329828.0945349}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4127536, "arrival": 1711329828.134027}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4127238, "arrival": 1711329828.0268338}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4308674, "arrival": 1711329828.1995165}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4300394, "arrival": 1711329828.090154}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4311843, "arrival": 1711329828.0529177}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4131718, "arrival": 1711329828.0880296}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.413024, "arrival": 1711329828.0886436}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4307604, "arrival": 1711329828.2014835}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.430283, "arrival": 1711329828.1982453}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4252138, "arrival": 1711329828.117024}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4304137, "arrival": 1711329828.0907497}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4295952, "arrival": 1711329828.196606}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4310288, "arrival": 1711329828.194948}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4315863, "arrival": 1711329828.2168589}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4308207, "arrival": 1711329828.3002188}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4566965, "arrival": 1711329828.095233}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4300175, "arrival": 1711329828.1778543}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.431205, "arrival": 1711329828.2128735}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.425604, "arrival": 1711329828.118592}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4130573, "arrival": 1711329828.0914438}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4315028, "arrival": 1711329828.2186458}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.41326, "arrival": 1711329828.088919}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.455205, "arrival": 1711329828.3088622}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4297702, "arrival": 1711329828.0439546}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4305701, "arrival": 1711329828.201151}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4131427, "arrival": 1711329828.0975115}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4303458, "arrival": 1711329828.217567}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4237926, "arrival": 1711329828.0921967}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4565005, "arrival": 1711329828.19714}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.412931, "arrival": 1711329828.087181}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4564848, "arrival": 1711329828.217099}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4253478, "arrival": 1711329828.2152958}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4296565, "arrival": 1711329828.0876408}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4131138, "arrival": 1711329828.0935209}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4256284, "arrival": 1711329828.0364265}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4255824, "arrival": 1711329828.1155152}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4254942, "arrival": 1711329828.1963444}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.430632, "arrival": 1711329828.1370096}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.431607, "arrival": 1711329828.1789174}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4307353, "arrival": 1711329828.2180653}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4129891, "arrival": 1711329828.1343565}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4297924, "arrival": 1711329828.0950086}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.430086, "arrival": 1711329828.116416}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4564679, "arrival": 1711329828.3014576}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4132895, "arrival": 1711329828.091784}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.429931, "arrival": 1711329828.119143}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4132295, "arrival": 1711329828.1346774}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4296312, "arrival": 1711329828.1178792}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4305491, "arrival": 1711329828.217806}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4552896, "arrival": 1711329828.214549}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4252815, "arrival": 1711329828.1960783}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4249973, "arrival": 1711329828.0301354}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4237406, "arrival": 1711329828.1350982}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4311204, "arrival": 1711329828.3004873}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4302402, "arrival": 1711329828.2994568}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.423715, "arrival": 1711329828.0470426}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4307823, "arrival": 1711329828.1143515}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.455311, "arrival": 1711329828.219006}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4310784, "arrival": 1711329828.201817}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4314392, "arrival": 1711329828.1152377}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4255612, "arrival": 1711329828.215535}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.431291, "arrival": 1711329828.3082907}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4297442, "arrival": 1711329828.1189098}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4314592, "arrival": 1711329828.3085048}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4565182, "arrival": 1711329828.3091116}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4301758, "arrival": 1711329828.2173316}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4601111, "arrival": 1711329828.3016927}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4305913, "arrival": 1711329828.1140687}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4310515, "arrival": 1711329828.2590392}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4132006, "arrival": 1711329828.028874}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4308882, "arrival": 1711329828.0522907}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4304993, "arrival": 1711329828.0465565}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.430065, "arrival": 1711329828.2512197}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.456399, "arrival": 1711329828.093157}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4314184, "arrival": 1711329828.216607}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.429681, "arrival": 1711329828.215786}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4564445, "arrival": 1711329828.2139275}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4303713, "arrival": 1711329828.200778}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4601324, "arrival": 1711329828.3030865}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4297047, "arrival": 1711329828.115818}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4301977, "arrival": 1711329828.2003732}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4299736, "arrival": 1711329828.1193697}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4235895, "arrival": 1711329828.1137912}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4566803, "arrival": 1711329828.2192347}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.430325, "arrival": 1711329828.1319287}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.413349, "arrival": 1711329828.0938044}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4237697, "arrival": 1711329828.089253}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4298868, "arrival": 1711329828.216021}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4315636, "arrival": 1711329828.301221}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4304368, "arrival": 1711329828.2997077}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4601727, "arrival": 1711329828.3093307}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.431336, "arrival": 1711329828.2184134}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.430846, "arrival": 1711329828.137337}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4299521, "arrival": 1711329828.0446742}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.425167, "arrival": 1711329828.1709542}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.429528, "arrival": 1711329828.094774}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4310994, "arrival": 1711329828.1146605}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4313152, "arrival": 1711329828.1954856}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4251158, "arrival": 1711329828.1167223}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4253047, "arrival": 1711329828.1173146}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4304783, "arrival": 1711329828.1987274}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4254405, "arrival": 1711329828.0355673}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329823.4298604, "arrival": 1711329828.0898821}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329824.4142458, "arrival": 1711329828.2150495}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4556875, "arrival": 1711329828.3506114}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.415512, "arrival": 1711329828.302383}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.416152, "arrival": 1711329828.302859}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4554102, "arrival": 1711329828.3474815}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.469389, "arrival": 1711329828.3648865}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4689438, "arrival": 1711329828.3508327}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4152212, "arrival": 1711329828.3037646}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4698498, "arrival": 1711329828.3606486}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.414512, "arrival": 1711329828.298203}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4552014, "arrival": 1711329828.3055575}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.455636, "arrival": 1711329828.3481846}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4702456, "arrival": 1711329828.363597}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4143791, "arrival": 1711329828.304211}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4554417, "arrival": 1711329828.350123}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4168558, "arrival": 1711329828.173461}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4148362, "arrival": 1711329828.2986834}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.415067, "arrival": 1711329828.304659}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4168994, "arrival": 1711329828.339647}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4699209, "arrival": 1711329828.3513114}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.416263, "arrival": 1711329828.3108618}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.415755, "arrival": 1711329828.2206197}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4697332, "arrival": 1711329828.351089}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4160714, "arrival": 1711329828.2208533}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.416423, "arrival": 1711329828.355856}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4145935, "arrival": 1711329828.3033073}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.415798, "arrival": 1711329828.3467476}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4151473, "arrival": 1711329828.2989542}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.469003, "arrival": 1711329828.3625019}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4152658, "arrival": 1711329828.2203984}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4701846, "arrival": 1711329828.360871}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4165397, "arrival": 1711329828.1337674}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4694788, "arrival": 1711329828.3542566}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.414761, "arrival": 1711329828.3044357}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.416114, "arrival": 1711329828.3556304}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4556491, "arrival": 1711329828.2519763}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4158723, "arrival": 1711329828.3050985}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4149485, "arrival": 1711329828.1979816}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.455355, "arrival": 1711329828.3615654}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4689176, "arrival": 1711329828.3586426}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.414456, "arrival": 1711329828.0954669}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.41619, "arrival": 1711329828.3095655}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4555762, "arrival": 1711329828.3503444}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4160345, "arrival": 1711329828.3418417}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4553826, "arrival": 1711329828.3477058}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4688368, "arrival": 1711329828.3496811}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4555328, "arrival": 1711329828.3492236}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4698725, "arrival": 1711329828.3597767}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4145544, "arrival": 1711329828.3074067}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4163003, "arrival": 1711329828.3450534}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4552934, "arrival": 1711329828.3488712}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4147992, "arrival": 1711329828.0957}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4165027, "arrival": 1711329828.3097906}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4156308, "arrival": 1711329828.2992074}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4690275, "arrival": 1711329828.352416}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.414677, "arrival": 1711329828.3459713}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4694567, "arrival": 1711329828.3470378}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.455276, "arrival": 1711329828.2196915}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4164662, "arrival": 1711329828.3403926}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4156687, "arrival": 1711329828.3080757}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4691508, "arrival": 1711329828.3565843}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4698973, "arrival": 1711329828.3633842}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4155922, "arrival": 1711329828.132884}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4148738, "arrival": 1711329828.3076391}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.414632, "arrival": 1711329828.197719}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4691277, "arrival": 1711329828.3588593}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4151838, "arrival": 1711329828.3078568}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.414991, "arrival": 1711329828.3462675}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4554563, "arrival": 1711329828.3060117}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4525363, "arrival": 1711329828.219465}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.470268, "arrival": 1711329828.3515341}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4151096, "arrival": 1711329828.1312613}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4157176, "arrival": 1711329828.3039892}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4149115, "arrival": 1711329828.3035448}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4557445, "arrival": 1711329828.3484187}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4163375, "arrival": 1711329828.3421013}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4696424, "arrival": 1711329828.3071814}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4158356, "arrival": 1711329828.3026338}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4159532, "arrival": 1711329828.3106542}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4697108, "arrival": 1711329828.3572729}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4163744, "arrival": 1711329828.221071}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4159918, "arrival": 1711329828.344759}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4692717, "arrival": 1711329828.30695}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4155529, "arrival": 1711329828.3048794}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4698048, "arrival": 1711329828.3554115}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4159102, "arrival": 1711329828.133227}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4695258, "arrival": 1711329828.3570263}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4875777, "arrival": 1711329828.3660047}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4553385, "arrival": 1711329828.305784}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4147213, "arrival": 1711329828.3019216}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4162266, "arrival": 1711329828.1335006}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4153166, "arrival": 1711329828.3465192}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4150293, "arrival": 1711329828.3021574}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4555058, "arrival": 1711329828.3479652}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4556744, "arrival": 1711329828.3535295}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4692948, "arrival": 1711329828.3540328}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.416656, "arrival": 1711329828.3423948}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4556997, "arrival": 1711329828.3431816}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4694116, "arrival": 1711329828.3574905}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4165814, "arrival": 1711329828.31107}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4696648, "arrival": 1711329828.354508}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4553242, "arrival": 1711329828.3499005}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4555497, "arrival": 1711329828.3533065}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.469758, "arrival": 1711329828.365338}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4698267, "arrival": 1711329828.3441856}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4693642, "arrival": 1711329828.343934}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.452567, "arrival": 1711329828.3457394}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4557574, "arrival": 1711329828.3064842}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4169736, "arrival": 1711329828.3426452}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4167767, "arrival": 1711329828.3406446}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4690557, "arrival": 1711329828.3486488}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.455396, "arrival": 1711329828.219923}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4550529, "arrival": 1711329828.3429394}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4168184, "arrival": 1711329828.3099964}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4555914, "arrival": 1711329828.306231}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4693174, "arrival": 1711329828.3590815}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4699683, "arrival": 1711329828.3581886}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4552383, "arrival": 1711329828.341178}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4697826, "arrival": 1711329828.3579478}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4166937, "arrival": 1711329828.22252}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.452553, "arrival": 1711329828.3401294}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4696198, "arrival": 1711329828.355197}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4525192, "arrival": 1711329828.3102076}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.416619, "arrival": 1711329828.345276}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4552224, "arrival": 1711329828.3584049}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4699907, "arrival": 1711329828.3599885}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4167352, "arrival": 1711329828.356075}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4691732, "arrival": 1711329828.343647}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.455717, "arrival": 1711329828.3622308}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4695969, "arrival": 1711329828.3577077}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4556065, "arrival": 1711329828.3620126}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.469687, "arrival": 1711329828.3595576}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4525023, "arrival": 1711329828.3409467}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4523945, "arrival": 1711329828.3053381}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4555187, "arrival": 1711329828.2201605}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4556615, "arrival": 1711329828.3494585}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4700127, "arrival": 1711329828.344435}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4557314, "arrival": 1711329828.3521957}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4690812, "arrival": 1711329828.3067193}, "models": {"yolo": {"arrival": 1711329825.3997853, "serving": 1711329827.9842882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4554913, "arrival": 1711329828.3517547}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4554722, "arrival": 1711329828.3617914}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4699461, "arrival": 1711329828.365778}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4695745, "arrival": 1711329828.3651304}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4554265, "arrival": 1711329828.3530843}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.452482, "arrival": 1711329828.3562953}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.469503, "arrival": 1711329828.3593147}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4691038, "arrival": 1711329828.353809}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4552586, "arrival": 1711329828.310435}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4556203, "arrival": 1711329828.351977}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.469223, "arrival": 1711329828.352636}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4553688, "arrival": 1711329828.3415096}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4169362, "arrival": 1711329828.3455026}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4692001, "arrival": 1711329828.364678}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4695477, "arrival": 1711329828.347264}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4689713, "arrival": 1711329828.343406}, "models": {"yolo": {"arrival": 1711329825.3711216, "serving": 1711329827.985945}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4692485, "arrival": 1711329828.354742}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4702058, "arrival": 1711329828.3640215}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4693396, "arrival": 1711329828.35681}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.4553082, "arrival": 1711329828.352859}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329824.469434, "arrival": 1711329828.3549826}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329825.4714873, "arrival": 1711329830.8654876}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.379959006+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329825.4296844}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.4711978, "arrival": 1711329830.8661382}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4300559, "arrival": 1711329830.842923}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4215314, "arrival": 1711329828.3662167}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4298563, "arrival": 1711329831.0100417}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.421558, "arrival": 1711329828.3604205}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429999, "arrival": 1711329830.8175154}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4304988, "arrival": 1711329830.799643}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380463997+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329825.4714024}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.379736912+00:00\"}\"\n>", "times": {"request": {"sending": 1711329825.429359}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.4300702, "arrival": 1711329830.8643103}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4294024, "arrival": 1711329830.8279722}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4302018, "arrival": 1711329831.0149875}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4655294, "arrival": 1711329830.849529}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.471331, "arrival": 1711329830.869208}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4289892, "arrival": 1711329828.3655577}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4294286, "arrival": 1711329830.8030157}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.37986671+00:00\"}\"\n>", "times": {"request": {"sending": 1711329825.4294682}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.4296567, "arrival": 1711329830.8168743}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4213815, "arrival": 1711329828.3627286}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4282053, "arrival": 1711329828.3631666}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4292753, "arrival": 1711329830.8051565}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4214416, "arrival": 1711329828.3602054}, "models": {"yolo": {"arrival": 1711329825.4860497, "serving": 1711329828.0146124}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380422313+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329825.471288}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.4709597, "arrival": 1711329830.86857}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4295378, "arrival": 1711329830.8035138}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380214746+00:00\"}\"\n>", "times": {"request": {"sending": 1711329825.465495}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.4292316, "arrival": 1711329828.3666604}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.379991433+00:00\"}\"\n>", "times": {"request": {"sending": 1711329825.4297984}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.471345, "arrival": 1711329831.0485682}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.43013, "arrival": 1711329830.8066096}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4299402, "arrival": 1711329830.8425581}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.42897, "arrival": 1711329830.992756}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.471273, "arrival": 1711329830.8436534}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4655614, "arrival": 1711329831.0276663}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.421584, "arrival": 1711329828.3613112}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4296288, "arrival": 1711329831.0057738}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429827, "arrival": 1711329830.8339152}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4298418, "arrival": 1711329830.845077}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4305127, "arrival": 1711329830.8493035}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4295902, "arrival": 1711329830.7362175}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4214876, "arrival": 1711329828.364237}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4713018, "arrival": 1711329830.8179297}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4215972, "arrival": 1711329828.3644497}, "models": {"yolo": {"arrival": 1711329825.4950955, "serving": 1711329828.0181353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.471432, "arrival": 1711329830.866587}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4304242, "arrival": 1711329831.0156019}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.471228, "arrival": 1711329831.0290644}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4293885, "arrival": 1711329830.8056877}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.470894, "arrival": 1711329830.8498352}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4215147, "arrival": 1711329830.739725}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4297118, "arrival": 1711329830.8336668}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4305267, "arrival": 1711329830.8650508}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4215014, "arrival": 1711329828.3638077}, "models": {"yolo": {"arrival": 1711329825.4598696, "serving": 1711329828.0086546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4712136, "arrival": 1711329830.868991}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.42967, "arrival": 1711329830.7746735}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4303315, "arrival": 1711329830.8069503}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380175468+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329825.430485}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.4301033, "arrival": 1711329830.8288016}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4297671, "arrival": 1711329830.81709}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429616, "arrival": 1711329830.8446324}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4295514, "arrival": 1711329830.8147466}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4856555, "arrival": 1711329830.844112}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4293745, "arrival": 1711329830.7348845}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4299545, "arrival": 1711329830.8532894}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4655128, "arrival": 1711329830.8001044}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429442, "arrival": 1711329830.80187}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4303071, "arrival": 1711329830.845425}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4292598, "arrival": 1711329830.7339203}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4214587, "arrival": 1711329830.8158073}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4297252, "arrival": 1711329830.8448515}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380259917+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329825.4656248}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.4300134, "arrival": 1711329830.776299}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4290063, "arrival": 1711329830.8011823}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4710147, "arrival": 1711329830.851266}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.421474, "arrival": 1711329828.3610892}, "models": {"yolo": {"arrival": 1711329825.491783, "serving": 1711329828.0159554}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.42951, "arrival": 1711329830.8444033}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4709806, "arrival": 1711329831.0281537}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4713593, "arrival": 1711329830.8652673}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4303873, "arrival": 1711329830.8648365}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4298127, "arrival": 1711329830.7720718}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429643, "arrival": 1711329830.8040867}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4713168, "arrival": 1711329830.866356}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4302158, "arrival": 1711329830.832983}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4215703, "arrival": 1711329830.8160725}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.430117, "arrival": 1711329830.8282785}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4293313, "arrival": 1711329830.8015}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4714699, "arrival": 1711329831.0507236}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.430084, "arrival": 1711329831.0143037}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4712436, "arrival": 1711329830.8535457}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4297395, "arrival": 1711329831.008798}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.471107, "arrival": 1711329831.0286086}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4567428, "arrival": 1711329830.8479621}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.471122, "arrival": 1711329830.8486943}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429024, "arrival": 1711329830.769036}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4712584, "arrival": 1711329830.863512}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429753, "arrival": 1711329830.8152711}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4655454, "arrival": 1711329830.8672485}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.465639, "arrival": 1711329830.8004787}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380031878+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329825.429911}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.4713733, "arrival": 1711329830.8637605}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380518693+00:00\"}\"\n>", "times": {"request": {"sending": 1711329825.4856768}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.4855702, "arrival": 1711329830.8639846}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429455, "arrival": 1711329830.7704291}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380374827+00:00\"}\"\n>", "times": {"request": {"sending": 1711329825.4711676}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380138504+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329825.4303465}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.428181, "arrival": 1711329828.3664455}, "models": {"yolo": {"arrival": 1711329825.4977229, "serving": 1711329828.0472727}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380075744+00:00\"}\"\n>", "times": {"request": {"sending": 1711329825.430027}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.4289253, "arrival": 1711329830.816335}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.465577, "arrival": 1711329830.8481765}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4711514, "arrival": 1711329830.8433995}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4656084, "arrival": 1711329830.815536}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4304411, "arrival": 1711329830.8333273}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4295244, "arrival": 1711329831.0047288}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.379914503+00:00\"}\"\n>", "times": {"request": {"sending": 1711329825.4295757}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.42987, "arrival": 1711329830.8208659}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.430471, "arrival": 1711329830.807189}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4215453, "arrival": 1711329828.362947}, "models": {"yolo": {"arrival": 1711329825.4917614, "serving": 1711329828.0128648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4216094, "arrival": 1711329830.8008559}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4303737, "arrival": 1711329830.8489995}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380300968+00:00\"}\"\n>", "times": {"request": {"sending": 1711329825.4710467}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.4296026, "arrival": 1711329830.8276324}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4301748, "arrival": 1711329830.843166}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4710917, "arrival": 1711329830.8687766}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380102693+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329825.4301422}}, "outputs": []}, {"times": {"request": {"sending": 1711329825.4711363, "arrival": 1711329830.863136}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4294152, "arrival": 1711329831.0034425}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4293456, "arrival": 1711329830.769793}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4301882, "arrival": 1711329830.8645802}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.43054, "arrival": 1711329831.0249155}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4714525, "arrival": 1711329830.882346}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4710772, "arrival": 1711329830.8537612}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429303, "arrival": 1711329830.9936943}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4654095, "arrival": 1711329830.8460953}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4300408, "arrival": 1711329830.7734878}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4654746, "arrival": 1711329830.8143508}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4282222, "arrival": 1711329830.8046422}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4301555, "arrival": 1711329830.798627}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4713876, "arrival": 1711329830.8438826}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.471062, "arrival": 1711329830.8149993}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4710305, "arrival": 1711329830.8322275}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429884, "arrival": 1711329830.8173063}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4709988, "arrival": 1711329830.848389}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.428108, "arrival": 1711329830.7405753}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4293168, "arrival": 1711329830.8024523}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4297838, "arrival": 1711329830.7751892}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4711826, "arrival": 1711329830.8177216}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4292896, "arrival": 1711329830.816611}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4298973, "arrival": 1711329830.7756486}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429986, "arrival": 1711329830.821127}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4294956, "arrival": 1711329830.8062093}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.43036, "arrival": 1711329830.7991507}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429564, "arrival": 1711329830.7710094}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4655929, "arrival": 1711329830.8464375}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429972, "arrival": 1711329831.013512}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.430454, "arrival": 1711329830.8457859}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4714172, "arrival": 1711329830.818135}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.4299257, "arrival": 1711329830.772725}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429698, "arrival": 1711329830.7715497}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329825.429483, "arrival": 1711329830.7355711}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329826.4117224, "arrival": 1711329830.850952}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4473, "arrival": 1711329830.9116044}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4464035, "arrival": 1711329830.901872}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4103749, "arrival": 1711329830.8183377}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4360197, "arrival": 1711329830.8765342}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4468954, "arrival": 1711329830.8893793}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4108825, "arrival": 1711329830.8677034}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4472506, "arrival": 1711329830.9011316}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4109945, "arrival": 1711329830.8659225}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447285, "arrival": 1711329830.8897588}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4476418, "arrival": 1711329830.9235291}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4320886, "arrival": 1711329830.8751256}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380951423+00:00\"}\"\n>", "times": {"request": {"sending": 1711329826.4471455}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.41107, "arrival": 1711329830.850504}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4105167, "arrival": 1711329830.8668075}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4359615, "arrival": 1711329831.0610297}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4353364, "arrival": 1711329831.056042}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380846448+00:00\"}\"\n>", "times": {"request": {"sending": 1711329826.4465103}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.4469576, "arrival": 1711329830.9118216}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4473548, "arrival": 1711329830.913934}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447507, "arrival": 1711329830.8911018}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447698, "arrival": 1711329830.9039657}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380872465+00:00\"}\"\n>", "times": {"request": {"sending": 1711329826.446651}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380681781+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329826.4321096}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.446582, "arrival": 1711329831.0615332}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.411277, "arrival": 1711329831.051458}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.410576, "arrival": 1711329830.8832853}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4109573, "arrival": 1711329831.0512176}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4474366, "arrival": 1711329831.0628495}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4106612, "arrival": 1711329830.8657064}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4106202, "arrival": 1711329831.050972}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4737291, "arrival": 1711329830.912467}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4107, "arrival": 1711329830.8754108}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.381029574+00:00\"}\"\n>", "times": {"request": {"sending": 1711329826.4474926}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.4114923, "arrival": 1711329830.8471029}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4360006, "arrival": 1711329830.8996959}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4109197, "arrival": 1711329830.8834968}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.435408, "arrival": 1711329830.8698616}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4475615, "arrival": 1711329830.9120498}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4354947, "arrival": 1711329830.867028}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380784168+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329826.4357727}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.4107454, "arrival": 1711329830.8501694}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447464, "arrival": 1711329830.9181268}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4115303, "arrival": 1711329830.8702998}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4474778, "arrival": 1711329830.9035246}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4469275, "arrival": 1711329830.8776274}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380561275+00:00\"}\"\n>", "times": {"request": {"sending": 1711329826.4107904}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.4359813, "arrival": 1711329830.8849823}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380653816+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329826.4117603}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.4356067, "arrival": 1711329830.876108}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447089, "arrival": 1711329831.0621555}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380598349+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329826.4111078}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.447422, "arrival": 1711329830.9196334}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.435514, "arrival": 1711329830.8839438}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4357903, "arrival": 1711329830.868147}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4108398, "arrival": 1711329830.832625}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.44745, "arrival": 1711329830.9113653}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4354553, "arrival": 1711329830.8758929}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4469886, "arrival": 1711329830.90237}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.432184, "arrival": 1711329830.900424}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4477818, "arrival": 1711329831.0634942}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.411835, "arrival": 1711329830.870534}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4475884, "arrival": 1711329830.9037452}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4471734, "arrival": 1711329830.904894}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4356618, "arrival": 1711329830.8841438}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4468791, "arrival": 1711329830.905337}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4360735, "arrival": 1711329830.8845646}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.435644, "arrival": 1711329830.8679218}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.446861, "arrival": 1711329830.9008985}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4471173, "arrival": 1711329830.9057546}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4476557, "arrival": 1711329831.0632837}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380898864+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329826.4469109}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.4321601, "arrival": 1711329830.8826275}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4114122, "arrival": 1711329830.8507314}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4356813, "arrival": 1711329830.9021394}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4111595, "arrival": 1711329830.846783}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.41124, "arrival": 1711329830.8837047}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4476693, "arrival": 1711329830.9122596}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.381050998+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329826.447603}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.4468274, "arrival": 1711329830.9111297}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4116452, "arrival": 1711329830.8694224}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4110324, "arrival": 1711329830.8767357}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.411203, "arrival": 1711329830.870083}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4471312, "arrival": 1711329830.8904626}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.380925273+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329826.447032}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.4476845, "arrival": 1711329830.9187145}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.432038, "arrival": 1711329830.8696423}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380703242+00:00\"}\"\n>", "times": {"request": {"sending": 1711329826.435476}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.446548, "arrival": 1711329830.9013705}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447201, "arrival": 1711329831.0623572}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4361148, "arrival": 1711329831.0613136}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4355516, "arrival": 1711329831.0562723}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.436055, "arrival": 1711329830.868359}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447749, "arrival": 1711329830.9134955}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4113722, "arrival": 1711329830.876937}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4471867, "arrival": 1711329830.9191694}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447548, "arrival": 1711329831.063072}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4474087, "arrival": 1711329830.9128926}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.381076804+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329826.447712}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.411683, "arrival": 1711329830.8771608}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4477644, "arrival": 1711329830.9239511}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4113257, "arrival": 1711329830.867467}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447075, "arrival": 1711329830.9189384}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4117975, "arrival": 1711329830.847425}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4464862, "arrival": 1711329830.8830624}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4471593, "arrival": 1711329830.8780794}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4473677, "arrival": 1711329830.9032917}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4465654, "arrival": 1711329830.9108913}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447521, "arrival": 1711329830.913113}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4319348, "arrival": 1711329831.055797}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4116068, "arrival": 1711329831.0555456}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4468458, "arrival": 1711329831.0617394}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4360917, "arrival": 1711329830.9106379}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380626677+00:00\"}\"\n>", "times": {"request": {"sending": 1711329826.4114513}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.4470038, "arrival": 1711329830.905533}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4465976, "arrival": 1711329830.8853884}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4355335, "arrival": 1711329830.9006636}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4320657, "arrival": 1711329830.8899748}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4358077, "arrival": 1711329830.884355}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4472344, "arrival": 1711329830.9137034}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.446681, "arrival": 1711329830.9041908}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4470456, "arrival": 1711329830.877847}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4358256, "arrival": 1711329830.9102116}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4355881, "arrival": 1711329830.8921406}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4466317, "arrival": 1711329830.8855913}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447218, "arrival": 1711329830.9028614}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4118714, "arrival": 1711329830.9001849}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4465299, "arrival": 1711329830.875662}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4469714, "arrival": 1711329831.0619488}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.446667, "arrival": 1711329830.8774014}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4473133, "arrival": 1711329830.9194076}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380977699+00:00\"}\"\n>", "times": {"request": {"sending": 1711329826.4472675}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.411568, "arrival": 1711329830.8999507}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.380820183+00:00\"}\"\n>", "times": {"request": {"sending": 1711329826.4360375}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.435718, "arrival": 1711329830.884768}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4471033, "arrival": 1711329830.902633}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4361365, "arrival": 1711329830.8851817}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4470172, "arrival": 1711329830.8901832}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4469426, "arrival": 1711329830.9044206}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447341, "arrival": 1711329830.9030774}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4475343, "arrival": 1711329830.9198482}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4357, "arrival": 1711329831.0565052}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4470606, "arrival": 1711329830.9046528}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4476159, "arrival": 1711329830.8913496}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.381003533+00:00\"}\"\n>", "times": {"request": {"sending": 1711329826.4473813}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.4476285, "arrival": 1711329830.913307}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4742458, "arrival": 1711329831.051691}, "models": {"yolo": {"arrival": 1711329830.7413676, "serving": 1711329831.005767}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4473279, "arrival": 1711329831.0625744}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4741988, "arrival": 1711329830.9232848}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4357548, "arrival": 1711329830.8763163}, "models": {"yolo": {"arrival": 1711329828.083514, "serving": 1711329830.7130988}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4473953, "arrival": 1711329830.890773}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.38074379+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329826.435626}}, "outputs": []}, {"times": {"request": {"sending": 1711329826.4321368, "arrival": 1711329830.8477437}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4477322, "arrival": 1711329830.8916736}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4355695, "arrival": 1711329830.8828523}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.447575, "arrival": 1711329830.918478}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4466162, "arrival": 1711329830.9051216}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.435737, "arrival": 1711329830.89932}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329826.4354322, "arrival": 1711329830.891903}, "models": {"yolo": {"arrival": 1711329828.0998795, "serving": 1711329830.7156267}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.381104703+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329826.4749615}}, "outputs": []}], [{"times": {"request": {"sending": 1711329827.4443576, "arrival": 1711329831.5100272}, "models": {"yolo": {"arrival": 1711329831.0135763, "serving": 1711329831.4933834}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4116998, "arrival": 1711329831.0502028}, "models": {"yolo": {"arrival": 1711329830.7663403, "serving": 1711329831.0025156}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4442685, "arrival": 1711329831.6229286}, "models": {"yolo": {"arrival": 1711329831.012149, "serving": 1711329831.6026626}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4440932, "arrival": 1711329831.4228806}, "models": {"yolo": {"arrival": 1711329831.0164266, "serving": 1711329831.4055908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4101357, "arrival": 1711329831.0636957}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4103453, "arrival": 1711329831.025652}, "models": {"yolo": {"arrival": 1711329830.737291, "serving": 1711329830.9923937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.409881, "arrival": 1711329830.9016106}, "models": {"yolo": {"arrival": 1711329828.0945032, "serving": 1711329830.7085443}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4407146, "arrival": 1711329831.5979786}, "models": {"yolo": {"arrival": 1711329831.0021093, "serving": 1711329831.5865777}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.44446, "arrival": 1711329831.6093056}, "models": {"yolo": {"arrival": 1711329831.0021093, "serving": 1711329831.5865777}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.410407, "arrival": 1711329830.921116}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.444713, "arrival": 1711329831.6109118}, "models": {"yolo": {"arrival": 1711329831.0021093, "serving": 1711329831.5865777}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4444926, "arrival": 1711329831.5108097}, "models": {"yolo": {"arrival": 1711329831.0135763, "serving": 1711329831.4933834}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4100053, "arrival": 1711329830.9202847}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.45814, "arrival": 1711329831.5247707}, "models": {"yolo": {"arrival": 1711329831.0135763, "serving": 1711329831.4933834}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4591072, "arrival": 1711329831.821246}, "models": {"yolo": {"arrival": 1711329831.5948784, "serving": 1711329831.7880456}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.444326, "arrival": 1711329831.6010883}, "models": {"yolo": {"arrival": 1711329831.0021093, "serving": 1711329831.5865777}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.458523, "arrival": 1711329831.8174796}, "models": {"yolo": {"arrival": 1711329831.4144454, "serving": 1711329831.7847795}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4110763, "arrival": 1711329831.0666306}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4118803, "arrival": 1711329830.9209201}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4407485, "arrival": 1711329831.0504665}, "models": {"yolo": {"arrival": 1711329830.7663403, "serving": 1711329831.0025156}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4589775, "arrival": 1711329831.8206332}, "models": {"yolo": {"arrival": 1711329831.5948784, "serving": 1711329831.7880456}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4580545, "arrival": 1711329831.5266502}, "models": {"yolo": {"arrival": 1711329831.006502, "serving": 1711329831.5002203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4112926, "arrival": 1711329831.026715}, "models": {"yolo": {"arrival": 1711329830.737291, "serving": 1711329830.9923937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4108627, "arrival": 1711329831.049689}, "models": {"yolo": {"arrival": 1711329830.7663403, "serving": 1711329831.0025156}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4104714, "arrival": 1711329831.0493982}, "models": {"yolo": {"arrival": 1711329830.7663403, "serving": 1711329831.0025156}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4594095, "arrival": 1711329831.8311696}, "models": {"yolo": {"arrival": 1711329831.4260814, "serving": 1711329831.7868295}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.38116855+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329827.410305}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4446187, "arrival": 1711329831.5239303}, "models": {"yolo": {"arrival": 1711329831.0135763, "serving": 1711329831.4933834}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.411655, "arrival": 1711329831.0295124}, "models": {"yolo": {"arrival": 1711329830.7714381, "serving": 1711329830.9976234}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.403594119+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329827.4441597}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4444275, "arrival": 1711329831.4381769}, "models": {"yolo": {"arrival": 1711329831.0164266, "serving": 1711329831.4055908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.444633, "arrival": 1711329831.0681152}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.444544, "arrival": 1711329831.6243365}, "models": {"yolo": {"arrival": 1711329831.012149, "serving": 1711329831.6026626}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4100728, "arrival": 1711329830.924189}, "models": {"yolo": {"arrival": 1711329828.0687525, "serving": 1711329830.7398345}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4445581, "arrival": 1711329831.4384522}, "models": {"yolo": {"arrival": 1711329831.0164266, "serving": 1711329831.4055908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4581654, "arrival": 1711329831.4205096}, "models": {"yolo": {"arrival": 1711329831.0179777, "serving": 1711329831.3957465}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.410525, "arrival": 1711329831.0638957}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4596508, "arrival": 1711329832.2786121}, "models": {"yolo": {"arrival": 1711329831.7834222, "serving": 1711329832.2437901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4114616, "arrival": 1711329830.9207175}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4106207, "arrival": 1711329831.0461743}, "models": {"yolo": {"arrival": 1711329830.757927, "serving": 1711329830.9986315}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4445286, "arrival": 1711329831.0481849}, "models": {"yolo": {"arrival": 1711329830.7635233, "serving": 1711329830.9974844}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4441776, "arrival": 1711329831.6003685}, "models": {"yolo": {"arrival": 1711329831.0021093, "serving": 1711329831.5865777}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4115372, "arrival": 1711329831.0567648}, "models": {"yolo": {"arrival": 1711329830.7413676, "serving": 1711329831.005767}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4583755, "arrival": 1711329831.816806}, "models": {"yolo": {"arrival": 1711329831.4144454, "serving": 1711329831.7847795}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.444511, "arrival": 1711329831.0679214}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.410224, "arrival": 1711329831.045582}, "models": {"yolo": {"arrival": 1711329830.757927, "serving": 1711329830.9986315}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.411176, "arrival": 1711329831.0466046}, "models": {"yolo": {"arrival": 1711329830.757927, "serving": 1711329830.9986315}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.410178, "arrival": 1711329830.9126694}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.459012, "arrival": 1711329831.8383605}, "models": {"yolo": {"arrival": 1711329831.5044143, "serving": 1711329831.8118236}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4112158, "arrival": 1711329831.0552866}, "models": {"yolo": {"arrival": 1711329830.7413676, "serving": 1711329831.005767}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.459122, "arrival": 1711329831.822496}, "models": {"yolo": {"arrival": 1711329831.5098376, "serving": 1711329831.7848217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4106677, "arrival": 1711329831.0549679}, "models": {"yolo": {"arrival": 1711329830.7413676, "serving": 1711329831.005767}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4596298, "arrival": 1711329831.839647}, "models": {"yolo": {"arrival": 1711329831.5044143, "serving": 1711329831.8118236}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.458188, "arrival": 1711329831.45394}, "models": {"yolo": {"arrival": 1711329831.0063083, "serving": 1711329831.415193}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4442868, "arrival": 1711329831.4377613}, "models": {"yolo": {"arrival": 1711329831.0164266, "serving": 1711329831.4055908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4407952, "arrival": 1711329831.61981}, "models": {"yolo": {"arrival": 1711329831.012149, "serving": 1711329831.6026626}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.43757557+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329827.458251}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.410265, "arrival": 1711329831.05192}, "models": {"yolo": {"arrival": 1711329830.7413676, "serving": 1711329831.005767}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4584372, "arrival": 1711329831.5304236}, "models": {"yolo": {"arrival": 1711329831.006502, "serving": 1711329831.5002203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4442365, "arrival": 1711329831.0675092}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.458268, "arrival": 1711329831.6115112}, "models": {"yolo": {"arrival": 1711329831.0021093, "serving": 1711329831.5865777}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.459199, "arrival": 1711329831.8194103}, "models": {"yolo": {"arrival": 1711329831.4144454, "serving": 1711329831.7847795}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4593778, "arrival": 1711329831.83914}, "models": {"yolo": {"arrival": 1711329831.5044143, "serving": 1711329831.8118236}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4105725, "arrival": 1711329830.9200666}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4119196, "arrival": 1711329831.6141727}, "models": {"yolo": {"arrival": 1711329831.012149, "serving": 1711329831.6026626}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.42549493+00:00\"}\"\n>", "times": {"request": {"sending": 1711329827.4445755}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4588258, "arrival": 1711329831.8180854}, "models": {"yolo": {"arrival": 1711329831.4144454, "serving": 1711329831.7847795}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.459318, "arrival": 1711329832.260572}, "models": {"yolo": {"arrival": 1711329831.79307, "serving": 1711329832.242326}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4446793, "arrival": 1711329831.438696}, "models": {"yolo": {"arrival": 1711329831.0164266, "serving": 1711329831.4055908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4591832, "arrival": 1711329831.8334157}, "models": {"yolo": {"arrival": 1711329831.6150713, "serving": 1711329831.7981782}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4584854, "arrival": 1711329831.4546182}, "models": {"yolo": {"arrival": 1711329831.0063083, "serving": 1711329831.415193}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4595618, "arrival": 1711329832.2661207}, "models": {"yolo": {"arrival": 1711329831.79307, "serving": 1711329832.242326}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4113696, "arrival": 1711329831.0499594}, "models": {"yolo": {"arrival": 1711329830.7663403, "serving": 1711329831.0025156}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4585028, "arrival": 1711329831.8323138}, "models": {"yolo": {"arrival": 1711329831.6150713, "serving": 1711329831.7981782}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.444341, "arrival": 1711329831.0322034}, "models": {"yolo": {"arrival": 1711329830.7714381, "serving": 1711329830.9976234}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4592574, "arrival": 1711329831.8388875}, "models": {"yolo": {"arrival": 1711329831.5044143, "serving": 1711329831.8118236}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.387671058+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329827.4107156}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4593484, "arrival": 1711329831.82984}, "models": {"yolo": {"arrival": 1711329831.5948784, "serving": 1711329831.7880456}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4443915, "arrival": 1711329831.0477648}, "models": {"yolo": {"arrival": 1711329830.7635233, "serving": 1711329830.9974844}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4446516, "arrival": 1711329831.048957}, "models": {"yolo": {"arrival": 1711329830.7635233, "serving": 1711329830.9974844}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4595976, "arrival": 1711329832.385822}, "models": {"yolo": {"arrival": 1711329831.8002532, "serving": 1711329832.3713877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4593022, "arrival": 1711329831.837283}, "models": {"yolo": {"arrival": 1711329831.6150713, "serving": 1711329831.7981782}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4444747, "arrival": 1711329831.5125332}, "models": {"yolo": {"arrival": 1711329831.006502, "serving": 1711329831.5002203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4594386, "arrival": 1711329832.2650745}, "models": {"yolo": {"arrival": 1711329831.79307, "serving": 1711329832.242326}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4592886, "arrival": 1711329831.8307755}, "models": {"yolo": {"arrival": 1711329831.4260814, "serving": 1711329831.7868295}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4594545, "arrival": 1711329830.5624995}, "models": {"yolo": {"arrival": 1711329830.4714255, "serving": 1711329830.5382242}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4591382, "arrival": 1711329831.8386276}, "models": {"yolo": {"arrival": 1711329831.5044143, "serving": 1711329831.8118236}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4592423, "arrival": 1711329831.823129}, "models": {"yolo": {"arrival": 1711329831.5098376, "serving": 1711329831.7848217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4117427, "arrival": 1711329831.0670943}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.444664, "arrival": 1711329831.62705}, "models": {"yolo": {"arrival": 1711329831.012149, "serving": 1711329831.6026626}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.444375, "arrival": 1711329831.067726}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4586027, "arrival": 1711329831.8200145}, "models": {"yolo": {"arrival": 1711329831.5948784, "serving": 1711329831.7880456}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.410768, "arrival": 1711329831.0261679}, "models": {"yolo": {"arrival": 1711329830.737291, "serving": 1711329830.9923937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4593327, "arrival": 1711329830.5613003}, "models": {"yolo": {"arrival": 1711329830.4714255, "serving": 1711329830.5382242}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4116118, "arrival": 1711329831.0271907}, "models": {"yolo": {"arrival": 1711329830.737291, "serving": 1711329830.9923937}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4590287, "arrival": 1711329831.7908983}, "models": {"yolo": {"arrival": 1711329831.405418, "serving": 1711329831.77368}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4584196, "arrival": 1711329831.612673}, "models": {"yolo": {"arrival": 1711329831.0021093, "serving": 1711329831.5865777}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.444605, "arrival": 1711329831.5132973}, "models": {"yolo": {"arrival": 1711329831.006502, "serving": 1711329831.5002203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4108155, "arrival": 1711329830.9213288}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.417444221+00:00\"}\"\n>", "times": {"request": {"sending": 1711329827.4444442}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4583387, "arrival": 1711329831.4543352}, "models": {"yolo": {"arrival": 1711329831.0063083, "serving": 1711329831.415193}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.394684002+00:00\"}\"\n>", "times": {"request": {"sending": 1711329827.4115746}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4592292, "arrival": 1711329831.8218503}, "models": {"yolo": {"arrival": 1711329831.5948784, "serving": 1711329831.7880456}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4587266, "arrival": 1711329831.4223526}, "models": {"yolo": {"arrival": 1711329831.0179777, "serving": 1711329831.3957465}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4597006, "arrival": 1711329832.267081}, "models": {"yolo": {"arrival": 1711329831.79307, "serving": 1711329832.242326}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4444096, "arrival": 1711329831.6236677}, "models": {"yolo": {"arrival": 1711329831.012149, "serving": 1711329831.6026626}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4584537, "arrival": 1711329831.5260708}, "models": {"yolo": {"arrival": 1711329831.0135763, "serving": 1711329831.4933834}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4590435, "arrival": 1711329831.4550664}, "models": {"yolo": {"arrival": 1711329831.0063083, "serving": 1711329831.415193}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.458997, "arrival": 1711329831.5320024}, "models": {"yolo": {"arrival": 1711329831.006502, "serving": 1711329831.5002203}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.40780506+00:00\"}\"\n>", "times": {"request": {"sending": 1711329827.444304}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4584699, "arrival": 1711329831.4219427}, "models": {"yolo": {"arrival": 1711329831.0179777, "serving": 1711329831.3957465}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.459503, "arrival": 1711329831.839388}, "models": {"yolo": {"arrival": 1711329831.5044143, "serving": 1711329831.8118236}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.430138597+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329827.4446955}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.458318, "arrival": 1711329831.4212608}, "models": {"yolo": {"arrival": 1711329831.0179777, "serving": 1711329831.3957465}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4582121, "arrival": 1711329831.627747}, "models": {"yolo": {"arrival": 1711329831.012149, "serving": 1711329831.6026626}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4113312, "arrival": 1711329830.92303}, "models": {"yolo": {"arrival": 1711329828.0917585, "serving": 1711329830.7396808}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.472237473+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329827.459213}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4582348, "arrival": 1711329831.8159082}, "models": {"yolo": {"arrival": 1711329831.4144454, "serving": 1711329831.7847795}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4445906, "arrival": 1711329831.6101174}, "models": {"yolo": {"arrival": 1711329831.0021093, "serving": 1711329831.5865777}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4441957, "arrival": 1711329831.0318277}, "models": {"yolo": {"arrival": 1711329830.7714381, "serving": 1711329830.9976234}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4442163, "arrival": 1711329831.508976}, "models": {"yolo": {"arrival": 1711329831.0135763, "serving": 1711329831.4933834}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4594681, "arrival": 1711329832.38496}, "models": {"yolo": {"arrival": 1711329831.8002532, "serving": 1711329832.3713877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4588003, "arrival": 1711329831.8326833}, "models": {"yolo": {"arrival": 1711329831.6150713, "serving": 1711329831.7981782}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4595323, "arrival": 1711329831.831554}, "models": {"yolo": {"arrival": 1711329831.4260814, "serving": 1711329831.7868295}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.459169, "arrival": 1711329831.4552755}, "models": {"yolo": {"arrival": 1711329831.0063083, "serving": 1711329831.415193}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.458285, "arrival": 1711329831.527189}, "models": {"yolo": {"arrival": 1711329831.006502, "serving": 1711329831.5002203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4114993, "arrival": 1711329831.0469935}, "models": {"yolo": {"arrival": 1711329830.757927, "serving": 1711329830.9986315}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.411127, "arrival": 1711329830.920511}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.398800701+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329827.4406364}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4593642, "arrival": 1711329831.8250816}, "models": {"yolo": {"arrival": 1711329831.5098376, "serving": 1711329831.7848217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4595795, "arrival": 1711329830.5631838}, "models": {"yolo": {"arrival": 1711329830.4714255, "serving": 1711329830.5382242}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4596138, "arrival": 1711329831.830326}, "models": {"yolo": {"arrival": 1711329831.5098376, "serving": 1711329831.7848217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4590626, "arrival": 1711329831.8330472}, "models": {"yolo": {"arrival": 1711329831.6150713, "serving": 1711329831.7981782}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.390737363+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329827.411255}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4407642, "arrival": 1711329831.0673013}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4592724, "arrival": 1711329831.7965627}, "models": {"yolo": {"arrival": 1711329831.405418, "serving": 1711329831.77368}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4114184, "arrival": 1711329831.0668852}, "models": {"yolo": {"arrival": 1711329828.0990653, "serving": 1711329830.978289}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4596682, "arrival": 1711329831.8319306}, "models": {"yolo": {"arrival": 1711329831.4260814, "serving": 1711329831.7868295}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.459006683+00:00\"}\"\n>", "times": {"request": {"sending": 1711329827.4589589}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.459155, "arrival": 1711329831.7953217}, "models": {"yolo": {"arrival": 1711329831.405418, "serving": 1711329831.77368}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.446681163+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329827.4585578}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4407332, "arrival": 1711329831.0312257}, "models": {"yolo": {"arrival": 1711329830.7714381, "serving": 1711329830.9976234}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.443158136+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329827.4584024}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4594872, "arrival": 1711329831.8257673}, "models": {"yolo": {"arrival": 1711329831.5098376, "serving": 1711329831.7848217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4442525, "arrival": 1711329831.047374}, "models": {"yolo": {"arrival": 1711329830.7635233, "serving": 1711329830.9974844}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4596844, "arrival": 1711329832.5746017}, "models": {"yolo": {"arrival": 1711329831.812256, "serving": 1711329832.5131166}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4583561, "arrival": 1711329831.628395}, "models": {"yolo": {"arrival": 1711329831.012149, "serving": 1711329831.6026626}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4582999, "arrival": 1711329831.525475}, "models": {"yolo": {"arrival": 1711329831.0135763, "serving": 1711329831.4933834}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4586847, "arrival": 1711329831.8380988}, "models": {"yolo": {"arrival": 1711329831.5044143, "serving": 1711329831.8118236}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4586422, "arrival": 1711329831.531392}, "models": {"yolo": {"arrival": 1711329831.006502, "serving": 1711329831.5002203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.459077, "arrival": 1711329831.818774}, "models": {"yolo": {"arrival": 1711329831.4144454, "serving": 1711329831.7847795}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.598674561+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329827.4597137}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4587634, "arrival": 1711329831.4548452}, "models": {"yolo": {"arrival": 1711329831.0063083, "serving": 1711329831.415193}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.459519, "arrival": 1711329831.7987792}, "models": {"yolo": {"arrival": 1711329831.405418, "serving": 1711329831.77368}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4407809, "arrival": 1711329830.9237392}, "models": {"yolo": {"arrival": 1711329828.0996437, "serving": 1711329830.7366822}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.459423, "arrival": 1711329831.8378131}, "models": {"yolo": {"arrival": 1711329831.6150713, "serving": 1711329831.7981782}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4119582, "arrival": 1711329831.0570202}, "models": {"yolo": {"arrival": 1711329830.7413676, "serving": 1711329831.005767}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.459089512+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 23.64 GiB total capacity; 1.36 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329827.4590912}}, "outputs": []}, {"times": {"request": {"sending": 1711329827.4593923, "arrival": 1711329831.7976372}, "models": {"yolo": {"arrival": 1711329831.405418, "serving": 1711329831.77368}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329827.4595482, "arrival": 1711329832.5739307}, "models": {"yolo": {"arrival": 1711329831.812256, "serving": 1711329832.5131166}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329828.4476125, "arrival": 1711329833.4194834}, "models": {"yolo": {"arrival": 1711329833.0676713, "serving": 1711329833.3682976}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.670308365+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329828.4475057}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4084823, "arrival": 1711329832.3007393}, "models": {"yolo": {"arrival": 1711329831.7947009, "serving": 1711329832.2574131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4467125, "arrival": 1711329833.0846193}, "models": {"yolo": {"arrival": 1711329832.613952, "serving": 1711329833.0547957}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4472077, "arrival": 1711329833.4110036}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4086185, "arrival": 1711329831.8369596}, "models": {"yolo": {"arrival": 1711329831.4260814, "serving": 1711329831.7868295}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4085786, "arrival": 1711329832.278959}, "models": {"yolo": {"arrival": 1711329831.7834222, "serving": 1711329832.2437901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4467428, "arrival": 1711329832.6692538}, "models": {"yolo": {"arrival": 1711329832.3809612, "serving": 1711329832.6156497}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4093094, "arrival": 1711329832.301586}, "models": {"yolo": {"arrival": 1711329831.7947009, "serving": 1711329832.2574131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4089403, "arrival": 1711329832.2794626}, "models": {"yolo": {"arrival": 1711329831.7834222, "serving": 1711329832.2437901}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.605212596+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329828.4354432}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4083757, "arrival": 1711329832.3865354}, "models": {"yolo": {"arrival": 1711329831.8002532, "serving": 1711329832.3713877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4471183, "arrival": 1711329833.2668653}, "models": {"yolo": {"arrival": 1711329832.6262448, "serving": 1711329833.2509623}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4467819, "arrival": 1711329832.6627123}, "models": {"yolo": {"arrival": 1711329832.2618701, "serving": 1711329832.6124985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.447491, "arrival": 1711329833.4131424}, "models": {"yolo": {"arrival": 1711329833.0676713, "serving": 1711329833.3682976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4090168, "arrival": 1711329832.5753648}, "models": {"yolo": {"arrival": 1711329831.812256, "serving": 1711329832.5131166}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4477673, "arrival": 1711329833.2726502}, "models": {"yolo": {"arrival": 1711329832.6262448, "serving": 1711329833.2509623}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.447582, "arrival": 1711329833.09858}, "models": {"yolo": {"arrival": 1711329832.613621, "serving": 1711329833.0611136}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4476857, "arrival": 1711329833.3637342}, "models": {"yolo": {"arrival": 1711329833.0589569, "serving": 1711329833.3443854}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4085352, "arrival": 1711329832.2964845}, "models": {"yolo": {"arrival": 1711329831.8211863, "serving": 1711329832.251821}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4360392, "arrival": 1711329832.6584444}, "models": {"yolo": {"arrival": 1711329832.2667785, "serving": 1711329832.6134863}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4464772, "arrival": 1711329832.6679606}, "models": {"yolo": {"arrival": 1711329832.3809612, "serving": 1711329832.6156497}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4089015, "arrival": 1711329832.29728}, "models": {"yolo": {"arrival": 1711329831.8211863, "serving": 1711329832.251821}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4465427, "arrival": 1711329832.625903}, "models": {"yolo": {"arrival": 1711329832.254528, "serving": 1711329832.6055558}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.598901316+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329828.4098682}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4469543, "arrival": 1711329832.656289}, "models": {"yolo": {"arrival": 1711329832.254528, "serving": 1711329832.6055558}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4096723, "arrival": 1711329832.299503}, "models": {"yolo": {"arrival": 1711329831.8211863, "serving": 1711329832.251821}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4089777, "arrival": 1711329831.837554}, "models": {"yolo": {"arrival": 1711329831.4260814, "serving": 1711329831.7868295}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.447384, "arrival": 1711329833.268403}, "models": {"yolo": {"arrival": 1711329832.6262448, "serving": 1711329833.2509623}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.670528799+00:00\"}\"\n>", "times": {"request": {"sending": 1711329828.4724147}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4478288, "arrival": 1711329833.1062171}, "models": {"yolo": {"arrival": 1711329832.613621, "serving": 1711329833.0611136}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.446887, "arrival": 1711329832.6701117}, "models": {"yolo": {"arrival": 1711329832.3809612, "serving": 1711329832.6156497}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.59877503+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329828.408781}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.447156, "arrival": 1711329833.1011415}, "models": {"yolo": {"arrival": 1711329832.6217265, "serving": 1711329833.0633705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4088237, "arrival": 1711329832.3965821}, "models": {"yolo": {"arrival": 1711329831.8002532, "serving": 1711329832.3713877}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.608252559+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329828.4468706}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4096322, "arrival": 1711329832.3019829}, "models": {"yolo": {"arrival": 1711329831.7947009, "serving": 1711329832.2574131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4088624, "arrival": 1711329832.301181}, "models": {"yolo": {"arrival": 1711329831.7947009, "serving": 1711329832.2574131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4093924, "arrival": 1711329832.2799037}, "models": {"yolo": {"arrival": 1711329831.7834222, "serving": 1711329832.2437901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4475656, "arrival": 1711329833.3588507}, "models": {"yolo": {"arrival": 1711329833.0589569, "serving": 1711329833.3443854}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.435867, "arrival": 1711329832.3063414}, "models": {"yolo": {"arrival": 1711329831.7947009, "serving": 1711329832.2574131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4090555, "arrival": 1711329832.2782357}, "models": {"yolo": {"arrival": 1711329831.79307, "serving": 1711329832.242326}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4469037, "arrival": 1711329832.668669}, "models": {"yolo": {"arrival": 1711329832.2667785, "serving": 1711329832.6134863}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.447455, "arrival": 1711329833.0980098}, "models": {"yolo": {"arrival": 1711329832.613621, "serving": 1711329833.0611136}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4318712, "arrival": 1711329832.4000597}, "models": {"yolo": {"arrival": 1711329831.8002532, "serving": 1711329832.3713877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4356608, "arrival": 1711329832.305938}, "models": {"yolo": {"arrival": 1711329831.7947009, "serving": 1711329832.2574131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4086573, "arrival": 1711329832.5750394}, "models": {"yolo": {"arrival": 1711329831.812256, "serving": 1711329832.5131166}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4474185, "arrival": 1711329833.102145}, "models": {"yolo": {"arrival": 1711329832.6217265, "serving": 1711329833.0633705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.446511, "arrival": 1711329832.6615274}, "models": {"yolo": {"arrival": 1711329832.2618701, "serving": 1711329832.6124985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.435946, "arrival": 1711329832.3023827}, "models": {"yolo": {"arrival": 1711329831.7959337, "serving": 1711329832.2437015}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4479008, "arrival": 1711329833.273393}, "models": {"yolo": {"arrival": 1711329832.6262448, "serving": 1711329833.2509623}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4474, "arrival": 1711329833.099616}, "models": {"yolo": {"arrival": 1711329832.6227572, "serving": 1711329833.062578}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.435994, "arrival": 1711329833.0818048}, "models": {"yolo": {"arrival": 1711329832.613952, "serving": 1711329833.0547957}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4355981, "arrival": 1711329832.654898}, "models": {"yolo": {"arrival": 1711329832.2518697, "serving": 1711329832.6046054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4320228, "arrival": 1711329832.3003368}, "models": {"yolo": {"arrival": 1711329831.7834222, "serving": 1711329832.2437901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4478004, "arrival": 1711329833.1076446}, "models": {"yolo": {"arrival": 1711329832.6217265, "serving": 1711329833.0633705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4477305, "arrival": 1711329833.4197748}, "models": {"yolo": {"arrival": 1711329833.0676713, "serving": 1711329833.3682976}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.605316604+00:00\"}\"\n>", "times": {"request": {"sending": 1711329828.4356174}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4360244, "arrival": 1711329832.664761}, "models": {"yolo": {"arrival": 1711329832.3809612, "serving": 1711329832.6156497}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4093523, "arrival": 1711329832.2978876}, "models": {"yolo": {"arrival": 1711329831.8211863, "serving": 1711329832.251821}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.435472, "arrival": 1711329832.4007683}, "models": {"yolo": {"arrival": 1711329831.8002532, "serving": 1711329832.3713877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4094331, "arrival": 1711329832.2803128}, "models": {"yolo": {"arrival": 1711329831.7959337, "serving": 1711329832.2437015}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4473488, "arrival": 1711329833.412853}, "models": {"yolo": {"arrival": 1711329833.0676713, "serving": 1711329833.3682976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4472833, "arrival": 1711329833.101657}, "models": {"yolo": {"arrival": 1711329832.6217265, "serving": 1711329833.0633705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4474752, "arrival": 1711329833.4119854}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.670487375+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329828.4478843}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4087214, "arrival": 1711329832.2777238}, "models": {"yolo": {"arrival": 1711329831.79307, "serving": 1711329832.242326}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.447174, "arrival": 1711329833.0800633}, "models": {"yolo": {"arrival": 1711329832.619892, "serving": 1711329833.0524848}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4320712, "arrival": 1711329832.5760117}, "models": {"yolo": {"arrival": 1711329831.812256, "serving": 1711329832.5131166}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4094722, "arrival": 1711329832.575598}, "models": {"yolo": {"arrival": 1711329831.812256, "serving": 1711329832.5131166}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4469905, "arrival": 1711329833.4122794}, "models": {"yolo": {"arrival": 1711329833.0676713, "serving": 1711329833.3682976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.409511, "arrival": 1711329832.621881}, "models": {"yolo": {"arrival": 1711329832.2518697, "serving": 1711329832.6046054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4356415, "arrival": 1711329832.4014049}, "models": {"yolo": {"arrival": 1711329831.8002532, "serving": 1711329832.3713877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4473324, "arrival": 1711329833.4116666}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4095924, "arrival": 1711329832.3979871}, "models": {"yolo": {"arrival": 1711329831.8002532, "serving": 1711329832.3713877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4475968, "arrival": 1711329833.4178}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.435902, "arrival": 1711329832.6601517}, "models": {"yolo": {"arrival": 1711329832.2618701, "serving": 1711329832.6124985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4463942, "arrival": 1711329832.6238217}, "models": {"yolo": {"arrival": 1711329832.254528, "serving": 1711329832.6055558}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.605403841+00:00\"}\"\n>", "times": {"request": {"sending": 1711329828.4360085}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.435751, "arrival": 1711329832.5764427}, "models": {"yolo": {"arrival": 1711329831.812256, "serving": 1711329832.5131166}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4091332, "arrival": 1711329832.3973532}, "models": {"yolo": {"arrival": 1711329831.8002532, "serving": 1711329832.3713877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.447717, "arrival": 1711329833.418188}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.598857069+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329828.4095516}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4715064, "arrival": 1711329833.3748493}, "models": {"yolo": {"arrival": 1711329833.0675986, "serving": 1711329833.356877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4319994, "arrival": 1711329832.2999425}, "models": {"yolo": {"arrival": 1711329831.8211863, "serving": 1711329832.251821}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4356794, "arrival": 1711329832.3031921}, "models": {"yolo": {"arrival": 1711329831.8211863, "serving": 1711329832.251821}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4478147, "arrival": 1711329833.365504}, "models": {"yolo": {"arrival": 1711329833.0589569, "serving": 1711329833.3443854}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4465277, "arrival": 1711329832.668969}, "models": {"yolo": {"arrival": 1711329832.25256, "serving": 1711329832.6123693}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4477026, "arrival": 1711329833.105624}, "models": {"yolo": {"arrival": 1711329832.613621, "serving": 1711329833.0611136}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.447253, "arrival": 1711329833.2677534}, "models": {"yolo": {"arrival": 1711329832.6262448, "serving": 1711329833.2509623}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4475493, "arrival": 1711329833.1026402}, "models": {"yolo": {"arrival": 1711329832.6217265, "serving": 1711329833.0633705}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.598818057+00:00\"}\"\n>", "times": {"request": {"sending": 1711329828.4090934}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4466248, "arrival": 1711329832.6593196}, "models": {"yolo": {"arrival": 1711329832.2667785, "serving": 1711329832.6134863}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4471927, "arrival": 1711329833.0969236}, "models": {"yolo": {"arrival": 1711329832.613621, "serving": 1711329833.0611136}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4467657, "arrival": 1711329832.6597311}, "models": {"yolo": {"arrival": 1711329832.2667785, "serving": 1711329832.6134863}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4465723, "arrival": 1711329833.084004}, "models": {"yolo": {"arrival": 1711329832.613952, "serving": 1711329833.0547957}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4710968, "arrival": 1711329833.4038217}, "models": {"yolo": {"arrival": 1711329833.0720346, "serving": 1711329833.362891}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4353569, "arrival": 1711329832.654028}, "models": {"yolo": {"arrival": 1711329832.2518697, "serving": 1711329832.6046054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4473126, "arrival": 1711329833.0974715}, "models": {"yolo": {"arrival": 1711329832.613621, "serving": 1711329833.0611136}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.435494, "arrival": 1711329832.3055239}, "models": {"yolo": {"arrival": 1711329831.7947009, "serving": 1711329832.2574131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.435537, "arrival": 1711329832.6632981}, "models": {"yolo": {"arrival": 1711329832.25256, "serving": 1711329832.6123693}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4355567, "arrival": 1711329832.282474}, "models": {"yolo": {"arrival": 1711329831.7959337, "serving": 1711329832.2437015}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4469373, "arrival": 1711329833.0794525}, "models": {"yolo": {"arrival": 1711329832.619892, "serving": 1711329833.0524848}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.447519, "arrival": 1711329833.2690587}, "models": {"yolo": {"arrival": 1711329832.6262448, "serving": 1711329833.2509623}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.608182484+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329828.4467268}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4469182, "arrival": 1711329832.6630087}, "models": {"yolo": {"arrival": 1711329832.2618701, "serving": 1711329832.6124985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4475343, "arrival": 1711329833.100109}, "models": {"yolo": {"arrival": 1711329832.6227572, "serving": 1711329833.062578}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4464936, "arrival": 1711329832.6588833}, "models": {"yolo": {"arrival": 1711329832.2667785, "serving": 1711329832.6134863}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.605476701+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329828.4465873}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4715288, "arrival": 1711329833.4187958}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4359775, "arrival": 1711329832.57666}, "models": {"yolo": {"arrival": 1711329831.812256, "serving": 1711329832.5131166}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4714622, "arrival": 1711329833.3671849}, "models": {"yolo": {"arrival": 1711329833.0589569, "serving": 1711329833.3443854}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.409751, "arrival": 1711329832.2807336}, "models": {"yolo": {"arrival": 1711329831.7959337, "serving": 1711329832.2437015}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.472372, "arrival": 1711329834.6871483}, "models": {"yolo": {"arrival": 1711329833.3819253, "serving": 1711329834.6670835}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.605363293+00:00\"}\"\n>", "times": {"request": {"sending": 1711329828.4358048}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.446419, "arrival": 1711329832.6567218}, "models": {"yolo": {"arrival": 1711329832.5271626, "serving": 1711329832.609353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4098296, "arrival": 1711329832.622992}, "models": {"yolo": {"arrival": 1711329832.2518697, "serving": 1711329832.6046054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4472983, "arrival": 1711329833.0806448}, "models": {"yolo": {"arrival": 1711329832.619892, "serving": 1711329833.0524848}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4468508, "arrival": 1711329833.096186}, "models": {"yolo": {"arrival": 1711329832.613952, "serving": 1711329833.0547957}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.670446959+00:00\"}\"\n>", "times": {"request": {"sending": 1711329828.4477494}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4466422, "arrival": 1711329832.6622643}, "models": {"yolo": {"arrival": 1711329832.2618701, "serving": 1711329832.6124985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4472222, "arrival": 1711329833.4125693}, "models": {"yolo": {"arrival": 1711329833.0676713, "serving": 1711329833.3682976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4359221, "arrival": 1711329832.6638815}, "models": {"yolo": {"arrival": 1711329832.25256, "serving": 1711329832.6123693}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.608280103+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329828.4470077}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4477837, "arrival": 1711329833.1066387}, "models": {"yolo": {"arrival": 1711329832.6227572, "serving": 1711329833.062578}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4355166, "arrival": 1711329832.3027866}, "models": {"yolo": {"arrival": 1711329831.8211863, "serving": 1711329832.251821}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4466963, "arrival": 1711329832.657588}, "models": {"yolo": {"arrival": 1711329832.5271626, "serving": 1711329832.609353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4097118, "arrival": 1711329832.2984903}, "models": {"yolo": {"arrival": 1711329831.7834222, "serving": 1711329832.2437901}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4479165, "arrival": 1711329833.1073163}, "models": {"yolo": {"arrival": 1711329832.6227572, "serving": 1711329833.062578}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.670210404+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329828.4473672}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4478476, "arrival": 1711329833.4184995}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.43584, "arrival": 1711329832.6644733}, "models": {"yolo": {"arrival": 1711329832.3809612, "serving": 1711329832.6156497}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.446836, "arrival": 1711329832.6580074}, "models": {"yolo": {"arrival": 1711329832.5271626, "serving": 1711329832.609353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4465578, "arrival": 1711329832.6571615}, "models": {"yolo": {"arrival": 1711329832.5271626, "serving": 1711329832.609353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4466617, "arrival": 1711329832.6698298}, "models": {"yolo": {"arrival": 1711329832.25256, "serving": 1711329832.6123693}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.446304, "arrival": 1711329832.6641848}, "models": {"yolo": {"arrival": 1711329832.25256, "serving": 1711329832.6123693}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4476562, "arrival": 1711329833.100599}, "models": {"yolo": {"arrival": 1711329832.6227572, "serving": 1711329833.062578}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.40979, "arrival": 1711329832.575805}, "models": {"yolo": {"arrival": 1711329831.812256, "serving": 1711329832.5131166}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.670366752+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329828.4476273}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4360547, "arrival": 1711329832.6608567}, "models": {"yolo": {"arrival": 1711329832.2618701, "serving": 1711329832.6124985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4466033, "arrival": 1711329832.6683502}, "models": {"yolo": {"arrival": 1711329832.3809612, "serving": 1711329832.6156497}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4356985, "arrival": 1711329832.6635892}, "models": {"yolo": {"arrival": 1711329832.25256, "serving": 1711329832.6123693}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4357774, "arrival": 1711329832.6553833}, "models": {"yolo": {"arrival": 1711329832.2518697, "serving": 1711329832.6046054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4469728, "arrival": 1711329833.4105852}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4474366, "arrival": 1711329833.0812395}, "models": {"yolo": {"arrival": 1711329832.619892, "serving": 1711329833.0524848}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4357173, "arrival": 1711329832.2990506}, "models": {"yolo": {"arrival": 1711329831.7959337, "serving": 1711329832.2437015}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.605436458+00:00\"}\"\n>", "times": {"request": {"sending": 1711329828.4464567}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4468007, "arrival": 1711329833.078622}, "models": {"yolo": {"arrival": 1711329832.619892, "serving": 1711329833.0524848}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4471383, "arrival": 1711329832.669548}, "models": {"yolo": {"arrival": 1711329832.2667785, "serving": 1711329832.6134863}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.447267, "arrival": 1711329833.0990994}, "models": {"yolo": {"arrival": 1711329832.6227572, "serving": 1711329833.062578}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4319673, "arrival": 1711329832.3050277}, "models": {"yolo": {"arrival": 1711329831.7947009, "serving": 1711329832.2574131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4464386, "arrival": 1711329833.0823529}, "models": {"yolo": {"arrival": 1711329832.613952, "serving": 1711329833.0547957}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.446819, "arrival": 1711329832.6558332}, "models": {"yolo": {"arrival": 1711329832.254528, "serving": 1711329832.6055558}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4355774, "arrival": 1711329832.5762196}, "models": {"yolo": {"arrival": 1711329831.812256, "serving": 1711329832.5131166}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4466798, "arrival": 1711329832.6264803}, "models": {"yolo": {"arrival": 1711329832.254528, "serving": 1711329832.6055558}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4476414, "arrival": 1711329833.2704992}, "models": {"yolo": {"arrival": 1711329832.6262448, "serving": 1711329833.2509623}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.40 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.608303675+00:00\"}\"\n>", "times": {"request": {"sending": 1711329828.447237}}, "outputs": []}, {"times": {"request": {"sending": 1711329828.4476705, "arrival": 1711329833.1069686}, "models": {"yolo": {"arrival": 1711329832.6217265, "serving": 1711329833.0633705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.4478674, "arrival": 1711329833.4199846}, "models": {"yolo": {"arrival": 1711329833.0676713, "serving": 1711329833.3682976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329828.432046, "arrival": 1711329832.2819824}, "models": {"yolo": {"arrival": 1711329831.7959337, "serving": 1711329832.2437015}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329829.4461906, "arrival": 1711329835.1693158}, "models": {"yolo": {"arrival": 1711329834.9323068, "serving": 1711329835.1554377}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.444891, "arrival": 1711329836.7094615}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.414867, "arrival": 1711329833.3756244}, "models": {"yolo": {"arrival": 1711329833.0675986, "serving": 1711329833.356877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4352682, "arrival": 1711329833.421034}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4447148, "arrival": 1711329836.68526}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.016201526+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4459095}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.015934202+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.4444294}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4349475, "arrival": 1711329836.6609921}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4151685, "arrival": 1711329833.3719742}, "models": {"yolo": {"arrival": 1711329833.258027, "serving": 1711329833.3569007}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.016019294+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.444994}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4452105, "arrival": 1711329834.9472313}, "models": {"yolo": {"arrival": 1711329834.2354183, "serving": 1711329834.9246502}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4448466, "arrival": 1711329836.685875}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4146945, "arrival": 1711329833.2740326}, "models": {"yolo": {"arrival": 1711329832.6262448, "serving": 1711329833.2509623}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4457016, "arrival": 1711329834.8702185}, "models": {"yolo": {"arrival": 1711329833.806287, "serving": 1711329834.8452475}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.43502, "arrival": 1711329834.6979725}, "models": {"yolo": {"arrival": 1711329833.3819253, "serving": 1711329834.6670835}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.67984667+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4350548}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4150727, "arrival": 1711329833.3760743}, "models": {"yolo": {"arrival": 1711329833.0675986, "serving": 1711329833.356877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445008, "arrival": 1711329836.7105772}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4149995, "arrival": 1711329833.4021547}, "models": {"yolo": {"arrival": 1711329833.0724533, "serving": 1711329833.358283}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4147785, "arrival": 1711329833.4014874}, "models": {"yolo": {"arrival": 1711329833.0724533, "serving": 1711329833.358283}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.760380341+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4462593}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.015775348+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4152405}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.016251336+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.4462044}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4150252, "arrival": 1711329833.404738}, "models": {"yolo": {"arrival": 1711329833.0720346, "serving": 1711329833.362891}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4453433, "arrival": 1711329834.947969}, "models": {"yolo": {"arrival": 1711329834.2354183, "serving": 1711329834.9246502}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.76031574+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4461458}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4153636, "arrival": 1711329833.3726435}, "models": {"yolo": {"arrival": 1711329833.258027, "serving": 1711329833.3569007}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.414812, "arrival": 1711329833.4042842}, "models": {"yolo": {"arrival": 1711329833.0720346, "serving": 1711329833.362891}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.015634316+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.4148397}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.01613683+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4454799}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4446938, "arrival": 1711329834.9315166}, "models": {"yolo": {"arrival": 1711329833.3712249, "serving": 1711329834.9129362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4456308, "arrival": 1711329834.9447584}, "models": {"yolo": {"arrival": 1711329833.3712249, "serving": 1711329834.9129362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4148984, "arrival": 1711329833.4190857}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.736237786+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.4458528}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.016228706+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4460495}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.73633058+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4459867}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4446166, "arrival": 1711329834.6997633}, "models": {"yolo": {"arrival": 1711329833.3819253, "serving": 1711329834.6670835}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4451375, "arrival": 1711329834.8654847}, "models": {"yolo": {"arrival": 1711329833.806287, "serving": 1711329834.8452475}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4452581, "arrival": 1711329834.8682091}, "models": {"yolo": {"arrival": 1711329833.806287, "serving": 1711329834.8452475}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4315448, "arrival": 1711329834.6942499}, "models": {"yolo": {"arrival": 1711329833.3819253, "serving": 1711329834.6670835}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4151206, "arrival": 1711329834.6927233}, "models": {"yolo": {"arrival": 1711329833.3819253, "serving": 1711329834.6670835}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4461763, "arrival": 1711329836.7080338}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445429, "arrival": 1711329834.9440632}, "models": {"yolo": {"arrival": 1711329833.3712249, "serving": 1711329834.9129362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4450796, "arrival": 1711329836.690346}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4456885, "arrival": 1711329836.72042}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.44531, "arrival": 1711329834.9422977}, "models": {"yolo": {"arrival": 1711329833.3712249, "serving": 1711329834.9129362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445445, "arrival": 1711329836.6962457}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4459224, "arrival": 1711329836.7210999}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4445791, "arrival": 1711329833.4212408}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.446292, "arrival": 1711329836.7083144}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4149256, "arrival": 1711329834.6919265}, "models": {"yolo": {"arrival": 1711329833.3819253, "serving": 1711329834.6670835}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.444965, "arrival": 1711329836.6897771}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4353642, "arrival": 1711329834.9296513}, "models": {"yolo": {"arrival": 1711329833.3712249, "serving": 1711329834.9129362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.431493, "arrival": 1711329833.400988}, "models": {"yolo": {"arrival": 1711329833.0675986, "serving": 1711329833.356877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4447865, "arrival": 1711329833.8096197}, "models": {"yolo": {"arrival": 1711329833.381424, "serving": 1711329833.7977877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445659, "arrival": 1711329834.9512699}, "models": {"yolo": {"arrival": 1711329834.2354183, "serving": 1711329834.9246502}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.415265, "arrival": 1711329833.4003956}, "models": {"yolo": {"arrival": 1711329833.0675986, "serving": 1711329833.356877}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.670568145+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.4149504}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.679940827+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.4448178}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.68008956+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.4452922}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.016050689+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.445109}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.015817151+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4314048}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4460835, "arrival": 1711329836.7214823}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4456456, "arrival": 1711329836.6971703}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4151921, "arrival": 1711329833.4028642}, "models": {"yolo": {"arrival": 1711329833.0724533, "serving": 1711329833.358283}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.679720277+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.4151447}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4459696, "arrival": 1711329835.3928912}, "models": {"yolo": {"arrival": 1711329835.0454247, "serving": 1711329835.367209}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4354346, "arrival": 1711329834.2491055}, "models": {"yolo": {"arrival": 1711329833.373555, "serving": 1711329834.2262323}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.015733087+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.415049}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4149747, "arrival": 1711329833.3713267}, "models": {"yolo": {"arrival": 1711329833.258027, "serving": 1711329833.3569007}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445937, "arrival": 1711329834.8725138}, "models": {"yolo": {"arrival": 1711329833.806287, "serving": 1711329834.8452475}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4458818, "arrival": 1711329836.6982696}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4154122, "arrival": 1711329833.4058607}, "models": {"yolo": {"arrival": 1711329833.0720346, "serving": 1711329833.362891}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4463081, "arrival": 1711329835.171735}, "models": {"yolo": {"arrival": 1711329834.9323068, "serving": 1711329835.1554377}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445154, "arrival": 1711329835.0569527}, "models": {"yolo": {"arrival": 1711329834.677225, "serving": 1711329835.0368218}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4315212, "arrival": 1711329833.4206119}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.43512, "arrival": 1711329836.683999}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4150963, "arrival": 1711329833.420187}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.016285984+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4832108}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4461286, "arrival": 1711329835.393759}, "models": {"yolo": {"arrival": 1711329835.0454247, "serving": 1711329835.367209}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4462457, "arrival": 1711329835.394436}, "models": {"yolo": {"arrival": 1711329835.0454247, "serving": 1711329835.367209}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4457731, "arrival": 1711329834.9524288}, "models": {"yolo": {"arrival": 1711329834.2354183, "serving": 1711329834.9246502}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445602, "arrival": 1711329835.0618567}, "models": {"yolo": {"arrival": 1711329834.677225, "serving": 1711329835.0368218}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.444979, "arrival": 1711329834.2511854}, "models": {"yolo": {"arrival": 1711329833.373555, "serving": 1711329834.2262323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4458675, "arrival": 1711329835.0579598}, "models": {"yolo": {"arrival": 1711329834.9231625, "serving": 1711329835.0412843}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.015956835+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4447508}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.015858929+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4349248}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.679873244+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.4353359}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.444832, "arrival": 1711329834.9324937}, "models": {"yolo": {"arrival": 1711329833.3712249, "serving": 1711329834.9129362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.434832, "arrival": 1711329833.411343}, "models": {"yolo": {"arrival": 1711329833.0724533, "serving": 1711329833.358283}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4450371, "arrival": 1711329835.0541728}, "models": {"yolo": {"arrival": 1711329834.677225, "serving": 1711329835.0368218}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4833877, "arrival": 1711329835.0426915}, "models": {"yolo": {"arrival": 1711329834.8535542, "serving": 1711329835.0277374}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4453259, "arrival": 1711329836.6914032}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445837, "arrival": 1711329835.3830311}, "models": {"yolo": {"arrival": 1711329835.0454247, "serving": 1711329835.367209}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4453707, "arrival": 1711329836.7197645}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4350855, "arrival": 1711329834.9288037}, "models": {"yolo": {"arrival": 1711329833.3712249, "serving": 1711329834.9129362}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.679896268+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4446566}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.680136934+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.445617}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.445715, "arrival": 1711329835.3820524}, "models": {"yolo": {"arrival": 1711329835.0454247, "serving": 1711329835.367209}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4451969, "arrival": 1711329836.6908133}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4349797, "arrival": 1711329833.4208272}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.44495, "arrival": 1711329834.9391935}, "models": {"yolo": {"arrival": 1711329833.3712249, "serving": 1711329834.9129362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445183, "arrival": 1711329834.9416149}, "models": {"yolo": {"arrival": 1711329833.3712249, "serving": 1711329834.9129362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4451234, "arrival": 1711329836.7108536}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.015893423+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.435193}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4457424, "arrival": 1711329834.9506633}, "models": {"yolo": {"arrival": 1711329833.3712249, "serving": 1711329834.9129362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4461613, "arrival": 1711329835.058853}, "models": {"yolo": {"arrival": 1711329834.9231625, "serving": 1711329835.0412843}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4153152, "arrival": 1711329834.6933568}, "models": {"yolo": {"arrival": 1711329833.3819253, "serving": 1711329834.6670835}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.760406914+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.484549}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4462183, "arrival": 1711329836.7217956}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.446034, "arrival": 1711329835.1686022}, "models": {"yolo": {"arrival": 1711329834.9323068, "serving": 1711329835.1554377}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.680112906+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4454129}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4454966, "arrival": 1711329836.7201014}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445895, "arrival": 1711329835.166049}, "models": {"yolo": {"arrival": 1711329834.9323068, "serving": 1711329835.1554377}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.435402, "arrival": 1711329836.6846232}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4450939, "arrival": 1711329834.251775}, "models": {"yolo": {"arrival": 1711329833.373555, "serving": 1711329834.2262323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4460194, "arrival": 1711329836.698773}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4449213, "arrival": 1711329835.0534759}, "models": {"yolo": {"arrival": 1711329834.677225, "serving": 1711329835.0368218}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4353054, "arrival": 1711329834.6986973}, "models": {"yolo": {"arrival": 1711329833.3819253, "serving": 1711329834.6670835}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.679796309+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4153397}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4452755, "arrival": 1711329835.057511}, "models": {"yolo": {"arrival": 1711329834.677225, "serving": 1711329835.0368218}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4845016, "arrival": 1711329835.699023}, "models": {"yolo": {"arrival": 1711329835.3760912, "serving": 1711329835.6837482}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4447334, "arrival": 1711329834.249874}, "models": {"yolo": {"arrival": 1711329833.373555, "serving": 1711329834.2262323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4445379, "arrival": 1711329836.6628995}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445821, "arrival": 1711329834.8718233}, "models": {"yolo": {"arrival": 1711329833.806287, "serving": 1711329834.8452475}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4453986, "arrival": 1711329835.061347}, "models": {"yolo": {"arrival": 1711329834.677225, "serving": 1711329835.0368218}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4462316, "arrival": 1711329835.041738}, "models": {"yolo": {"arrival": 1711329834.8535542, "serving": 1711329835.0277374}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4458048, "arrival": 1711329836.7206933}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4455862, "arrival": 1711329834.8696122}, "models": {"yolo": {"arrival": 1711329833.806287, "serving": 1711329834.8452475}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.680016084+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.4450512}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4449055, "arrival": 1711329833.8123121}, "models": {"yolo": {"arrival": 1711329833.381424, "serving": 1711329833.7977877}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4352307, "arrival": 1711329836.6622055}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4448607, "arrival": 1711329834.2505329}, "models": {"yolo": {"arrival": 1711329833.373555, "serving": 1711329834.2262323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4351547, "arrival": 1711329834.2444153}, "models": {"yolo": {"arrival": 1711329833.373555, "serving": 1711329834.2262323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4152157, "arrival": 1711329833.4053886}, "models": {"yolo": {"arrival": 1711329833.0720346, "serving": 1711329833.362891}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4447694, "arrival": 1711329836.7092426}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445023, "arrival": 1711329833.8131406}, "models": {"yolo": {"arrival": 1711329833.381424, "serving": 1711329833.7977877}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.681333106+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4457293}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4833364, "arrival": 1711329836.7221665}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.434879, "arrival": 1711329833.4063203}, "models": {"yolo": {"arrival": 1711329833.0720346, "serving": 1711329833.362891}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.015988346+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.4448745}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.434728, "arrival": 1711329833.3996654}, "models": {"yolo": {"arrival": 1711329833.258027, "serving": 1711329833.3569007}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4454625, "arrival": 1711329834.949933}, "models": {"yolo": {"arrival": 1711329834.2354183, "serving": 1711329834.9246502}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.016158738+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.445673}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4448028, "arrival": 1711329834.700388}, "models": {"yolo": {"arrival": 1711329833.3819253, "serving": 1711329834.6670835}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4462764, "arrival": 1711329835.0608284}, "models": {"yolo": {"arrival": 1711329834.9231625, "serving": 1711329835.0412843}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4450653, "arrival": 1711329834.9400167}, "models": {"yolo": {"arrival": 1711329833.3712249, "serving": 1711329834.9129362}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.016087169+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.445224}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4153876, "arrival": 1711329833.4033456}, "models": {"yolo": {"arrival": 1711329833.0724533, "serving": 1711329833.358283}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.679980049+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.444935}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4152899, "arrival": 1711329833.420399}, "models": {"yolo": {"arrival": 1711329832.628175, "serving": 1711329833.3639674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445756, "arrival": 1711329836.6977372}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.016180187+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.4457893}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.445241, "arrival": 1711329836.7152054}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.679823479+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.431566}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.016114067+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329829.4453573}}, "outputs": []}, {"times": {"request": {"sending": 1711329829.4461114, "arrival": 1711329835.0408294}, "models": {"yolo": {"arrival": 1711329834.8535542, "serving": 1711329835.0277374}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.445384, "arrival": 1711329834.8689759}, "models": {"yolo": {"arrival": 1711329833.806287, "serving": 1711329834.8452475}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329829.4460027, "arrival": 1711329835.0584254}, "models": {"yolo": {"arrival": 1711329834.9231625, "serving": 1711329835.0412843}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.47 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.680052449+00:00\"}\"\n>", "times": {"request": {"sending": 1711329829.445168}}, "outputs": []}], [{"times": {"request": {"sending": 1711329830.449627, "arrival": 1711329836.7369323}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.016569027+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4321368}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4322631, "arrival": 1711329835.7332106}, "models": {"yolo": {"arrival": 1711329835.5380158, "serving": 1711329835.69695}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.760454496+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4100096}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.448842, "arrival": 1711329836.7275455}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.760431351+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.409687}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4102447, "arrival": 1711329835.4739206}, "models": {"yolo": {"arrival": 1711329835.0376112, "serving": 1711329835.4595685}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4492817, "arrival": 1711329836.7409675}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.016342209+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4098527}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.409565, "arrival": 1711329836.7225792}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4492958, "arrival": 1711329835.7355502}, "models": {"yolo": {"arrival": 1711329835.4707553, "serving": 1711329835.6917527}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4320815, "arrival": 1711329835.6312242}, "models": {"yolo": {"arrival": 1711329835.0522103, "serving": 1711329835.602074}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.760610024+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4322083}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4097328, "arrival": 1711329835.6176581}, "models": {"yolo": {"arrival": 1711329835.0522103, "serving": 1711329835.602074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4491885, "arrival": 1711329836.4883773}, "models": {"yolo": {"arrival": 1711329836.1236088, "serving": 1711329836.4445198}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4492364, "arrival": 1711329836.7319157}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4104667, "arrival": 1711329835.5450952}, "models": {"yolo": {"arrival": 1711329835.1651838, "serving": 1711329835.5284393}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4093082, "arrival": 1711329835.6131253}, "models": {"yolo": {"arrival": 1711329835.0522103, "serving": 1711329835.602074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4487774, "arrival": 1711329836.1468196}, "models": {"yolo": {"arrival": 1711329835.6929402, "serving": 1711329836.1120496}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.047808144+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.449885}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.03832691+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4495149}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:51.502202022+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4494503}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4098134, "arrival": 1711329835.5438724}, "models": {"yolo": {"arrival": 1711329835.1651838, "serving": 1711329835.5284393}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4094062, "arrival": 1711329836.7085803}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4101274, "arrival": 1711329835.5446472}, "models": {"yolo": {"arrival": 1711329835.1651838, "serving": 1711329835.5284393}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.47395, "arrival": 1711329836.73763}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4320986, "arrival": 1711329836.72666}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4489915, "arrival": 1711329836.7311819}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.44931, "arrival": 1711329836.4888144}, "models": {"yolo": {"arrival": 1711329836.1236088, "serving": 1711329836.4445198}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4297905, "arrival": 1711329835.7182608}, "models": {"yolo": {"arrival": 1711329835.3760912, "serving": 1711329835.6837482}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4487338, "arrival": 1711329836.7371953}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.430113, "arrival": 1711329835.4804056}, "models": {"yolo": {"arrival": 1711329835.0376112, "serving": 1711329835.4595685}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:51.502223683+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4495897}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.760675741+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.448954}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.432225, "arrival": 1711329835.63173}, "models": {"yolo": {"arrival": 1711329835.0522103, "serving": 1711329835.602074}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.052965819+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4755845}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.016637144+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4487145}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4494982, "arrival": 1711329836.1650443}, "models": {"yolo": {"arrival": 1711329835.7083795, "serving": 1711329836.137463}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.02539876+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4491456}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4094677, "arrival": 1711329835.17244}, "models": {"yolo": {"arrival": 1711329834.9323068, "serving": 1711329835.1554377}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4109635, "arrival": 1711329835.474187}, "models": {"yolo": {"arrival": 1711329835.0376112, "serving": 1711329835.4595685}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4497788, "arrival": 1711329836.1488664}, "models": {"yolo": {"arrival": 1711329835.7023299, "serving": 1711329836.1225119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.016393282+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.410511}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4096475, "arrival": 1711329835.7025325}, "models": {"yolo": {"arrival": 1711329835.3760912, "serving": 1711329835.6837482}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.016321139+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4095194}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4321153, "arrival": 1711329835.5520542}, "models": {"yolo": {"arrival": 1711329835.1651838, "serving": 1711329835.5284393}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.760631927+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4485734}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4275653, "arrival": 1711329836.715644}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4097736, "arrival": 1711329836.7088032}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4298382, "arrival": 1711329835.627758}, "models": {"yolo": {"arrival": 1711329835.0522103, "serving": 1711329835.602074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4300528, "arrival": 1711329835.5502808}, "models": {"yolo": {"arrival": 1711329835.1651838, "serving": 1711329835.5284393}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.409892, "arrival": 1711329836.7263803}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.760698438+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.449087}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4739296, "arrival": 1711329836.4094343}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4320197, "arrival": 1711329835.480684}, "models": {"yolo": {"arrival": 1711329835.0376112, "serving": 1711329835.4595685}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4275408, "arrival": 1711329835.625832}, "models": {"yolo": {"arrival": 1711329835.0522103, "serving": 1711329835.602074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4299655, "arrival": 1711329836.123158}, "models": {"yolo": {"arrival": 1711329835.6929402, "serving": 1711329836.1120496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4323335, "arrival": 1711329836.1428385}, "models": {"yolo": {"arrival": 1711329835.6929402, "serving": 1711329836.1120496}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.01647499+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4299016}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4100885, "arrival": 1711329836.7090232}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.016428505+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4276102}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4096072, "arrival": 1711329835.0514383}, "models": {"yolo": {"arrival": 1711329834.8535542, "serving": 1711329835.0277374}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4495518, "arrival": 1711329836.14847}, "models": {"yolo": {"arrival": 1711329835.7023299, "serving": 1711329836.1225119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4103653, "arrival": 1711329835.620716}, "models": {"yolo": {"arrival": 1711329835.0522103, "serving": 1711329835.602074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4321728, "arrival": 1711329835.482199}, "models": {"yolo": {"arrival": 1711329835.0376112, "serving": 1711329835.4595685}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4102054, "arrival": 1711329836.727759}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4298596, "arrival": 1711329836.718977}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4490707, "arrival": 1711329836.4875662}, "models": {"yolo": {"arrival": 1711329836.1236088, "serving": 1711329836.4445198}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.760545273+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4299843}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4756508, "arrival": 1711329836.1557324}, "models": {"yolo": {"arrival": 1711329835.7023299, "serving": 1711329836.1225119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:51.502179526+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4493284}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4300938, "arrival": 1711329836.7324371}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4100497, "arrival": 1711329835.6198876}, "models": {"yolo": {"arrival": 1711329835.0522103, "serving": 1711329835.602074}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.016363462+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4101665}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:51.502275852+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4738557}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4323, "arrival": 1711329836.7342155}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4104207, "arrival": 1711329836.714973}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4492528, "arrival": 1711329835.7365835}, "models": {"yolo": {"arrival": 1711329835.5380158, "serving": 1711329835.69695}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.448821, "arrival": 1711329835.7229135}, "models": {"yolo": {"arrival": 1711329835.6108532, "serving": 1711329835.690246}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.449799, "arrival": 1711329836.5163279}, "models": {"yolo": {"arrival": 1711329836.1236088, "serving": 1711329836.4445198}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4499195, "arrival": 1711329836.1553383}, "models": {"yolo": {"arrival": 1711329835.7023299, "serving": 1711329836.1225119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4490557, "arrival": 1711329835.7327437}, "models": {"yolo": {"arrival": 1711329835.4707553, "serving": 1711329835.6917527}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.409971, "arrival": 1711329835.7037008}, "models": {"yolo": {"arrival": 1711329835.3760912, "serving": 1711329835.6837482}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.448917, "arrival": 1711329835.7324598}, "models": {"yolo": {"arrival": 1711329835.4707553, "serving": 1711329835.6917527}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4099314, "arrival": 1711329835.473472}, "models": {"yolo": {"arrival": 1711329835.0376112, "serving": 1711329835.4595685}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.448647, "arrival": 1711329835.7223763}, "models": {"yolo": {"arrival": 1711329835.6108532, "serving": 1711329835.690246}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4756289, "arrival": 1711329836.7469642}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4320407, "arrival": 1711329836.127706}, "models": {"yolo": {"arrival": 1711329835.6929402, "serving": 1711329836.1120496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4299264, "arrival": 1711329836.7285495}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4499028, "arrival": 1711329836.7467527}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.429946, "arrival": 1711329835.4794252}, "models": {"yolo": {"arrival": 1711329835.0376112, "serving": 1711329835.4595685}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4492218, "arrival": 1711329836.4026284}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4491737, "arrival": 1711329835.7353363}, "models": {"yolo": {"arrival": 1711329835.4707553, "serving": 1711329835.6917527}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4497247, "arrival": 1711329836.1662848}, "models": {"yolo": {"arrival": 1711329835.7083795, "serving": 1711329836.137463}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4276314, "arrival": 1711329836.728217}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.760522951+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4298158}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4494023, "arrival": 1711329836.7438629}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4494643, "arrival": 1711329836.4077168}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4319239, "arrival": 1711329835.6301732}, "models": {"yolo": {"arrival": 1711329835.0522103, "serving": 1711329835.602074}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:51.502293427+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4756885}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4320016, "arrival": 1711329836.7329712}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4498372, "arrival": 1711329836.4088457}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4109054, "arrival": 1711329836.7279885}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.016535766+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4319806}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4322414, "arrival": 1711329836.7269046}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.016948524+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4488811}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4321892, "arrival": 1711329836.141585}, "models": {"yolo": {"arrival": 1711329835.6929402, "serving": 1711329836.1120496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4494307, "arrival": 1711329836.4892197}, "models": {"yolo": {"arrival": 1711329836.1236088, "serving": 1711329836.4445198}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4487522, "arrival": 1711329835.7321918}, "models": {"yolo": {"arrival": 1711329835.4707553, "serving": 1711329835.6917527}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.760477589+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4103248}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4490418, "arrival": 1711329836.7405667}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4491315, "arrival": 1711329835.7363563}, "models": {"yolo": {"arrival": 1711329835.5380158, "serving": 1711329835.69695}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.760566558+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4318745}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4491596, "arrival": 1711329836.740769}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4102852, "arrival": 1711329835.704577}, "models": {"yolo": {"arrival": 1711329835.3760912, "serving": 1711329835.6837482}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4490094, "arrival": 1711329835.7351265}, "models": {"yolo": {"arrival": 1711329835.5380158, "serving": 1711329835.69695}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4499352, "arrival": 1711329836.7066965}, "models": {"yolo": {"arrival": 1711329836.4526856, "serving": 1711329836.6740572}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4756703, "arrival": 1711329836.7069879}, "models": {"yolo": {"arrival": 1711329836.4526856, "serving": 1711329836.6740572}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4496098, "arrival": 1711329836.4082837}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4319623, "arrival": 1711329835.5508056}, "models": {"yolo": {"arrival": 1711329835.1651838, "serving": 1711329835.5284393}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.448694, "arrival": 1711329835.7334049}, "models": {"yolo": {"arrival": 1711329835.5380158, "serving": 1711329835.69695}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.016602837+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4322798}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.7605889+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.432058}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4494162, "arrival": 1711329836.1479535}, "models": {"yolo": {"arrival": 1711329835.7023299, "serving": 1711329836.1225119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:51.502243381+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4498196}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.44987, "arrival": 1711329836.1667974}, "models": {"yolo": {"arrival": 1711329835.7083795, "serving": 1711329836.137463}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.432317, "arrival": 1711329835.731699}, "models": {"yolo": {"arrival": 1711329835.4707553, "serving": 1711329835.6917527}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.429881, "arrival": 1711329835.549886}, "models": {"yolo": {"arrival": 1711329835.1651838, "serving": 1711329835.5284393}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.449571, "arrival": 1711329836.5158956}, "models": {"yolo": {"arrival": 1711329836.1236088, "serving": 1711329836.4445198}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:51.502117055+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4492044}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4319444, "arrival": 1711329836.7260118}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.042474986+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.449745}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.016496218+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.430072}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4489355, "arrival": 1711329836.1474154}, "models": {"yolo": {"arrival": 1711329835.6929402, "serving": 1711329836.1120496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4491172, "arrival": 1711329836.7316191}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4489737, "arrival": 1711329835.7233055}, "models": {"yolo": {"arrival": 1711329835.6108532, "serving": 1711329835.690246}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4488633, "arrival": 1711329835.7348845}, "models": {"yolo": {"arrival": 1711329835.5380158, "serving": 1711329835.69695}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4495347, "arrival": 1711329836.7462142}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.034098811+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4493852}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4486701, "arrival": 1711329836.7272189}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.019734173+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4490268}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4300332, "arrival": 1711329836.719343}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.449357, "arrival": 1711329836.732196}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.473967, "arrival": 1711329836.1704712}, "models": {"yolo": {"arrival": 1711329835.7083795, "serving": 1711329836.137463}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.427419, "arrival": 1711329835.7174072}, "models": {"yolo": {"arrival": 1711329835.3760912, "serving": 1711329835.6837482}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4493432, "arrival": 1711329836.4035676}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:50.760501209+00:00\"}\"\n>", "times": {"request": {"sending": 1711329830.4275112}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4297223, "arrival": 1711329835.4790084}, "models": {"yolo": {"arrival": 1711329835.0376112, "serving": 1711329835.4595685}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.449854, "arrival": 1711329836.7374177}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:50.76065277+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.48 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4487994}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.4493704, "arrival": 1711329836.1646955}, "models": {"yolo": {"arrival": 1711329835.7083795, "serving": 1711329836.137463}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4300127, "arrival": 1711329835.6296444}, "models": {"yolo": {"arrival": 1711329835.0522103, "serving": 1711329835.602074}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.03045182+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329830.4492674}}, "outputs": []}, {"times": {"request": {"sending": 1711329830.449762, "arrival": 1711329836.746488}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4494786, "arrival": 1711329836.7326524}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.449102, "arrival": 1711329835.7237284}, "models": {"yolo": {"arrival": 1711329835.6108532, "serving": 1711329835.690246}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4301322, "arrival": 1711329836.126503}, "models": {"yolo": {"arrival": 1711329835.6929402, "serving": 1711329836.1120496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4488995, "arrival": 1711329836.7402456}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4321558, "arrival": 1711329836.733868}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329830.4275877, "arrival": 1711329835.549398}, "models": {"yolo": {"arrival": 1711329835.1651838, "serving": 1711329835.5284393}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329831.4233515, "arrival": 1711329836.410112}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.055040105+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.4235027}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4536958, "arrival": 1711329837.658048}, "models": {"yolo": {"arrival": 1711329837.2748463, "serving": 1711329837.638615}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4627914, "arrival": 1711329836.7664514}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4622495, "arrival": 1711329836.7659552}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4369757, "arrival": 1711329837.3022308}, "models": {"yolo": {"arrival": 1711329836.6864748, "serving": 1711329837.2652447}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.437251, "arrival": 1711329837.3054338}, "models": {"yolo": {"arrival": 1711329836.6864748, "serving": 1711329837.2652447}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.437166, "arrival": 1711329836.749649}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.462483, "arrival": 1711329837.1136572}, "models": {"yolo": {"arrival": 1711329836.672381, "serving": 1711329837.0882285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4536371, "arrival": 1711329836.8164392}, "models": {"yolo": {"arrival": 1711329836.5242937, "serving": 1711329836.796791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4235497, "arrival": 1711329836.1573117}, "models": {"yolo": {"arrival": 1711329835.7023299, "serving": 1711329836.1225119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.423477, "arrival": 1711329836.1707354}, "models": {"yolo": {"arrival": 1711329835.7083795, "serving": 1711329836.137463}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.424133, "arrival": 1711329836.7154248}, "models": {"yolo": {"arrival": 1711329836.4526856, "serving": 1711329836.6740572}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4235268, "arrival": 1711329836.7518792}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4238608, "arrival": 1711329836.171179}, "models": {"yolo": {"arrival": 1711329835.7083795, "serving": 1711329836.137463}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4234457, "arrival": 1711329836.7396734}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4370942, "arrival": 1711329836.6994038}, "models": {"yolo": {"arrival": 1711329836.4013088, "serving": 1711329836.6635664}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:51.502311993+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.4235964}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.453666, "arrival": 1711329836.7650418}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4238386, "arrival": 1711329836.7436}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4624655, "arrival": 1711329836.7662084}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4533906, "arrival": 1711329836.755232}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4239278, "arrival": 1711329836.1643476}, "models": {"yolo": {"arrival": 1711329835.7023299, "serving": 1711329836.1225119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.453085, "arrival": 1711329836.806159}, "models": {"yolo": {"arrival": 1711329836.5242937, "serving": 1711329836.796791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.436475, "arrival": 1711329836.7446141}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.46693, "arrival": 1711329837.1723838}, "models": {"yolo": {"arrival": 1711329836.8045022, "serving": 1711329837.1477091}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4669728, "arrival": 1711329837.0274417}, "models": {"yolo": {"arrival": 1711329836.6729646, "serving": 1711329837.0141153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4236462, "arrival": 1711329836.7400286}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4237688, "arrival": 1711329836.7076266}, "models": {"yolo": {"arrival": 1711329836.4526856, "serving": 1711329836.6740572}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4235733, "arrival": 1711329836.707293}, "models": {"yolo": {"arrival": 1711329836.4526856, "serving": 1711329836.6740572}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4367013, "arrival": 1711329837.284708}, "models": {"yolo": {"arrival": 1711329836.6864748, "serving": 1711329837.2652447}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4535353, "arrival": 1711329835.0451205}, "models": {"yolo": {"arrival": 1711329834.1380365, "serving": 1711329835.0265148}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4370093, "arrival": 1711329836.6808143}, "models": {"yolo": {"arrival": 1711329836.3732188, "serving": 1711329836.6488366}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.147705388+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.4368556}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.099123026+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.4367867}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4670084, "arrival": 1711329838.0166004}, "models": {"yolo": {"arrival": 1711329837.6457245, "serving": 1711329838.002212}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.140344173+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.4239736}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4368217, "arrival": 1711329836.4221172}, "models": {"yolo": {"arrival": 1711329836.131628, "serving": 1711329836.3920896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4624317, "arrival": 1711329836.8179674}, "models": {"yolo": {"arrival": 1711329836.5242937, "serving": 1711329836.796791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4530618, "arrival": 1711329836.7550182}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4628565, "arrival": 1711329837.4688942}, "models": {"yolo": {"arrival": 1711329837.0714188, "serving": 1711329837.4217556}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4369586, "arrival": 1711329836.6990669}, "models": {"yolo": {"arrival": 1711329836.4013088, "serving": 1711329836.6635664}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4535642, "arrival": 1711329837.1090271}, "models": {"yolo": {"arrival": 1711329836.672381, "serving": 1711329837.0882285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4372845, "arrival": 1711329836.682421}, "models": {"yolo": {"arrival": 1711329836.3732188, "serving": 1711329836.6488366}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4624152, "arrival": 1711329836.7606184}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.147750262+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.4369917}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.095575924+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.43665}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.108958316+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.4370592}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4368384, "arrival": 1711329837.2855554}, "models": {"yolo": {"arrival": 1711329836.6864748, "serving": 1711329837.2652447}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.424021, "arrival": 1711329836.7441895}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4238157, "arrival": 1711329836.4156694}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.147614809+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.4365826}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4626853, "arrival": 1711329838.013687}, "models": {"yolo": {"arrival": 1711329837.6457245, "serving": 1711329838.002212}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4370425, "arrival": 1711329836.5371187}, "models": {"yolo": {"arrival": 1711329836.1528952, "serving": 1711329836.5157983}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.150473784+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.4537103}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4534624, "arrival": 1711329837.6557634}, "models": {"yolo": {"arrival": 1711329837.2748463, "serving": 1711329837.638615}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.06436616+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.423699}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4627223, "arrival": 1711329837.0842357}, "models": {"yolo": {"arrival": 1711329836.6567607, "serving": 1711329837.0620987}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4242246, "arrival": 1711329836.171881}, "models": {"yolo": {"arrival": 1711329835.7083795, "serving": 1711329836.137463}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.423623, "arrival": 1711329836.4107752}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4369414, "arrival": 1711329836.7600262}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.147794557+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.4371295}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4537404, "arrival": 1711329836.7603714}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4236693, "arrival": 1711329836.1709611}, "models": {"yolo": {"arrival": 1711329835.7083795, "serving": 1711329836.137463}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.453519, "arrival": 1711329836.815719}, "models": {"yolo": {"arrival": 1711329836.5242937, "serving": 1711329836.796791}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.150497135+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.4623709}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.147503824+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.4241557}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.068695482+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.4238834}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4363942, "arrival": 1711329836.421032}, "models": {"yolo": {"arrival": 1711329836.131628, "serving": 1711329836.3920896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4536803, "arrival": 1711329837.1099138}, "models": {"yolo": {"arrival": 1711329836.672381, "serving": 1711329837.0882285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4368033, "arrival": 1711329836.7585566}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.121390938+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.4373355}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.453505, "arrival": 1711329836.7558792}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4372349, "arrival": 1711329836.699834}, "models": {"yolo": {"arrival": 1711329836.4013088, "serving": 1711329836.6635664}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.140247161+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.4237921}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4537253, "arrival": 1711329837.0808496}, "models": {"yolo": {"arrival": 1711329836.6567607, "serving": 1711329837.0620987}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4364946, "arrival": 1711329836.5356789}, "models": {"yolo": {"arrival": 1711329836.1528952, "serving": 1711329836.5157983}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.453491, "arrival": 1711329837.078912}, "models": {"yolo": {"arrival": 1711329836.6567607, "serving": 1711329837.0620987}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4239507, "arrival": 1711329836.714667}, "models": {"yolo": {"arrival": 1711329836.4526856, "serving": 1711329836.6740572}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4531212, "arrival": 1711329836.7633612}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4368737, "arrival": 1711329836.6724677}, "models": {"yolo": {"arrival": 1711329836.3732188, "serving": 1711329836.6488366}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4367352, "arrival": 1711329836.4206252}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.423746, "arrival": 1711329836.1581442}, "models": {"yolo": {"arrival": 1711329835.7023299, "serving": 1711329836.1225119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4366846, "arrival": 1711329836.4214797}, "models": {"yolo": {"arrival": 1711329836.131628, "serving": 1711329836.3920896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4239056, "arrival": 1711329836.7524862}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4237232, "arrival": 1711329836.7521377}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.147926289+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.453357}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.074154033+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.4240656}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.11724631+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.4372013}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.436317, "arrival": 1711329836.754804}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4372177, "arrival": 1711329836.7622793}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4373186, "arrival": 1711329836.5390012}, "models": {"yolo": {"arrival": 1711329836.1528952, "serving": 1711329836.5157983}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.147839372+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.4372666}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4370263, "arrival": 1711329836.749435}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.42418, "arrival": 1711329836.4198284}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.466953, "arrival": 1711329835.3977857}, "models": {"yolo": {"arrival": 1711329835.0342498, "serving": 1711329835.3661432}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4241111, "arrival": 1711329836.4208286}, "models": {"yolo": {"arrival": 1711329836.131628, "serving": 1711329836.3920896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.436617, "arrival": 1711329836.748713}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.147566401+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.436437}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4668608, "arrival": 1711329837.018752}, "models": {"yolo": {"arrival": 1711329836.6847014, "serving": 1711329837.0064054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4627738, "arrival": 1711329835.3950498}, "models": {"yolo": {"arrival": 1711329835.0342498, "serving": 1711329835.3661432}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4627397, "arrival": 1711329836.7608478}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4367697, "arrival": 1711329836.536486}, "models": {"yolo": {"arrival": 1711329836.1528952, "serving": 1711329836.5157983}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4669929, "arrival": 1711329837.1149356}, "models": {"yolo": {"arrival": 1711329836.672381, "serving": 1711329837.0882285}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.147656116+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.436718}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4373686, "arrival": 1711329836.70025}, "models": {"yolo": {"arrival": 1711329836.4013088, "serving": 1711329836.6635664}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.132057876+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.45342}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.147882637+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.4374037}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.152955824+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.4628417}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4240427, "arrival": 1711329836.171385}, "models": {"yolo": {"arrival": 1711329835.7083795, "serving": 1711329836.137463}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4239984, "arrival": 1711329836.4160964}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4373026, "arrival": 1711329836.7545872}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.153012366+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.4670274}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4364567, "arrival": 1711329836.4202063}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.07965945+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.424246}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.103562543+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.436924}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.436752, "arrival": 1711329836.7489653}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.15044899+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.453592}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4534338, "arrival": 1711329836.76371}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4371831, "arrival": 1711329836.5385826}, "models": {"yolo": {"arrival": 1711329836.1528952, "serving": 1711329836.5157983}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4537692, "arrival": 1711329835.0495358}, "models": {"yolo": {"arrival": 1711329834.1380365, "serving": 1711329835.0265148}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4534476, "arrival": 1711329836.7062457}, "models": {"yolo": {"arrival": 1711329836.4013088, "serving": 1711329836.6635664}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4240882, "arrival": 1711329836.7541928}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4535491, "arrival": 1711329836.7648003}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.086245723+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.4365125}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.462352, "arrival": 1711329838.011448}, "models": {"yolo": {"arrival": 1711329837.6457245, "serving": 1711329838.002212}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4371128, "arrival": 1711329837.304836}, "models": {"yolo": {"arrival": 1711329836.6864748, "serving": 1711329837.2652447}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.437352, "arrival": 1711329836.7625468}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.453608, "arrival": 1711329837.0799398}, "models": {"yolo": {"arrival": 1711329836.6567607, "serving": 1711329837.0620987}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4623969, "arrival": 1711329837.0830472}, "models": {"yolo": {"arrival": 1711329836.6567607, "serving": 1711329837.0620987}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.437148, "arrival": 1711329836.6817234}, "models": {"yolo": {"arrival": 1711329836.3732188, "serving": 1711329836.6488366}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4628222, "arrival": 1711329838.0144334}, "models": {"yolo": {"arrival": 1711329837.6457245, "serving": 1711329838.002212}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4537542, "arrival": 1711329836.8173518}, "models": {"yolo": {"arrival": 1711329836.5242937, "serving": 1711329836.796791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4364173, "arrival": 1711329837.280339}, "models": {"yolo": {"arrival": 1711329836.6864748, "serving": 1711329837.2652447}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4365654, "arrival": 1711329837.2835543}, "models": {"yolo": {"arrival": 1711329836.6864748, "serving": 1711329837.2652447}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.150372279+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329831.453476}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4366672, "arrival": 1711329836.758201}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4370759, "arrival": 1711329836.762021}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4242027, "arrival": 1711329836.7444053}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4535785, "arrival": 1711329837.6573572}, "models": {"yolo": {"arrival": 1711329837.2748463, "serving": 1711329837.638615}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.462448, "arrival": 1711329835.0505073}, "models": {"yolo": {"arrival": 1711329834.1380365, "serving": 1711329835.0265148}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.453622, "arrival": 1711329836.7561178}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4373863, "arrival": 1711329837.3061366}, "models": {"yolo": {"arrival": 1711329836.6864748, "serving": 1711329837.2652447}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4623306, "arrival": 1711329837.112941}, "models": {"yolo": {"arrival": 1711329836.672381, "serving": 1711329837.0882285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4369073, "arrival": 1711329836.5367832}, "models": {"yolo": {"arrival": 1711329836.1528952, "serving": 1711329836.5157983}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 148.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.126002159+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.4531033}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4365313, "arrival": 1711329836.7578433}, "models": {"yolo": {"arrival": 1711329833.405906, "serving": 1711329836.6434119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4529848, "arrival": 1711329836.6833587}, "models": {"yolo": {"arrival": 1711329836.3732188, "serving": 1711329836.6488366}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4531572, "arrival": 1711329837.6548746}, "models": {"yolo": {"arrival": 1711329837.2748463, "serving": 1711329837.638615}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4531422, "arrival": 1711329836.7058234}, "models": {"yolo": {"arrival": 1711329836.4013088, "serving": 1711329836.6635664}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4368908, "arrival": 1711329836.7491899}, "models": {"yolo": {"arrival": 1711329833.4002728, "serving": 1711329836.651044}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.56 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.150860266+00:00\"}\"\n>", "times": {"request": {"sending": 1711329831.4627051}}, "outputs": []}, {"times": {"request": {"sending": 1711329831.4536521, "arrival": 1711329835.0487554}, "models": {"yolo": {"arrival": 1711329834.1380365, "serving": 1711329835.0265148}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4366007, "arrival": 1711329836.4204223}, "models": {"yolo": {"arrival": 1711329835.7029908, "serving": 1711329836.3617404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4366326, "arrival": 1711329836.5361817}, "models": {"yolo": {"arrival": 1711329836.1528952, "serving": 1711329836.5157983}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.462807, "arrival": 1711329837.1142993}, "models": {"yolo": {"arrival": 1711329836.672381, "serving": 1711329837.0882285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4365487, "arrival": 1711329836.4212377}, "models": {"yolo": {"arrival": 1711329836.131628, "serving": 1711329836.3920896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4627564, "arrival": 1711329837.1678233}, "models": {"yolo": {"arrival": 1711329836.8045022, "serving": 1711329837.1477091}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4533753, "arrival": 1711329837.075626}, "models": {"yolo": {"arrival": 1711329836.6567607, "serving": 1711329837.0620987}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329831.4534056, "arrival": 1711329836.8120792}, "models": {"yolo": {"arrival": 1711329836.5242937, "serving": 1711329836.796791}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329832.4171057, "arrival": 1711329838.4828393}, "models": {"yolo": {"arrival": 1711329838.0116768, "serving": 1711329838.4215727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4167213, "arrival": 1711329838.019147}, "models": {"yolo": {"arrival": 1711329837.6457245, "serving": 1711329838.002212}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4177396, "arrival": 1711329837.743666}, "models": {"yolo": {"arrival": 1711329837.4295928, "serving": 1711329837.7248447}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5735855, "arrival": 1711329838.2254176}, "models": {"yolo": {"arrival": 1711329837.9820685, "serving": 1711329838.2142165}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5727935, "arrival": 1711329837.8184297}, "models": {"yolo": {"arrival": 1711329837.3593354, "serving": 1711329837.8022344}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.416306, "arrival": 1711329837.4693468}, "models": {"yolo": {"arrival": 1711329837.0714188, "serving": 1711329837.4217556}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4167933, "arrival": 1711329837.4916363}, "models": {"yolo": {"arrival": 1711329837.0140393, "serving": 1711329837.4745853}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5732884, "arrival": 1711329838.3529434}, "models": {"yolo": {"arrival": 1711329837.7721272, "serving": 1711329838.3334308}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.416599, "arrival": 1711329837.0219445}, "models": {"yolo": {"arrival": 1711329836.6847014, "serving": 1711329837.0064054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4173083, "arrival": 1711329838.4830863}, "models": {"yolo": {"arrival": 1711329838.0116768, "serving": 1711329838.4215727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4164984, "arrival": 1711329837.390994}, "models": {"yolo": {"arrival": 1711329837.0968091, "serving": 1711329837.3768618}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5737133, "arrival": 1711329836.574017}, "models": {"yolo": {"arrival": 1711329836.1356883, "serving": 1711329836.5510316}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.751518789+00:00\"}\"\n>", "times": {"request": {"sending": 1711329832.4167457}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.4172838, "arrival": 1711329837.3972242}, "models": {"yolo": {"arrival": 1711329837.0968091, "serving": 1711329837.3768618}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4163835, "arrival": 1711329837.0210354}, "models": {"yolo": {"arrival": 1711329836.6847014, "serving": 1711329837.0064054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.572778, "arrival": 1711329836.1510782}, "models": {"yolo": {"arrival": 1711329835.694921, "serving": 1711329836.1210525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5726087, "arrival": 1711329837.854814}, "models": {"yolo": {"arrival": 1711329837.483481, "serving": 1711329837.82387}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4165235, "arrival": 1711329838.0172389}, "models": {"yolo": {"arrival": 1711329837.6457245, "serving": 1711329838.002212}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4170341, "arrival": 1711329835.7189176}, "models": {"yolo": {"arrival": 1711329835.374463, "serving": 1711329835.6847072}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4168663, "arrival": 1711329837.0307884}, "models": {"yolo": {"arrival": 1711329836.6729646, "serving": 1711329837.0141153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5730426, "arrival": 1711329837.8219147}, "models": {"yolo": {"arrival": 1711329837.3593354, "serving": 1711329837.8022344}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.552078, "arrival": 1711329837.4167833}, "models": {"yolo": {"arrival": 1711329837.1565566, "serving": 1711329837.3937206}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.755804833+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329832.4171298}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.763295569+00:00\"}\"\n>", "times": {"request": {"sending": 1711329832.5732005}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.4175985, "arrival": 1711329837.4162982}, "models": {"yolo": {"arrival": 1711329837.1565566, "serving": 1711329837.3937206}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4170105, "arrival": 1711329837.1760669}, "models": {"yolo": {"arrival": 1711329836.8045022, "serving": 1711329837.1477091}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.552227, "arrival": 1711329837.7997963}, "models": {"yolo": {"arrival": 1711329837.385844, "serving": 1711329837.762955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573685, "arrival": 1711329838.10985}, "models": {"yolo": {"arrival": 1711329837.832329, "serving": 1711329838.0853634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.417405, "arrival": 1711329837.415722}, "models": {"yolo": {"arrival": 1711329837.1565566, "serving": 1711329837.3937206}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.417622, "arrival": 1711329835.7204888}, "models": {"yolo": {"arrival": 1711329835.374463, "serving": 1711329835.6847072}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5724392, "arrival": 1711329838.9044366}, "models": {"yolo": {"arrival": 1711329838.4375465, "serving": 1711329838.8761592}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5731444, "arrival": 1711329836.1597667}, "models": {"yolo": {"arrival": 1711329835.694921, "serving": 1711329836.1210525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4173572, "arrival": 1711329837.736105}, "models": {"yolo": {"arrival": 1711329837.4295928, "serving": 1711329837.7248447}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4166968, "arrival": 1711329837.3919322}, "models": {"yolo": {"arrival": 1711329837.0968091, "serving": 1711329837.3768618}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.416473, "arrival": 1711329837.0285094}, "models": {"yolo": {"arrival": 1711329836.6729646, "serving": 1711329837.0141153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4175498, "arrival": 1711329837.7428043}, "models": {"yolo": {"arrival": 1711329837.4295928, "serving": 1711329837.7248447}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.552317, "arrival": 1711329835.7330036}, "models": {"yolo": {"arrival": 1711329835.374463, "serving": 1711329835.6847072}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4175014, "arrival": 1711329838.483313}, "models": {"yolo": {"arrival": 1711329838.0116768, "serving": 1711329838.4215727}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.755830202+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329832.4173324}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.4176455, "arrival": 1711329837.3726094}, "models": {"yolo": {"arrival": 1711329837.0230103, "serving": 1711329837.3505092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4168177, "arrival": 1711329837.1752715}, "models": {"yolo": {"arrival": 1711329836.8045022, "serving": 1711329837.1477091}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.759949847+00:00\"}\"\n>", "times": {"request": {"sending": 1711329832.5725763}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.4172552, "arrival": 1711329837.3688715}, "models": {"yolo": {"arrival": 1711329837.0230103, "serving": 1711329837.3505092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4164467, "arrival": 1711329835.3985462}, "models": {"yolo": {"arrival": 1711329835.0342498, "serving": 1711329835.3661432}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4164174, "arrival": 1711329837.1732206}, "models": {"yolo": {"arrival": 1711329836.8045022, "serving": 1711329837.1477091}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5730567, "arrival": 1711329838.3515384}, "models": {"yolo": {"arrival": 1711329837.7721272, "serving": 1711329838.3334308}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4169629, "arrival": 1711329837.4701328}, "models": {"yolo": {"arrival": 1711329837.0714188, "serving": 1711329837.4217556}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5522113, "arrival": 1711329837.3732889}, "models": {"yolo": {"arrival": 1711329837.0230103, "serving": 1711329837.3505092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5725296, "arrival": 1711329837.8178127}, "models": {"yolo": {"arrival": 1711329837.3593354, "serving": 1711329837.8022344}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.751415511+00:00\"}\"\n>", "times": {"request": {"sending": 1711329832.4165485}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.416574, "arrival": 1711329837.469591}, "models": {"yolo": {"arrival": 1711329837.0714188, "serving": 1711329837.4217556}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5736425, "arrival": 1711329839.2311544}, "models": {"yolo": {"arrival": 1711329838.8847709, "serving": 1711329839.1848402}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5731716, "arrival": 1711329838.3522782}, "models": {"yolo": {"arrival": 1711329837.7721272, "serving": 1711329838.3334308}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.416624, "arrival": 1711329837.1741319}, "models": {"yolo": {"arrival": 1711329836.8045022, "serving": 1711329837.1477091}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4173813, "arrival": 1711329837.4932778}, "models": {"yolo": {"arrival": 1711329837.0140393, "serving": 1711329837.4745853}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4175744, "arrival": 1711329837.4937193}, "models": {"yolo": {"arrival": 1711329837.0140393, "serving": 1711329837.4745853}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.572914, "arrival": 1711329837.8212626}, "models": {"yolo": {"arrival": 1711329837.3593354, "serving": 1711329837.8022344}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4174287, "arrival": 1711329835.7201147}, "models": {"yolo": {"arrival": 1711329835.374463, "serving": 1711329835.6847072}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573274, "arrival": 1711329838.133867}, "models": {"yolo": {"arrival": 1711329837.811214, "serving": 1711329838.1229203}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.755747663+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329832.4169383}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.5729787, "arrival": 1711329838.1063209}, "models": {"yolo": {"arrival": 1711329837.733612, "serving": 1711329838.0838733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4166727, "arrival": 1711329837.0294416}, "models": {"yolo": {"arrival": 1711329836.6729646, "serving": 1711329837.0141153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5732293, "arrival": 1711329838.0944548}, "models": {"yolo": {"arrival": 1711329837.832329, "serving": 1711329838.0853634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4166484, "arrival": 1711329835.3992808}, "models": {"yolo": {"arrival": 1711329835.0342498, "serving": 1711329835.3661432}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5730257, "arrival": 1711329836.1578066}, "models": {"yolo": {"arrival": 1711329835.694921, "serving": 1711329836.1210525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4167695, "arrival": 1711329837.4698112}, "models": {"yolo": {"arrival": 1711329837.0714188, "serving": 1711329837.4217556}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5730999, "arrival": 1711329838.1087475}, "models": {"yolo": {"arrival": 1711329837.733612, "serving": 1711329838.0838733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4170582, "arrival": 1711329837.3640742}, "models": {"yolo": {"arrival": 1711329837.0230103, "serving": 1711329837.3505092}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.755876877+00:00\"}\"\n>", "times": {"request": {"sending": 1711329832.4177158}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.4168422, "arrival": 1711329835.399877}, "models": {"yolo": {"arrival": 1711329835.0342498, "serving": 1711329835.3661432}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4171546, "arrival": 1711329837.4703465}, "models": {"yolo": {"arrival": 1711329837.0714188, "serving": 1711329837.4217556}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4172027, "arrival": 1711329837.4148834}, "models": {"yolo": {"arrival": 1711329837.1565566, "serving": 1711329837.3937206}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.572824, "arrival": 1711329838.9059346}, "models": {"yolo": {"arrival": 1711329838.4375465, "serving": 1711329838.8761592}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.759847337+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329832.5522573}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.5522728, "arrival": 1711329837.7453396}, "models": {"yolo": {"arrival": 1711329837.4295928, "serving": 1711329837.7248447}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.417669, "arrival": 1711329837.7990682}, "models": {"yolo": {"arrival": 1711329837.385844, "serving": 1711329837.762955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.41689, "arrival": 1711329837.3943233}, "models": {"yolo": {"arrival": 1711329837.0968091, "serving": 1711329837.3768618}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5728717, "arrival": 1711329837.8557103}, "models": {"yolo": {"arrival": 1711329837.483481, "serving": 1711329837.82387}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5725443, "arrival": 1711329837.8049538}, "models": {"yolo": {"arrival": 1711329837.385844, "serving": 1711329837.762955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5734167, "arrival": 1711329839.2301145}, "models": {"yolo": {"arrival": 1711329838.8847709, "serving": 1711329839.1848402}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 23.64 GiB total capacity; 1.62 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 23.64 GiB total capacity; 1.62 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:53.488269999+00:00\"}\"\n>", "times": {"request": {"sending": 1711329832.5736573}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.572514, "arrival": 1711329836.1496754}, "models": {"yolo": {"arrival": 1711329835.694921, "serving": 1711329836.1210525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573699, "arrival": 1711329838.2261312}, "models": {"yolo": {"arrival": 1711329837.9820685, "serving": 1711329838.2142165}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.755854048+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329832.4175253}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.5521922, "arrival": 1711329835.731266}, "models": {"yolo": {"arrival": 1711329835.374463, "serving": 1711329835.6847072}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 23.64 GiB total capacity; 1.62 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:53.488294981+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 23.64 GiB total capacity; 1.62 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329832.5737698}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.5734866, "arrival": 1711329836.5723429}, "models": {"yolo": {"arrival": 1711329836.1356883, "serving": 1711329836.5510316}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4176927, "arrival": 1711329838.4835408}, "models": {"yolo": {"arrival": 1711329838.0116768, "serving": 1711329838.4215727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5733871, "arrival": 1711329838.1374738}, "models": {"yolo": {"arrival": 1711329837.811214, "serving": 1711329838.1229203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5737274, "arrival": 1711329838.1412601}, "models": {"yolo": {"arrival": 1711329837.811214, "serving": 1711329838.1229203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.572499, "arrival": 1711329837.9909406}, "models": {"yolo": {"arrival": 1711329837.4052439, "serving": 1711329837.9740438}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.416987, "arrival": 1711329837.4920735}, "models": {"yolo": {"arrival": 1711329837.0140393, "serving": 1711329837.4745853}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.416914, "arrival": 1711329838.482433}, "models": {"yolo": {"arrival": 1711329838.0116768, "serving": 1711329838.4215727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4174528, "arrival": 1711329837.369717}, "models": {"yolo": {"arrival": 1711329837.0230103, "serving": 1711329837.3505092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5522869, "arrival": 1711329837.504402}, "models": {"yolo": {"arrival": 1711329837.0140393, "serving": 1711329837.4745853}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4170818, "arrival": 1711329837.3950846}, "models": {"yolo": {"arrival": 1711329837.0968091, "serving": 1711329837.3768618}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 23.64 GiB total capacity; 1.62 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 23.64 GiB total capacity; 1.62 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:53.488241872+00:00\"}\"\n>", "times": {"request": {"sending": 1711329832.5735428}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.5731297, "arrival": 1711329837.9987645}, "models": {"yolo": {"arrival": 1711329837.4052439, "serving": 1711329837.9740438}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573303, "arrival": 1711329839.2296004}, "models": {"yolo": {"arrival": 1711329838.8847709, "serving": 1711329839.1848402}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.417231, "arrival": 1711329835.7196994}, "models": {"yolo": {"arrival": 1711329835.374463, "serving": 1711329835.6847072}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.572407, "arrival": 1711329837.8170524}, "models": {"yolo": {"arrival": 1711329837.3593354, "serving": 1711329837.8022344}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4174776, "arrival": 1711329837.7962737}, "models": {"yolo": {"arrival": 1711329837.385844, "serving": 1711329837.762955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5730708, "arrival": 1711329838.907534}, "models": {"yolo": {"arrival": 1711329838.4375465, "serving": 1711329838.8761592}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573629, "arrival": 1711329838.3583117}, "models": {"yolo": {"arrival": 1711329837.7721272, "serving": 1711329838.3334308}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5724835, "arrival": 1711329837.853765}, "models": {"yolo": {"arrival": 1711329837.483481, "serving": 1711329837.82387}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5730078, "arrival": 1711329837.998058}, "models": {"yolo": {"arrival": 1711329837.4052439, "serving": 1711329837.9740438}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5723715, "arrival": 1711329837.989138}, "models": {"yolo": {"arrival": 1711329837.4052439, "serving": 1711329837.9740438}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5522418, "arrival": 1711329838.4837546}, "models": {"yolo": {"arrival": 1711329838.0116768, "serving": 1711329838.4215727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4171786, "arrival": 1711329837.4923217}, "models": {"yolo": {"arrival": 1711329837.0140393, "serving": 1711329837.4745853}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5732436, "arrival": 1711329837.9992435}, "models": {"yolo": {"arrival": 1711329837.4052439, "serving": 1711329837.9740438}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.4177632, "arrival": 1711329837.4946241}, "models": {"yolo": {"arrival": 1711329837.0140393, "serving": 1711329837.4745853}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5731149, "arrival": 1711329837.856899}, "models": {"yolo": {"arrival": 1711329837.483481, "serving": 1711329837.82387}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5728576, "arrival": 1711329838.105791}, "models": {"yolo": {"arrival": 1711329837.733612, "serving": 1711329838.0838733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.57247, "arrival": 1711329838.1035793}, "models": {"yolo": {"arrival": 1711329837.733612, "serving": 1711329838.0838733}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.763318699+00:00\"}\"\n>", "times": {"request": {"sending": 1711329832.573317}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.5723052, "arrival": 1711329837.8490133}, "models": {"yolo": {"arrival": 1711329837.483481, "serving": 1711329837.82387}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573373, "arrival": 1711329836.1606145}, "models": {"yolo": {"arrival": 1711329835.694921, "serving": 1711329836.1210525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5736003, "arrival": 1711329836.5733027}, "models": {"yolo": {"arrival": 1711329836.1356883, "serving": 1711329836.5510316}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573671, "arrival": 1711329838.4947107}, "models": {"yolo": {"arrival": 1711329838.0932302, "serving": 1711329838.4533775}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5733447, "arrival": 1711329838.0967028}, "models": {"yolo": {"arrival": 1711329837.832329, "serving": 1711329838.0853634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573402, "arrival": 1711329838.356448}, "models": {"yolo": {"arrival": 1711329837.7721272, "serving": 1711329838.3334308}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5729434, "arrival": 1711329838.9066448}, "models": {"yolo": {"arrival": 1711329838.4375465, "serving": 1711329838.8761592}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5733585, "arrival": 1711329837.9996831}, "models": {"yolo": {"arrival": 1711329837.4052439, "serving": 1711329837.9740438}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5733309, "arrival": 1711329838.4940917}, "models": {"yolo": {"arrival": 1711329838.0932302, "serving": 1711329838.4533775}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5725918, "arrival": 1711329838.104303}, "models": {"yolo": {"arrival": 1711329837.733612, "serving": 1711329838.0838733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5737417, "arrival": 1711329838.4839673}, "models": {"yolo": {"arrival": 1711329838.3425584, "serving": 1711329838.4402504}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5683842, "arrival": 1711329837.7460139}, "models": {"yolo": {"arrival": 1711329837.4295928, "serving": 1711329837.7248447}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5735707, "arrival": 1711329838.1028826}, "models": {"yolo": {"arrival": 1711329837.832329, "serving": 1711329838.0853634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5723908, "arrival": 1711329836.1492946}, "models": {"yolo": {"arrival": 1711329835.694921, "serving": 1711329836.1210525}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.763192454+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329832.5728385}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.5729923, "arrival": 1711329837.8563163}, "models": {"yolo": {"arrival": 1711329837.483481, "serving": 1711329837.82387}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5731578, "arrival": 1711329837.822524}, "models": {"yolo": {"arrival": 1711329837.3593354, "serving": 1711329837.8022344}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.759926121+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329832.5724542}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.5728998, "arrival": 1711329836.1515882}, "models": {"yolo": {"arrival": 1711329835.694921, "serving": 1711329836.1210525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573755, "arrival": 1711329839.237862}, "models": {"yolo": {"arrival": 1711329838.8847709, "serving": 1711329839.1848402}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5728858, "arrival": 1711329837.994323}, "models": {"yolo": {"arrival": 1711329837.4052439, "serving": 1711329837.9740438}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:52.759899913+00:00\"}\"\n>", "times": {"request": {"sending": 1711329832.5683637}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.5732584, "arrival": 1711329836.1601276}, "models": {"yolo": {"arrival": 1711329835.694921, "serving": 1711329836.1210525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5523312, "arrival": 1711329837.8147202}, "models": {"yolo": {"arrival": 1711329837.3593354, "serving": 1711329837.8022344}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573459, "arrival": 1711329838.0973504}, "models": {"yolo": {"arrival": 1711329837.832329, "serving": 1711329838.0853634}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.763245105+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329832.5729613}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.5724227, "arrival": 1711329837.8040216}, "models": {"yolo": {"arrival": 1711329837.385844, "serving": 1711329837.762955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573501, "arrival": 1711329838.1382945}, "models": {"yolo": {"arrival": 1711329837.811214, "serving": 1711329838.1229203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573473, "arrival": 1711329838.222925}, "models": {"yolo": {"arrival": 1711329837.9820685, "serving": 1711329838.2142165}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5682583, "arrival": 1711329837.803316}, "models": {"yolo": {"arrival": 1711329837.385844, "serving": 1711329837.762955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5728092, "arrival": 1711329838.3468597}, "models": {"yolo": {"arrival": 1711329837.7721272, "serving": 1711329838.3334308}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573529, "arrival": 1711329839.2306175}, "models": {"yolo": {"arrival": 1711329838.8847709, "serving": 1711329839.1848402}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5523021, "arrival": 1711329837.4190044}, "models": {"yolo": {"arrival": 1711329837.1565566, "serving": 1711329837.3937206}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5731857, "arrival": 1711329839.207294}, "models": {"yolo": {"arrival": 1711329838.8847709, "serving": 1711329839.1848402}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5729294, "arrival": 1711329838.3480227}, "models": {"yolo": {"arrival": 1711329837.7721272, "serving": 1711329838.3334308}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5732152, "arrival": 1711329838.1094964}, "models": {"yolo": {"arrival": 1711329837.733612, "serving": 1711329838.0838733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.573557, "arrival": 1711329838.4945087}, "models": {"yolo": {"arrival": 1711329838.0932302, "serving": 1711329838.4533775}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5727615, "arrival": 1711329837.9936335}, "models": {"yolo": {"arrival": 1711329837.4052439, "serving": 1711329837.9740438}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5735145, "arrival": 1711329838.3574188}, "models": {"yolo": {"arrival": 1711329837.7721272, "serving": 1711329838.3334308}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 23.64 GiB total capacity; 1.62 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:53.488150161+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 23.64 GiB total capacity; 1.62 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329832.5734308}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.5736144, "arrival": 1711329838.1390052}, "models": {"yolo": {"arrival": 1711329837.811214, "serving": 1711329838.1229203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5725603, "arrival": 1711329838.9052901}, "models": {"yolo": {"arrival": 1711329838.4375465, "serving": 1711329838.8761592}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329832.5734441, "arrival": 1711329838.4942942}, "models": {"yolo": {"arrival": 1711329838.0932302, "serving": 1711329838.4533775}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:52.763270607+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.60 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329832.5730848}}, "outputs": []}, {"times": {"request": {"sending": 1711329832.5683415, "arrival": 1711329838.8996007}, "models": {"yolo": {"arrival": 1711329838.4375465, "serving": 1711329838.8761592}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329833.4483695, "arrival": 1711329839.2373648}, "models": {"yolo": {"arrival": 1711329838.4547613, "serving": 1711329839.1782691}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4319186, "arrival": 1711329840.0837486}, "models": {"yolo": {"arrival": 1711329839.211133, "serving": 1711329840.048507}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4439027, "arrival": 1711329839.233407}, "models": {"yolo": {"arrival": 1711329838.4547613, "serving": 1711329839.1782691}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.424933, "arrival": 1711329838.6117663}, "models": {"yolo": {"arrival": 1711329838.223513, "serving": 1711329838.601092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4248219, "arrival": 1711329838.2275243}, "models": {"yolo": {"arrival": 1711329837.9820685, "serving": 1711329838.2142165}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.424805, "arrival": 1711329838.4878454}, "models": {"yolo": {"arrival": 1711329838.0932872, "serving": 1711329838.443445}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4248362, "arrival": 1711329836.5749748}, "models": {"yolo": {"arrival": 1711329836.1356883, "serving": 1711329836.5510316}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.429432, "arrival": 1711329838.5034819}, "models": {"yolo": {"arrival": 1711329838.0932302, "serving": 1711329838.4533775}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4584587, "arrival": 1711329839.197175}, "models": {"yolo": {"arrival": 1711329839.0089245, "serving": 1711329839.171731}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4294508, "arrival": 1711329838.4884555}, "models": {"yolo": {"arrival": 1711329838.0932872, "serving": 1711329838.443445}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4439895, "arrival": 1711329838.942524}, "models": {"yolo": {"arrival": 1711329838.611194, "serving": 1711329838.9320157}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4485407, "arrival": 1711329838.9448497}, "models": {"yolo": {"arrival": 1711329838.611194, "serving": 1711329838.9320157}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4487123, "arrival": 1711329839.0185273}, "models": {"yolo": {"arrival": 1711329838.4653141, "serving": 1711329839.0008464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4316547, "arrival": 1711329838.493886}, "models": {"yolo": {"arrival": 1711329838.1315322, "serving": 1711329838.451798}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.458489, "arrival": 1711329840.7910874}, "models": {"yolo": {"arrival": 1711329840.404138, "serving": 1711329840.771535}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4440038, "arrival": 1711329836.8666162}, "models": {"yolo": {"arrival": 1711329836.561544, "serving": 1711329836.8408935}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4438634, "arrival": 1711329836.8657622}, "models": {"yolo": {"arrival": 1711329836.561544, "serving": 1711329836.8408935}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.117039435+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329833.4319358}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.119489466+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329833.4486322}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.4742165, "arrival": 1711329840.9495094}, "models": {"yolo": {"arrival": 1711329840.7798052, "serving": 1711329840.9402816}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.448729, "arrival": 1711329839.2392404}, "models": {"yolo": {"arrival": 1711329838.4547613, "serving": 1711329839.1782691}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.432008, "arrival": 1711329838.4979315}, "models": {"yolo": {"arrival": 1711329838.1315322, "serving": 1711329838.451798}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4248784, "arrival": 1711329840.07109}, "models": {"yolo": {"arrival": 1711329839.211133, "serving": 1711329840.048507}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4317179, "arrival": 1711329838.8631387}, "models": {"yolo": {"arrival": 1711329838.4615378, "serving": 1711329838.8469586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4583166, "arrival": 1711329839.1960802}, "models": {"yolo": {"arrival": 1711329839.0089245, "serving": 1711329839.171731}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.431848, "arrival": 1711329838.9167116}, "models": {"yolo": {"arrival": 1711329838.4533837, "serving": 1711329838.9039686}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.11719899+00:00\"}\"\n>", "times": {"request": {"sending": 1711329833.444067}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.4247458, "arrival": 1711329838.5029573}, "models": {"yolo": {"arrival": 1711329838.0932302, "serving": 1711329838.4533775}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4585435, "arrival": 1711329839.326115}, "models": {"yolo": {"arrival": 1711329838.9140296, "serving": 1711329839.2933054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4440496, "arrival": 1711329840.456473}, "models": {"yolo": {"arrival": 1711329840.057322, "serving": 1711329840.3958516}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4319656, "arrival": 1711329838.9185324}, "models": {"yolo": {"arrival": 1711329838.4533837, "serving": 1711329838.9039686}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4485748, "arrival": 1711329839.0155964}, "models": {"yolo": {"arrival": 1711329838.4653141, "serving": 1711329839.0008464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4438407, "arrival": 1711329838.6222813}, "models": {"yolo": {"arrival": 1711329838.223513, "serving": 1711329838.601092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4440975, "arrival": 1711329838.9213111}, "models": {"yolo": {"arrival": 1711329838.4533837, "serving": 1711329838.9039686}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4581513, "arrival": 1711329839.3249526}, "models": {"yolo": {"arrival": 1711329838.9140296, "serving": 1711329839.2933054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4248645, "arrival": 1711329838.4841828}, "models": {"yolo": {"arrival": 1711329838.3425584, "serving": 1711329838.4402504}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4249196, "arrival": 1711329838.4881346}, "models": {"yolo": {"arrival": 1711329838.0932872, "serving": 1711329838.443445}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4741242, "arrival": 1711329840.0779407}, "models": {"yolo": {"arrival": 1711329839.6990101, "serving": 1711329840.051467}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4248512, "arrival": 1711329838.141956}, "models": {"yolo": {"arrival": 1711329837.811214, "serving": 1711329838.1229203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4318624, "arrival": 1711329838.6191382}, "models": {"yolo": {"arrival": 1711329838.223513, "serving": 1711329838.601092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.458562, "arrival": 1711329839.23821}, "models": {"yolo": {"arrival": 1711329838.9401596, "serving": 1711329839.1894088}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.11965725+00:00\"}\"\n>", "times": {"request": {"sending": 1711329833.4581192}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.448558, "arrival": 1711329837.2903597}, "models": {"yolo": {"arrival": 1711329836.8505633, "serving": 1711329837.274658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4319506, "arrival": 1711329838.8645053}, "models": {"yolo": {"arrival": 1711329838.4615378, "serving": 1711329838.8469586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4281807, "arrival": 1711329838.4845123}, "models": {"yolo": {"arrival": 1711329838.3425584, "serving": 1711329838.4402504}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4281614, "arrival": 1711329838.493224}, "models": {"yolo": {"arrival": 1711329838.1315322, "serving": 1711329838.451798}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4487445, "arrival": 1711329840.4571617}, "models": {"yolo": {"arrival": 1711329840.057322, "serving": 1711329840.3958516}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.117097443+00:00\"}\"\n>", "times": {"request": {"sending": 1711329833.4320507}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.458, "arrival": 1711329842.3706355}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4579682, "arrival": 1711329840.7877436}, "models": {"yolo": {"arrival": 1711329840.404138, "serving": 1711329840.771535}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.119699358+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329833.458503}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.4281049, "arrival": 1711329836.5785642}, "models": {"yolo": {"arrival": 1711329836.1356883, "serving": 1711329836.5510316}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4439573, "arrival": 1711329839.1727614}, "models": {"yolo": {"arrival": 1711329838.8523993, "serving": 1711329839.1555114}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4580615, "arrival": 1711329839.194469}, "models": {"yolo": {"arrival": 1711329839.0089245, "serving": 1711329839.171731}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4319043, "arrival": 1711329839.2037652}, "models": {"yolo": {"arrival": 1711329838.4547613, "serving": 1711329839.1782691}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.431733, "arrival": 1711329838.4985514}, "models": {"yolo": {"arrival": 1711329838.0932872, "serving": 1711329838.443445}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.457936, "arrival": 1711329839.1906433}, "models": {"yolo": {"arrival": 1711329839.0089245, "serving": 1711329839.171731}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.10596921+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329833.431375}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.458474, "arrival": 1711329839.7127125}, "models": {"yolo": {"arrival": 1711329839.1917968, "serving": 1711329839.6920342}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4489431, "arrival": 1711329839.3219728}, "models": {"yolo": {"arrival": 1711329838.9140296, "serving": 1711329839.2933054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.432023, "arrival": 1711329839.233056}, "models": {"yolo": {"arrival": 1711329838.4547613, "serving": 1711329839.1782691}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4439738, "arrival": 1711329838.9208088}, "models": {"yolo": {"arrival": 1711329838.4533837, "serving": 1711329838.9039686}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4580307, "arrival": 1711329839.2318723}, "models": {"yolo": {"arrival": 1711329838.9401596, "serving": 1711329839.1894088}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.431409, "arrival": 1711329838.4975023}, "models": {"yolo": {"arrival": 1711329838.0932872, "serving": 1711329838.443445}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4318764, "arrival": 1711329836.862363}, "models": {"yolo": {"arrival": 1711329836.561544, "serving": 1711329836.8408935}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4585252, "arrival": 1711329842.3780372}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.458428, "arrival": 1711329839.2327008}, "models": {"yolo": {"arrival": 1711329838.9401596, "serving": 1711329839.1894088}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4312901, "arrival": 1711329838.6157422}, "models": {"yolo": {"arrival": 1711329838.223513, "serving": 1711329838.601092}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.119561013+00:00\"}\"\n>", "times": {"request": {"sending": 1711329833.4576244}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.43198, "arrival": 1711329838.6213322}, "models": {"yolo": {"arrival": 1711329838.223513, "serving": 1711329838.601092}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.119596944+00:00\"}\"\n>", "times": {"request": {"sending": 1711329833.4578488}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.431748, "arrival": 1711329838.618442}, "models": {"yolo": {"arrival": 1711329838.223513, "serving": 1711329838.601092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.448976, "arrival": 1711329837.2918262}, "models": {"yolo": {"arrival": 1711329836.8505633, "serving": 1711329837.274658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4583008, "arrival": 1711329837.3017197}, "models": {"yolo": {"arrival": 1711329836.8505633, "serving": 1711329837.274658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.431804, "arrival": 1711329840.0831923}, "models": {"yolo": {"arrival": 1711329839.211133, "serving": 1711329840.048507}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.105905867+00:00\"}\"\n>", "times": {"request": {"sending": 1711329833.4312377}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.448923, "arrival": 1711329842.3329408}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4317763, "arrival": 1711329838.4972353}, "models": {"yolo": {"arrival": 1711329838.1315322, "serving": 1711329838.451798}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4486165, "arrival": 1711329840.456944}, "models": {"yolo": {"arrival": 1711329840.057322, "serving": 1711329840.3958516}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.119679137+00:00\"}\"\n>", "times": {"request": {"sending": 1711329833.45836}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.4485972, "arrival": 1711329839.2389557}, "models": {"yolo": {"arrival": 1711329838.4547613, "serving": 1711329839.1782691}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4316711, "arrival": 1711329839.202445}, "models": {"yolo": {"arrival": 1711329838.4547613, "serving": 1711329839.1782691}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4580464, "arrival": 1711329837.3011222}, "models": {"yolo": {"arrival": 1711329836.8505633, "serving": 1711329837.274658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.431307, "arrival": 1711329836.5797477}, "models": {"yolo": {"arrival": 1711329836.1356883, "serving": 1711329836.5510316}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.121037095+00:00\"}\"\n>", "times": {"request": {"sending": 1711329833.4742434}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.4577162, "arrival": 1711329842.3338668}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.448991, "arrival": 1711329839.0193486}, "models": {"yolo": {"arrival": 1711329838.4653141, "serving": 1711329839.0008464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4486635, "arrival": 1711329839.315481}, "models": {"yolo": {"arrival": 1711329838.9140296, "serving": 1711329839.2933054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4312155, "arrival": 1711329840.076403}, "models": {"yolo": {"arrival": 1711329839.211133, "serving": 1711329840.048507}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4583306, "arrival": 1711329839.7120833}, "models": {"yolo": {"arrival": 1711329839.1917968, "serving": 1711329839.6920342}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.457952, "arrival": 1711329839.7078648}, "models": {"yolo": {"arrival": 1711329839.1917968, "serving": 1711329839.6920342}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4579196, "arrival": 1711329837.300578}, "models": {"yolo": {"arrival": 1711329836.8505633, "serving": 1711329837.274658}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.119438657+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329833.4484787}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.443753, "arrival": 1711329838.9192038}, "models": {"yolo": {"arrival": 1711329838.4533837, "serving": 1711329838.9039686}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 23.64 GiB total capacity; 1.62 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:53.48834467+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 23.64 GiB total capacity; 1.62 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329833.4293838}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.457745, "arrival": 1711329839.322753}, "models": {"yolo": {"arrival": 1711329838.9140296, "serving": 1711329839.2933054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4440823, "arrival": 1711329839.1745183}, "models": {"yolo": {"arrival": 1711329838.8523993, "serving": 1711329839.1555114}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.43114, "arrival": 1711329838.4934328}, "models": {"yolo": {"arrival": 1711329838.1315322, "serving": 1711329838.451798}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4577637, "arrival": 1711329838.9493258}, "models": {"yolo": {"arrival": 1711329838.611194, "serving": 1711329838.9320157}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4312742, "arrival": 1711329838.4921596}, "models": {"yolo": {"arrival": 1711329838.0932872, "serving": 1711329838.443445}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4489603, "arrival": 1711329838.948361}, "models": {"yolo": {"arrival": 1711329838.611194, "serving": 1711329838.9320157}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.458387, "arrival": 1711329839.325535}, "models": {"yolo": {"arrival": 1711329838.9140296, "serving": 1711329839.2933054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4441433, "arrival": 1711329839.0146563}, "models": {"yolo": {"arrival": 1711329838.4653141, "serving": 1711329839.0008464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4580762, "arrival": 1711329839.7113595}, "models": {"yolo": {"arrival": 1711329839.1917968, "serving": 1711329839.6920342}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4294937, "arrival": 1711329836.5792212}, "models": {"yolo": {"arrival": 1711329836.1356883, "serving": 1711329836.5510316}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4582849, "arrival": 1711329839.2323427}, "models": {"yolo": {"arrival": 1711329838.9401596, "serving": 1711329839.1894088}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4486797, "arrival": 1711329838.946436}, "models": {"yolo": {"arrival": 1711329838.611194, "serving": 1711329838.9320157}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4281983, "arrival": 1711329840.071958}, "models": {"yolo": {"arrival": 1711329839.211133, "serving": 1711329840.048507}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4316864, "arrival": 1711329840.0822845}, "models": {"yolo": {"arrival": 1711329839.211133, "serving": 1711329840.048507}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4441128, "arrival": 1711329838.9440634}, "models": {"yolo": {"arrival": 1711329838.611194, "serving": 1711329838.9320157}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4578166, "arrival": 1711329839.2397563}, "models": {"yolo": {"arrival": 1711329838.4547613, "serving": 1711329839.1782691}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4578338, "arrival": 1711329840.7870753}, "models": {"yolo": {"arrival": 1711329840.404138, "serving": 1711329840.771535}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4485216, "arrival": 1711329838.9221604}, "models": {"yolo": {"arrival": 1711329838.4533837, "serving": 1711329838.9039686}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4583454, "arrival": 1711329840.7896178}, "models": {"yolo": {"arrival": 1711329840.404138, "serving": 1711329840.771535}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.119630507+00:00\"}\"\n>", "times": {"request": {"sending": 1711329833.457985}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 23.64 GiB total capacity; 1.62 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:53.488322687+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 23.64 GiB total capacity; 1.62 GiB already allocated; 4.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329833.4248912}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.444128, "arrival": 1711329836.8673604}, "models": {"yolo": {"arrival": 1711329836.561544, "serving": 1711329836.8408935}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4580147, "arrival": 1711329839.324363}, "models": {"yolo": {"arrival": 1711329838.9140296, "serving": 1711329839.2933054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4440348, "arrival": 1711329839.2369616}, "models": {"yolo": {"arrival": 1711329838.4547613, "serving": 1711329839.1782691}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.432064, "arrival": 1711329839.169559}, "models": {"yolo": {"arrival": 1711329838.8523993, "serving": 1711329839.1555114}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4318902, "arrival": 1711329838.4977193}, "models": {"yolo": {"arrival": 1711329838.1315322, "serving": 1711329838.451798}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.448498, "arrival": 1711329839.175502}, "models": {"yolo": {"arrival": 1711329838.8523993, "serving": 1711329839.1555114}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4578855, "arrival": 1711329839.3236604}, "models": {"yolo": {"arrival": 1711329838.9140296, "serving": 1711329839.2933054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.431393, "arrival": 1711329838.8607063}, "models": {"yolo": {"arrival": 1711329838.4615378, "serving": 1711329838.8469586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4313233, "arrival": 1711329838.4936793}, "models": {"yolo": {"arrival": 1711329838.1315322, "serving": 1711329838.451798}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4579024, "arrival": 1711329839.2315147}, "models": {"yolo": {"arrival": 1711329838.9401596, "serving": 1711329839.1894088}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4490263, "arrival": 1711329840.7858443}, "models": {"yolo": {"arrival": 1711329840.404138, "serving": 1711329840.771535}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4585786, "arrival": 1711329837.716104}, "models": {"yolo": {"arrival": 1711329837.284848, "serving": 1711329837.6957893}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.431762, "arrival": 1711329836.8598073}, "models": {"yolo": {"arrival": 1711329836.561544, "serving": 1711329836.8408935}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4249058, "arrival": 1711329838.5032415}, "models": {"yolo": {"arrival": 1711329838.0932302, "serving": 1711329838.4533775}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4578648, "arrival": 1711329842.3353508}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4316356, "arrival": 1711329836.5802822}, "models": {"yolo": {"arrival": 1711329836.1356883, "serving": 1711329836.5510316}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.119525567+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329833.4489062}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.117141052+00:00\"}\"\n>", "times": {"request": {"sending": 1711329833.4439375}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.4584424, "arrival": 1711329837.3066835}, "models": {"yolo": {"arrival": 1711329836.8505633, "serving": 1711329837.274658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.458598, "arrival": 1711329840.2453697}, "models": {"yolo": {"arrival": 1711329839.1807413, "serving": 1711329840.2101297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.448456, "arrival": 1711329840.4567187}, "models": {"yolo": {"arrival": 1711329840.057322, "serving": 1711329840.3958516}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4440186, "arrival": 1711329839.0135105}, "models": {"yolo": {"arrival": 1711329838.4653141, "serving": 1711329839.0008464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4313567, "arrival": 1711329840.077127}, "models": {"yolo": {"arrival": 1711329839.211133, "serving": 1711329840.048507}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4318328, "arrival": 1711329838.8638947}, "models": {"yolo": {"arrival": 1711329838.4615378, "serving": 1711329838.8469586}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4439197, "arrival": 1711329840.4560757}, "models": {"yolo": {"arrival": 1711329840.057322, "serving": 1711329840.3958516}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.448648, "arrival": 1711329842.3318338}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4314249, "arrival": 1711329838.6164706}, "models": {"yolo": {"arrival": 1711329838.223513, "serving": 1711329838.601092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4438822, "arrival": 1711329839.0096903}, "models": {"yolo": {"arrival": 1711329838.4653141, "serving": 1711329839.0008464}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.116901617+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329833.431703}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.4311929, "arrival": 1711329838.4847322}, "models": {"yolo": {"arrival": 1711329838.3425584, "serving": 1711329838.4402504}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4581044, "arrival": 1711329840.7884}, "models": {"yolo": {"arrival": 1711329840.404138, "serving": 1711329840.771535}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4317899, "arrival": 1711329839.2031956}, "models": {"yolo": {"arrival": 1711329838.4547613, "serving": 1711329839.1782691}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.457782, "arrival": 1711329837.2999363}, "models": {"yolo": {"arrival": 1711329836.8505633, "serving": 1711329837.274658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4583735, "arrival": 1711329842.3776355}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.116984983+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.67 GiB already allocated; 92.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329833.4318192}}, "outputs": []}, {"times": {"request": {"sending": 1711329833.4486969, "arrival": 1711329837.291165}, "models": {"yolo": {"arrival": 1711329836.8505633, "serving": 1711329837.274658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.457799, "arrival": 1711329839.0198734}, "models": {"yolo": {"arrival": 1711329838.4653141, "serving": 1711329839.0008464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4490094, "arrival": 1711329839.2394974}, "models": {"yolo": {"arrival": 1711329838.4547613, "serving": 1711329839.1782691}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4320354, "arrival": 1711329840.0842857}, "models": {"yolo": {"arrival": 1711329839.211133, "serving": 1711329840.048507}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4319932, "arrival": 1711329836.8631706}, "models": {"yolo": {"arrival": 1711329836.561544, "serving": 1711329836.8408935}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4313405, "arrival": 1711329839.2013454}, "models": {"yolo": {"arrival": 1711329838.4547613, "serving": 1711329839.1782691}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4581342, "arrival": 1711329842.3770432}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4294748, "arrival": 1711329838.6150105}, "models": {"yolo": {"arrival": 1711329838.223513, "serving": 1711329838.601092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329833.4312572, "arrival": 1711329838.5037117}, "models": {"yolo": {"arrival": 1711329838.0932302, "serving": 1711329838.4533775}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329834.4479012, "arrival": 1711329840.5832903}, "models": {"yolo": {"arrival": 1711329840.2187097, "serving": 1711329840.5729434}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4476736, "arrival": 1711329840.5811093}, "models": {"yolo": {"arrival": 1711329840.2187097, "serving": 1711329840.5729434}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.449229, "arrival": 1711329842.4028478}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4391623, "arrival": 1711329838.0873318}, "models": {"yolo": {"arrival": 1711329837.7041295, "serving": 1711329838.0708926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4340954, "arrival": 1711329840.457797}, "models": {"yolo": {"arrival": 1711329840.058783, "serving": 1711329840.406752}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4490948, "arrival": 1711329842.399786}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.621429405+00:00\"}\"\n>", "times": {"request": {"sending": 1711329834.4390998}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.4771397, "arrival": 1711329835.668948}, "models": {"yolo": {"arrival": 1711329835.440096, "serving": 1711329835.658191}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.41052, "arrival": 1711329840.2473383}, "models": {"yolo": {"arrival": 1711329839.1807413, "serving": 1711329840.2101297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4492831, "arrival": 1711329838.9115796}, "models": {"yolo": {"arrival": 1711329838.4516127, "serving": 1711329838.8866918}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.607368513+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329834.4102924}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.449044, "arrival": 1711329841.6752596}, "models": {"yolo": {"arrival": 1711329841.5365846, "serving": 1711329841.6212306}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4340332, "arrival": 1711329839.8981404}, "models": {"yolo": {"arrival": 1711329839.2010665, "serving": 1711329839.8711727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4101763, "arrival": 1711329840.2467453}, "models": {"yolo": {"arrival": 1711329839.1807413, "serving": 1711329840.2101297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4392557, "arrival": 1711329840.2444372}, "models": {"yolo": {"arrival": 1711329839.7462654, "serving": 1711329840.2104304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4101372, "arrival": 1711329837.7216034}, "models": {"yolo": {"arrival": 1711329837.284848, "serving": 1711329837.6957893}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.410334, "arrival": 1711329842.3912282}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4109137, "arrival": 1711329840.955167}, "models": {"yolo": {"arrival": 1711329840.7798052, "serving": 1711329840.9402816}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4097164, "arrival": 1711329839.7495015}, "models": {"yolo": {"arrival": 1711329839.3042457, "serving": 1711329839.737967}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.621612771+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329834.44867}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.4096043, "arrival": 1711329842.390478}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4480522, "arrival": 1711329840.272651}, "models": {"yolo": {"arrival": 1711329839.8792582, "serving": 1711329840.255313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4340117, "arrival": 1711329840.238077}, "models": {"yolo": {"arrival": 1711329839.7462654, "serving": 1711329840.2104304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.44925, "arrival": 1711329840.931479}, "models": {"yolo": {"arrival": 1711329840.582549, "serving": 1711329840.9138508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4102526, "arrival": 1711329840.953254}, "models": {"yolo": {"arrival": 1711329840.7798052, "serving": 1711329840.9402816}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4477565, "arrival": 1711329840.2706807}, "models": {"yolo": {"arrival": 1711329839.8792582, "serving": 1711329840.255313}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.607421599+00:00\"}\"\n>", "times": {"request": {"sending": 1711329834.410953}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.4771159, "arrival": 1711329842.3243084}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4097698, "arrival": 1711329839.2385795}, "models": {"yolo": {"arrival": 1711329838.9401596, "serving": 1711329839.1894088}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4477797, "arrival": 1711329838.0904782}, "models": {"yolo": {"arrival": 1711329837.7041295, "serving": 1711329838.0708926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4107575, "arrival": 1711329839.8951547}, "models": {"yolo": {"arrival": 1711329839.2010665, "serving": 1711329839.8711727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4770265, "arrival": 1711329841.003383}, "models": {"yolo": {"arrival": 1711329840.7272606, "serving": 1711329840.9850633}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4107966, "arrival": 1711329837.7230806}, "models": {"yolo": {"arrival": 1711329837.284848, "serving": 1711329837.6957893}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4485133, "arrival": 1711329840.7380114}, "models": {"yolo": {"arrival": 1711329840.416586, "serving": 1711329840.7196167}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4491274, "arrival": 1711329841.1004636}, "models": {"yolo": {"arrival": 1711329840.588141, "serving": 1711329841.091083}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4389882, "arrival": 1711329840.2388396}, "models": {"yolo": {"arrival": 1711329839.7462654, "serving": 1711329840.2104304}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.621698917+00:00\"}\"\n>", "times": {"request": {"sending": 1711329834.449215}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.4391932, "arrival": 1711329840.4582124}, "models": {"yolo": {"arrival": 1711329840.058783, "serving": 1711329840.406752}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4108748, "arrival": 1711329840.4573746}, "models": {"yolo": {"arrival": 1711329840.058783, "serving": 1711329840.406752}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.409815, "arrival": 1711329837.7206297}, "models": {"yolo": {"arrival": 1711329837.284848, "serving": 1711329837.6957893}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4105978, "arrival": 1711329840.9539156}, "models": {"yolo": {"arrival": 1711329840.7798052, "serving": 1711329840.9402816}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4391472, "arrival": 1711329840.2696195}, "models": {"yolo": {"arrival": 1711329839.8792582, "serving": 1711329840.255313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4111109, "arrival": 1711329837.7239492}, "models": {"yolo": {"arrival": 1711329837.284848, "serving": 1711329837.6957893}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4341145, "arrival": 1711329841.5424497}, "models": {"yolo": {"arrival": 1711329840.9506905, "serving": 1711329841.5259905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4491448, "arrival": 1711329838.9110587}, "models": {"yolo": {"arrival": 1711329838.4516127, "serving": 1711329838.8866918}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4110339, "arrival": 1711329840.2371943}, "models": {"yolo": {"arrival": 1711329839.7462654, "serving": 1711329840.2104304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4098577, "arrival": 1711329840.2461057}, "models": {"yolo": {"arrival": 1711329839.1807413, "serving": 1711329840.2101297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4100213, "arrival": 1711329842.391016}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4339895, "arrival": 1711329842.3946426}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4098988, "arrival": 1711329840.078632}, "models": {"yolo": {"arrival": 1711329839.6990101, "serving": 1711329840.051467}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4102147, "arrival": 1711329840.0791936}, "models": {"yolo": {"arrival": 1711329839.6990101, "serving": 1711329840.051467}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.439491, "arrival": 1711329842.395554}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4481456, "arrival": 1711329842.3983996}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4100604, "arrival": 1711329839.7503428}, "models": {"yolo": {"arrival": 1711329839.3042457, "serving": 1711329839.737967}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4099402, "arrival": 1711329840.9524417}, "models": {"yolo": {"arrival": 1711329840.7798052, "serving": 1711329840.9402816}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4390233, "arrival": 1711329838.084864}, "models": {"yolo": {"arrival": 1711329837.7041295, "serving": 1711329838.0708926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4489756, "arrival": 1711329840.6078074}, "models": {"yolo": {"arrival": 1711329840.2659144, "serving": 1711329840.5788538}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.439007, "arrival": 1711329839.898821}, "models": {"yolo": {"arrival": 1711329839.2010665, "serving": 1711329839.8711727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.448847, "arrival": 1711329840.6072896}, "models": {"yolo": {"arrival": 1711329840.2659144, "serving": 1711329840.5788538}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.607292947+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329834.4099796}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.4484253, "arrival": 1711329842.3986132}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4491985, "arrival": 1711329842.3211973}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4390743, "arrival": 1711329841.5431745}, "models": {"yolo": {"arrival": 1711329840.9506905, "serving": 1711329841.5259905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.448034, "arrival": 1711329840.589394}, "models": {"yolo": {"arrival": 1711329840.2187097, "serving": 1711329840.5729434}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.448621, "arrival": 1711329840.8790967}, "models": {"yolo": {"arrival": 1711329840.4049969, "serving": 1711329840.8637116}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4339116, "arrival": 1711329840.457585}, "models": {"yolo": {"arrival": 1711329840.058783, "serving": 1711329840.406752}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4486356, "arrival": 1711329840.9949994}, "models": {"yolo": {"arrival": 1711329840.7272606, "serving": 1711329840.9850633}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4491625, "arrival": 1711329840.884082}, "models": {"yolo": {"arrival": 1711329840.4049969, "serving": 1711329840.8637116}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4390426, "arrival": 1711329840.2519271}, "models": {"yolo": {"arrival": 1711329839.1807413, "serving": 1711329840.2101297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.410373, "arrival": 1711329839.7510586}, "models": {"yolo": {"arrival": 1711329839.3042457, "serving": 1711329839.737967}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4340546, "arrival": 1711329838.0836864}, "models": {"yolo": {"arrival": 1711329837.7041295, "serving": 1711329838.0708926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4487028, "arrival": 1711329840.924292}, "models": {"yolo": {"arrival": 1711329840.582549, "serving": 1711329840.9138508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4487376, "arrival": 1711329838.4927151}, "models": {"yolo": {"arrival": 1711329838.0790591, "serving": 1711329838.4412844}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4489567, "arrival": 1711329840.9286659}, "models": {"yolo": {"arrival": 1711329840.582549, "serving": 1711329840.9138508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4492989, "arrival": 1711329841.2665353}, "models": {"yolo": {"arrival": 1711329840.8750172, "serving": 1711329841.2527542}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4107184, "arrival": 1711329839.7519379}, "models": {"yolo": {"arrival": 1711329839.3042457, "serving": 1711329839.737967}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4392715, "arrival": 1711329840.2702022}, "models": {"yolo": {"arrival": 1711329839.8792582, "serving": 1711329840.255313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4340746, "arrival": 1711329840.2515354}, "models": {"yolo": {"arrival": 1711329839.1807413, "serving": 1711329840.2101297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4104798, "arrival": 1711329837.7224634}, "models": {"yolo": {"arrival": 1711329837.284848, "serving": 1711329837.6957893}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4490273, "arrival": 1711329841.000362}, "models": {"yolo": {"arrival": 1711329840.7272606, "serving": 1711329840.9850633}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4392102, "arrival": 1711329841.5535986}, "models": {"yolo": {"arrival": 1711329840.9506905, "serving": 1711329841.5259905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4100993, "arrival": 1711329839.8936172}, "models": {"yolo": {"arrival": 1711329839.2010665, "serving": 1711329839.8711727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4392407, "arrival": 1711329842.395346}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.449181, "arrival": 1711329841.0024054}, "models": {"yolo": {"arrival": 1711329840.7272606, "serving": 1711329840.9850633}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4104393, "arrival": 1711329839.8942614}, "models": {"yolo": {"arrival": 1711329839.2010665, "serving": 1711329839.8711727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4487703, "arrival": 1711329840.998598}, "models": {"yolo": {"arrival": 1711329840.7272606, "serving": 1711329840.9850633}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.448607, "arrival": 1711329838.4856021}, "models": {"yolo": {"arrival": 1711329838.0790591, "serving": 1711329838.4412844}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.448184, "arrival": 1711329840.5950835}, "models": {"yolo": {"arrival": 1711329840.2659144, "serving": 1711329840.5788538}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4478374, "arrival": 1711329840.734502}, "models": {"yolo": {"arrival": 1711329840.416586, "serving": 1711329840.7196167}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4480186, "arrival": 1711329842.3981788}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4486866, "arrival": 1711329842.3990211}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4485312, "arrival": 1711329841.5602918}, "models": {"yolo": {"arrival": 1711329840.9506905, "serving": 1711329841.5259905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4108357, "arrival": 1711329840.2504609}, "models": {"yolo": {"arrival": 1711329839.1807413, "serving": 1711329840.2101297}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.621543468+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329834.4481297}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.4391322, "arrival": 1711329840.243622}, "models": {"yolo": {"arrival": 1711329839.7462654, "serving": 1711329840.2104304}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.62134654+00:00\"}\"\n>", "times": {"request": {"sending": 1711329834.4339652}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.621453531+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329834.4392262}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.621518335+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329834.448003}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.4484432, "arrival": 1711329840.5917885}, "models": {"yolo": {"arrival": 1711329840.2187097, "serving": 1711329840.5729434}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4489422, "arrival": 1711329842.399567}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4109943, "arrival": 1711329842.3916607}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.447988, "arrival": 1711329841.5557814}, "models": {"yolo": {"arrival": 1711329840.9506905, "serving": 1711329841.5259905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4393175, "arrival": 1711329840.4584403}, "models": {"yolo": {"arrival": 1711329840.058783, "serving": 1711329840.406752}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4480832, "arrival": 1711329840.8718467}, "models": {"yolo": {"arrival": 1711329840.4049969, "serving": 1711329840.8637116}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4481614, "arrival": 1711329840.5907128}, "models": {"yolo": {"arrival": 1711329840.2187097, "serving": 1711329840.5729434}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4489915, "arrival": 1711329838.9104223}, "models": {"yolo": {"arrival": 1711329838.4516127, "serving": 1711329838.8866918}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.448205, "arrival": 1711329838.485169}, "models": {"yolo": {"arrival": 1711329838.0790591, "serving": 1711329838.4412844}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4389672, "arrival": 1711329842.394904}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.448908, "arrival": 1711329841.6748626}, "models": {"yolo": {"arrival": 1711329841.5365846, "serving": 1711329841.6212306}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4485934, "arrival": 1711329840.60461}, "models": {"yolo": {"arrival": 1711329840.2659144, "serving": 1711329840.5788538}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.621495911+00:00\"}\"\n>", "times": {"request": {"sending": 1711329834.4478703}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.410559, "arrival": 1711329840.079724}, "models": {"yolo": {"arrival": 1711329839.6990101, "serving": 1711329840.051467}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4488335, "arrival": 1711329840.927922}, "models": {"yolo": {"arrival": 1711329840.582549, "serving": 1711329840.9138508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.447938, "arrival": 1711329838.0911224}, "models": {"yolo": {"arrival": 1711329837.7041295, "serving": 1711329838.0708926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4488926, "arrival": 1711329840.9994233}, "models": {"yolo": {"arrival": 1711329840.7272606, "serving": 1711329840.9850633}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.448098, "arrival": 1711329840.7360864}, "models": {"yolo": {"arrival": 1711329840.416586, "serving": 1711329840.7196167}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4478538, "arrival": 1711329841.5551574}, "models": {"yolo": {"arrival": 1711329840.9506905, "serving": 1711329841.5259905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4338176, "arrival": 1711329840.2509499}, "models": {"yolo": {"arrival": 1711329839.1807413, "serving": 1711329840.2101297}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.607396016+00:00\"}\"\n>", "times": {"request": {"sending": 1711329834.4106379}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.448368, "arrival": 1711329840.7367194}, "models": {"yolo": {"arrival": 1711329840.416586, "serving": 1711329840.7196167}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.621475192+00:00\"}\"\n>", "times": {"request": {"sending": 1711329834.4394753}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.4485643, "arrival": 1711329842.3988132}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4487538, "arrival": 1711329840.8797686}, "models": {"yolo": {"arrival": 1711329840.4049969, "serving": 1711329840.8637116}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.410679, "arrival": 1711329842.391445}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4483497, "arrival": 1711329840.8727298}, "models": {"yolo": {"arrival": 1711329840.4049969, "serving": 1711329840.8637116}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4394584, "arrival": 1711329841.5545037}, "models": {"yolo": {"arrival": 1711329840.9506905, "serving": 1711329841.5259905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4477992, "arrival": 1711329840.4125395}, "models": {"yolo": {"arrival": 1711329840.218915, "serving": 1711329840.3951924}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4479203, "arrival": 1711329840.2721899}, "models": {"yolo": {"arrival": 1711329839.8792582, "serving": 1711329840.255313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4393027, "arrival": 1711329840.2526977}, "models": {"yolo": {"arrival": 1711329839.1807413, "serving": 1711329840.2101297}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.621589095+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329834.4485483}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.4339395, "arrival": 1711329841.5410492}, "models": {"yolo": {"arrival": 1711329840.9506905, "serving": 1711329841.5259905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4392872, "arrival": 1711329838.0880854}, "models": {"yolo": {"arrival": 1711329837.7041295, "serving": 1711329838.0708926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4110727, "arrival": 1711329839.8974566}, "models": {"yolo": {"arrival": 1711329839.2010665, "serving": 1711329839.8711727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.447886, "arrival": 1711329842.3979}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4486523, "arrival": 1711329841.6739826}, "models": {"yolo": {"arrival": 1711329841.5365846, "serving": 1711329841.6212306}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.621634419+00:00\"}\"\n>", "times": {"request": {"sending": 1711329834.4488041}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.448461, "arrival": 1711329840.6022758}, "models": {"yolo": {"arrival": 1711329840.2659144, "serving": 1711329840.5788538}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.621655817+00:00\"}\"\n>", "times": {"request": {"sending": 1711329834.448926}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.448718, "arrival": 1711329840.6066666}, "models": {"yolo": {"arrival": 1711329840.2659144, "serving": 1711329840.5788538}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.439058, "arrival": 1711329840.458004}, "models": {"yolo": {"arrival": 1711329840.058783, "serving": 1711329840.406752}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.621402348+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329834.438887}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.4479723, "arrival": 1711329840.7354217}, "models": {"yolo": {"arrival": 1711329840.416586, "serving": 1711329840.7196167}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4479566, "arrival": 1711329840.4133961}, "models": {"yolo": {"arrival": 1711329840.218915, "serving": 1711329840.3951924}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4481144, "arrival": 1711329841.5582356}, "models": {"yolo": {"arrival": 1711329840.9506905, "serving": 1711329841.5259905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4484954, "arrival": 1711329840.8781998}, "models": {"yolo": {"arrival": 1711329840.4049969, "serving": 1711329840.8637116}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4483852, "arrival": 1711329841.5593362}, "models": {"yolo": {"arrival": 1711329840.9506905, "serving": 1711329841.5259905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4487867, "arrival": 1711329841.6744306}, "models": {"yolo": {"arrival": 1711329841.5365846, "serving": 1711329841.6212306}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4491107, "arrival": 1711329840.929281}, "models": {"yolo": {"arrival": 1711329840.582549, "serving": 1711329840.9138508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4490085, "arrival": 1711329840.8831837}, "models": {"yolo": {"arrival": 1711329840.4049969, "serving": 1711329840.8637116}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4391782, "arrival": 1711329840.2523112}, "models": {"yolo": {"arrival": 1711329839.1807413, "serving": 1711329840.2101297}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:54.621567315+00:00\"}\"\n>", "times": {"request": {"sending": 1711329834.448407}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:54.62167754+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.53 GiB already allocated; 148.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329834.449077}}, "outputs": []}, {"times": {"request": {"sending": 1711329834.448579, "arrival": 1711329840.592522}, "models": {"yolo": {"arrival": 1711329840.2187097, "serving": 1711329840.5729434}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4488604, "arrival": 1711329838.4930086}, "models": {"yolo": {"arrival": 1711329838.0790591, "serving": 1711329838.4412844}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4480677, "arrival": 1711329838.4849534}, "models": {"yolo": {"arrival": 1711329838.0790591, "serving": 1711329838.4412844}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4488192, "arrival": 1711329842.39924}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.448477, "arrival": 1711329838.485386}, "models": {"yolo": {"arrival": 1711329838.0790591, "serving": 1711329838.4412844}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.439116, "arrival": 1711329842.3951373}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.449266, "arrival": 1711329841.1035173}, "models": {"yolo": {"arrival": 1711329840.588141, "serving": 1711329841.091083}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329834.4488745, "arrival": 1711329840.8823144}, "models": {"yolo": {"arrival": 1711329840.4049969, "serving": 1711329840.8637116}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329835.409336, "arrival": 1711329841.2807877}, "models": {"yolo": {"arrival": 1711329840.9941297, "serving": 1711329841.2649114}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4092364, "arrival": 1711329838.9120975}, "models": {"yolo": {"arrival": 1711329838.4516127, "serving": 1711329838.8866918}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4096184, "arrival": 1711329841.2814865}, "models": {"yolo": {"arrival": 1711329840.9941297, "serving": 1711329841.2649114}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4248312, "arrival": 1711329842.3494227}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4664679, "arrival": 1711329841.7819796}, "models": {"yolo": {"arrival": 1711329841.267347, "serving": 1711329841.7555263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4329891, "arrival": 1711329841.684315}, "models": {"yolo": {"arrival": 1711329841.2732346, "serving": 1711329841.625845}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.432943, "arrival": 1711329841.7727866}, "models": {"yolo": {"arrival": 1711329841.267347, "serving": 1711329841.7555263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4330363, "arrival": 1711329842.4126568}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.432836, "arrival": 1711329839.9754243}, "models": {"yolo": {"arrival": 1711329839.198064, "serving": 1711329839.9610813}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4664915, "arrival": 1711329840.25715}, "models": {"yolo": {"arrival": 1711329839.96886, "serving": 1711329840.2433934}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4093995, "arrival": 1711329842.325417}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.409276, "arrival": 1711329841.267597}, "models": {"yolo": {"arrival": 1711329840.8750172, "serving": 1711329841.2527542}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4665108, "arrival": 1711329842.3639667}, "models": {"yolo": {"arrival": 1711329842.0186505, "serving": 1711329842.3242552}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4090753, "arrival": 1711329842.403135}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4354477, "arrival": 1711329842.0441763}, "models": {"yolo": {"arrival": 1711329841.6382344, "serving": 1711329842.014088}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4665554, "arrival": 1711329843.1047885}, "models": {"yolo": {"arrival": 1711329842.686357, "serving": 1711329843.0705736}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4331877, "arrival": 1711329841.774639}, "models": {"yolo": {"arrival": 1711329841.267347, "serving": 1711329841.7555263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4332762, "arrival": 1711329842.413111}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4328525, "arrival": 1711329841.6729653}, "models": {"yolo": {"arrival": 1711329841.2642481, "serving": 1711329841.623958}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4219887, "arrival": 1711329842.3479106}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4325411, "arrival": 1711329841.3312058}, "models": {"yolo": {"arrival": 1711329840.9237494, "serving": 1711329841.311248}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.539131706+00:00\"}\"\n>", "times": {"request": {"sending": 1711329835.4357724}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.4695346, "arrival": 1711329840.257556}, "models": {"yolo": {"arrival": 1711329839.96886, "serving": 1711329840.2433934}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4095619, "arrival": 1711329838.9138243}, "models": {"yolo": {"arrival": 1711329838.4516127, "serving": 1711329838.8866918}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4355419, "arrival": 1711329842.4138682}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:56.538992613+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329835.4331448}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.433005, "arrival": 1711329842.380569}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.433082, "arrival": 1711329839.9771678}, "models": {"yolo": {"arrival": 1711329839.198064, "serving": 1711329839.9610813}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4355674, "arrival": 1711329841.7811043}, "models": {"yolo": {"arrival": 1711329841.267347, "serving": 1711329841.7555263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.469557, "arrival": 1711329842.3645215}, "models": {"yolo": {"arrival": 1711329842.0186505, "serving": 1711329842.3242552}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4092042, "arrival": 1711329841.1045763}, "models": {"yolo": {"arrival": 1711329840.588141, "serving": 1711329841.091083}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4243848, "arrival": 1711329841.3269792}, "models": {"yolo": {"arrival": 1711329840.9237494, "serving": 1711329841.311248}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4357252, "arrival": 1711329842.363365}, "models": {"yolo": {"arrival": 1711329842.0186505, "serving": 1711329842.3242552}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4100409, "arrival": 1711329839.2080784}, "models": {"yolo": {"arrival": 1711329838.8954146, "serving": 1711329839.1861207}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4333324, "arrival": 1711329842.0321681}, "models": {"yolo": {"arrival": 1711329841.6328652, "serving": 1711329842.0106678}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4327567, "arrival": 1711329842.3513093}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4095902, "arrival": 1711329841.276167}, "models": {"yolo": {"arrival": 1711329840.8750172, "serving": 1711329841.2527542}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4326792, "arrival": 1711329841.3326023}, "models": {"yolo": {"arrival": 1711329840.9237494, "serving": 1711329841.311248}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4094725, "arrival": 1711329842.4034548}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4091663, "arrival": 1711329840.93244}, "models": {"yolo": {"arrival": 1711329840.582549, "serving": 1711329840.9138508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4097056, "arrival": 1711329842.4036813}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.409966, "arrival": 1711329841.3231266}, "models": {"yolo": {"arrival": 1711329840.9237494, "serving": 1711329841.311248}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4245834, "arrival": 1711329841.1148446}, "models": {"yolo": {"arrival": 1711329840.588141, "serving": 1711329841.091083}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.433068, "arrival": 1711329841.7738926}, "models": {"yolo": {"arrival": 1711329841.267347, "serving": 1711329841.7555263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.424708, "arrival": 1711329842.4045818}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4356618, "arrival": 1711329842.4140716}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4329126, "arrival": 1711329842.4123871}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4246042, "arrival": 1711329839.2108064}, "models": {"yolo": {"arrival": 1711329838.8954146, "serving": 1711329839.1861207}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4097335, "arrival": 1711329841.3221376}, "models": {"yolo": {"arrival": 1711329840.9237494, "serving": 1711329841.311248}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4096768, "arrival": 1711329835.6716454}, "models": {"yolo": {"arrival": 1711329835.440096, "serving": 1711329835.658191}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.435756, "arrival": 1711329843.1042914}, "models": {"yolo": {"arrival": 1711329842.686357, "serving": 1711329843.0705736}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4094412, "arrival": 1711329835.670918}, "models": {"yolo": {"arrival": 1711329835.440096, "serving": 1711329835.658191}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.424498, "arrival": 1711329842.3483663}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:56.539157579+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329835.4718113}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.435709, "arrival": 1711329840.2567472}, "models": {"yolo": {"arrival": 1711329839.96886, "serving": 1711329840.2433934}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4696014, "arrival": 1711329843.1206157}, "models": {"yolo": {"arrival": 1711329842.686357, "serving": 1711329843.0705736}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4095025, "arrival": 1711329841.320082}, "models": {"yolo": {"arrival": 1711329840.9237494, "serving": 1711329841.311248}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4694164, "arrival": 1711329842.4185908}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.433234, "arrival": 1711329842.0180056}, "models": {"yolo": {"arrival": 1711329841.6365738, "serving": 1711329842.003304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4246256, "arrival": 1711329841.670572}, "models": {"yolo": {"arrival": 1711329841.2642481, "serving": 1711329841.623958}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4247696, "arrival": 1711329839.2112203}, "models": {"yolo": {"arrival": 1711329838.8954146, "serving": 1711329839.1861207}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4247494, "arrival": 1711329841.2682924}, "models": {"yolo": {"arrival": 1711329841.0983741, "serving": 1711329841.2525535}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4328194, "arrival": 1711329841.2725265}, "models": {"yolo": {"arrival": 1711329841.0983741, "serving": 1711329841.2525535}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4355028, "arrival": 1711329842.0245302}, "models": {"yolo": {"arrival": 1711329841.6365738, "serving": 1711329842.003304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4325995, "arrival": 1711329841.6720722}, "models": {"yolo": {"arrival": 1711329841.2642481, "serving": 1711329841.623958}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4097621, "arrival": 1711329841.1134923}, "models": {"yolo": {"arrival": 1711329840.588141, "serving": 1711329841.091083}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4665349, "arrival": 1711329842.369281}, "models": {"yolo": {"arrival": 1711329842.0108223, "serving": 1711329842.3233461}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4096475, "arrival": 1711329842.3461778}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4334745, "arrival": 1711329842.6982307}, "models": {"yolo": {"arrival": 1711329842.3212411, "serving": 1711329842.6793513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4325821, "arrival": 1711329839.2205791}, "models": {"yolo": {"arrival": 1711329838.8954146, "serving": 1711329839.1861207}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.539034712+00:00\"}\"\n>", "times": {"request": {"sending": 1711329835.4332626}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:56.525669817+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329835.4328969}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.4695795, "arrival": 1711329842.369732}, "models": {"yolo": {"arrival": 1711329842.0108223, "serving": 1711329842.3233461}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4327412, "arrival": 1711329841.6832929}, "models": {"yolo": {"arrival": 1711329841.2732346, "serving": 1711329841.625845}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4329274, "arrival": 1711329841.6776204}, "models": {"yolo": {"arrival": 1711329841.320868, "serving": 1711329841.628378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4356287, "arrival": 1711329843.1024752}, "models": {"yolo": {"arrival": 1711329842.686357, "serving": 1711329843.0705736}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4327111, "arrival": 1711329839.2289085}, "models": {"yolo": {"arrival": 1711329838.8954146, "serving": 1711329839.1861207}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4095323, "arrival": 1711329841.1129775}, "models": {"yolo": {"arrival": 1711329840.588141, "serving": 1711329841.091083}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4663413, "arrival": 1711329842.0464222}, "models": {"yolo": {"arrival": 1711329841.6382344, "serving": 1711329842.014088}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4245627, "arrival": 1711329841.3282137}, "models": {"yolo": {"arrival": 1711329840.9237494, "serving": 1711329841.311248}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4097905, "arrival": 1711329838.9145472}, "models": {"yolo": {"arrival": 1711329838.4516127, "serving": 1711329838.8866918}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4334311, "arrival": 1711329839.9801514}, "models": {"yolo": {"arrival": 1711329839.198064, "serving": 1711329839.9610813}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4325633, "arrival": 1711329841.271062}, "models": {"yolo": {"arrival": 1711329841.0983741, "serving": 1711329841.2525535}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.466252, "arrival": 1711329842.4182787}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4246662, "arrival": 1711329842.3488326}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4334452, "arrival": 1711329842.0328968}, "models": {"yolo": {"arrival": 1711329841.6328652, "serving": 1711329842.0106678}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.435741, "arrival": 1711329842.3688078}, "models": {"yolo": {"arrival": 1711329842.0108223, "serving": 1711329842.3233461}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4330983, "arrival": 1711329842.0291}, "models": {"yolo": {"arrival": 1711329841.6328652, "serving": 1711329842.0106678}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4355803, "arrival": 1711329840.256263}, "models": {"yolo": {"arrival": 1711329839.96886, "serving": 1711329840.2433934}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4333184, "arrival": 1711329839.9784377}, "models": {"yolo": {"arrival": 1711329839.198064, "serving": 1711329839.9610813}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4324677, "arrival": 1711329842.4047832}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4099371, "arrival": 1711329842.4038928}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:56.539073525+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329835.4354002}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.4328818, "arrival": 1711329842.3516874}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4356132, "arrival": 1711329842.0266595}, "models": {"yolo": {"arrival": 1711329841.6365738, "serving": 1711329842.003304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4354887, "arrival": 1711329842.04151}, "models": {"yolo": {"arrival": 1711329841.6328652, "serving": 1711329842.0106678}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4334168, "arrival": 1711329841.776367}, "models": {"yolo": {"arrival": 1711329841.267347, "serving": 1711329841.7555263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4334028, "arrival": 1711329841.6848223}, "models": {"yolo": {"arrival": 1711329841.320868, "serving": 1711329841.628378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4098196, "arrival": 1711329841.2769904}, "models": {"yolo": {"arrival": 1711329840.8750172, "serving": 1711329841.2527542}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.409878, "arrival": 1711329842.347244}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.43346, "arrival": 1711329842.0234509}, "models": {"yolo": {"arrival": 1711329841.6365738, "serving": 1711329842.003304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4356937, "arrival": 1711329841.78152}, "models": {"yolo": {"arrival": 1711329841.267347, "serving": 1711329841.7555263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4333608, "arrival": 1711329842.6975288}, "models": {"yolo": {"arrival": 1711329842.3212411, "serving": 1711329842.6793513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4354324, "arrival": 1711329842.4136689}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4331298, "arrival": 1711329842.694553}, "models": {"yolo": {"arrival": 1711329842.3212411, "serving": 1711329842.6793513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4218624, "arrival": 1711329841.277643}, "models": {"yolo": {"arrival": 1711329840.8750172, "serving": 1711329841.2527542}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4244761, "arrival": 1711329841.2877977}, "models": {"yolo": {"arrival": 1711329840.9941297, "serving": 1711329841.2649114}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4355547, "arrival": 1711329842.0445676}, "models": {"yolo": {"arrival": 1711329841.6382344, "serving": 1711329842.014088}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.525683236+00:00\"}\"\n>", "times": {"request": {"sending": 1711329835.4330206}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:56.52558369+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329835.4248514}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.4331148, "arrival": 1711329841.6850715}, "models": {"yolo": {"arrival": 1711329841.2732346, "serving": 1711329841.625845}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4331586, "arrival": 1711329842.412908}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4695148, "arrival": 1711329841.7827947}, "models": {"yolo": {"arrival": 1711329841.267347, "serving": 1711329841.7555263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4326642, "arrival": 1711329842.4049885}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4247906, "arrival": 1711329841.671614}, "models": {"yolo": {"arrival": 1711329841.2642481, "serving": 1711329841.623958}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.424646, "arrival": 1711329841.2925725}, "models": {"yolo": {"arrival": 1711329840.9941297, "serving": 1711329841.2649114}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4333887, "arrival": 1711329842.4133418}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.424455, "arrival": 1711329841.2782712}, "models": {"yolo": {"arrival": 1711329840.8750172, "serving": 1711329841.2527542}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.539054799+00:00\"}\"\n>", "times": {"request": {"sending": 1711329835.4333751}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.409849, "arrival": 1711329841.2819004}, "models": {"yolo": {"arrival": 1711329840.9941297, "serving": 1711329841.2649114}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4247282, "arrival": 1711329841.3304913}, "models": {"yolo": {"arrival": 1711329840.9237494, "serving": 1711329841.311248}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.435516, "arrival": 1711329843.1018314}, "models": {"yolo": {"arrival": 1711329842.686357, "serving": 1711329843.0705736}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4356775, "arrival": 1711329842.0459368}, "models": {"yolo": {"arrival": 1711329841.6382344, "serving": 1711329842.014088}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.522306716+00:00\"}\"\n>", "times": {"request": {"sending": 1711329835.4220142}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.539113198+00:00\"}\"\n>", "times": {"request": {"sending": 1711329835.4356468}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.4248104, "arrival": 1711329841.6827774}, "models": {"yolo": {"arrival": 1711329841.2732346, "serving": 1711329841.625845}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:56.525655776+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329835.4327726}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.539144651+00:00\"}\"\n>", "times": {"request": {"sending": 1711329835.4665785}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.433051, "arrival": 1711329841.6821473}, "models": {"yolo": {"arrival": 1711329841.320868, "serving": 1711329841.628378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4326315, "arrival": 1711329842.3498752}, "models": {"yolo": {"arrival": 1711329841.6361275, "serving": 1711329842.3094022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:56.522346017+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329835.4245198}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.432616, "arrival": 1711329841.6830354}, "models": {"yolo": {"arrival": 1711329841.2732346, "serving": 1711329841.625845}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4694912, "arrival": 1711329842.0539293}, "models": {"yolo": {"arrival": 1711329841.6382344, "serving": 1711329842.014088}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.522432879+00:00\"}\"\n>", "times": {"request": {"sending": 1711329835.4246864}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.4100027, "arrival": 1711329841.1139126}, "models": {"yolo": {"arrival": 1711329840.588141, "serving": 1711329841.091083}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4354613, "arrival": 1711329841.7806044}, "models": {"yolo": {"arrival": 1711329841.267347, "serving": 1711329841.7555263}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.522204491+00:00\"}\"\n>", "times": {"request": {"sending": 1711329835.4099066}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.525638372+00:00\"}\"\n>", "times": {"request": {"sending": 1711329835.4326472}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.433173, "arrival": 1711329841.6824825}, "models": {"yolo": {"arrival": 1711329841.320868, "serving": 1711329841.628378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.433304, "arrival": 1711329841.7753618}, "models": {"yolo": {"arrival": 1711329841.267347, "serving": 1711329841.7555263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4332025, "arrival": 1711329839.9775813}, "models": {"yolo": {"arrival": 1711329839.198064, "serving": 1711329839.9610813}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4329593, "arrival": 1711329839.97646}, "models": {"yolo": {"arrival": 1711329839.198064, "serving": 1711329839.9610813}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4332478, "arrival": 1711329842.6954658}, "models": {"yolo": {"arrival": 1711329842.3212411, "serving": 1711329842.6793513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4355977, "arrival": 1711329842.0421}, "models": {"yolo": {"arrival": 1711329841.6328652, "serving": 1711329842.0106678}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.42441, "arrival": 1711329841.1143162}, "models": {"yolo": {"arrival": 1711329840.588141, "serving": 1711329841.091083}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4329739, "arrival": 1711329841.6735635}, "models": {"yolo": {"arrival": 1711329841.2642481, "serving": 1711329841.623958}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.435475, "arrival": 1711329839.980778}, "models": {"yolo": {"arrival": 1711329839.198064, "serving": 1711329839.9610813}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4333467, "arrival": 1711329842.0221462}, "models": {"yolo": {"arrival": 1711329841.6365738, "serving": 1711329842.003304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.432726, "arrival": 1711329841.6725147}, "models": {"yolo": {"arrival": 1711329841.2642481, "serving": 1711329841.623958}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4245422, "arrival": 1711329842.404388}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4243152, "arrival": 1711329842.4041889}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.432696, "arrival": 1711329841.2718682}, "models": {"yolo": {"arrival": 1711329841.0983741, "serving": 1711329841.2525535}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4244332, "arrival": 1711329839.2101614}, "models": {"yolo": {"arrival": 1711329838.8954146, "serving": 1711329839.1861207}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.539093748+00:00\"}\"\n>", "times": {"request": {"sending": 1711329835.4355292}}, "outputs": []}, {"times": {"request": {"sending": 1711329835.4328039, "arrival": 1711329841.67724}, "models": {"yolo": {"arrival": 1711329841.320868, "serving": 1711329841.628378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.421956, "arrival": 1711329841.287164}, "models": {"yolo": {"arrival": 1711329840.9941297, "serving": 1711329841.2649114}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4328668, "arrival": 1711329841.6835475}, "models": {"yolo": {"arrival": 1711329841.2732346, "serving": 1711329841.625845}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4332902, "arrival": 1711329841.684052}, "models": {"yolo": {"arrival": 1711329841.320868, "serving": 1711329841.628378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4332168, "arrival": 1711329842.030067}, "models": {"yolo": {"arrival": 1711329841.6328652, "serving": 1711329842.0106678}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329835.4327888, "arrival": 1711329842.4119568}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}], [{"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.362614183+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329836.4868894}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4498901, "arrival": 1711329842.7826157}, "models": {"yolo": {"arrival": 1711329842.3364813, "serving": 1711329842.7589023}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.539170068+00:00\"}\"\n>", "times": {"request": {"sending": 1711329836.4225953}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.350561037+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329836.422822}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.486988, "arrival": 1711329842.7721364}, "models": {"yolo": {"arrival": 1711329842.342777, "serving": 1711329842.739653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.487253, "arrival": 1711329843.1258578}, "models": {"yolo": {"arrival": 1711329842.74912, "serving": 1711329843.0849333}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4853532, "arrival": 1711329843.1305802}, "models": {"yolo": {"arrival": 1711329842.8767562, "serving": 1711329843.073346}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4488356, "arrival": 1711329842.990495}, "models": {"yolo": {"arrival": 1711329842.3354306, "serving": 1711329842.9657896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4231038, "arrival": 1711329842.9893713}, "models": {"yolo": {"arrival": 1711329842.3354306, "serving": 1711329842.9657896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4502513, "arrival": 1711329842.391878}, "models": {"yolo": {"arrival": 1711329842.0257022, "serving": 1711329842.3327627}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4692533, "arrival": 1711329844.4018352}, "models": {"yolo": {"arrival": 1711329843.8819537, "serving": 1711329844.3868082}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.362565738+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329836.4865835}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.485183, "arrival": 1711329843.1221554}, "models": {"yolo": {"arrival": 1711329842.7683907, "serving": 1711329843.0776596}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4690335, "arrival": 1711329842.7039902}, "models": {"yolo": {"arrival": 1711329842.33976, "serving": 1711329842.6852188}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4870174, "arrival": 1711329841.2933567}, "models": {"yolo": {"arrival": 1711329840.907441, "serving": 1711329841.2672462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4228082, "arrival": 1711329843.5395105}, "models": {"yolo": {"arrival": 1711329843.0814016, "serving": 1711329843.5197525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4228492, "arrival": 1711329842.3790643}, "models": {"yolo": {"arrival": 1711329842.023036, "serving": 1711329842.3265717}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4225214, "arrival": 1711329842.0490732}, "models": {"yolo": {"arrival": 1711329841.7652028, "serving": 1711329842.0164268}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4859617, "arrival": 1711329843.12334}, "models": {"yolo": {"arrival": 1711329842.7683907, "serving": 1711329843.0776596}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4231293, "arrival": 1711329843.5418684}, "models": {"yolo": {"arrival": 1711329843.0814016, "serving": 1711329843.5197525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4491656, "arrival": 1711329840.9102633}, "models": {"yolo": {"arrival": 1711329840.5859141, "serving": 1711329840.8980837}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.362495363+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329836.485099}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.485592, "arrival": 1711329842.7698703}, "models": {"yolo": {"arrival": 1711329842.342777, "serving": 1711329842.739653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4867902, "arrival": 1711329843.8673582}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.448999, "arrival": 1711329843.895445}, "models": {"yolo": {"arrival": 1711329843.5284526, "serving": 1711329843.8727188}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4853058, "arrival": 1711329843.1011908}, "models": {"yolo": {"arrival": 1711329842.97598, "serving": 1711329843.0703397}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.356412733+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329836.4504607}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4852657, "arrival": 1711329841.289481}, "models": {"yolo": {"arrival": 1711329840.907441, "serving": 1711329841.2672462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4857516, "arrival": 1711329843.6999984}, "models": {"yolo": {"arrival": 1711329843.0838504, "serving": 1711329843.6785}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4867587, "arrival": 1711329841.2929933}, "models": {"yolo": {"arrival": 1711329840.907441, "serving": 1711329841.2672462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4870517, "arrival": 1711329843.8689716}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4225018, "arrival": 1711329842.054348}, "models": {"yolo": {"arrival": 1711329841.6382344, "serving": 1711329842.014088}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4225376, "arrival": 1711329840.5942738}, "models": {"yolo": {"arrival": 1711329840.251927, "serving": 1711329840.577104}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4872856, "arrival": 1711329841.675651}, "models": {"yolo": {"arrival": 1711329841.2778516, "serving": 1711329841.6247783}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.450291, "arrival": 1711329840.9145083}, "models": {"yolo": {"arrival": 1711329840.5859141, "serving": 1711329840.8980837}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4229558, "arrival": 1711329842.379504}, "models": {"yolo": {"arrival": 1711329842.023036, "serving": 1711329842.3265717}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4869554, "arrival": 1711329843.1322184}, "models": {"yolo": {"arrival": 1711329842.7683907, "serving": 1711329843.0776596}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4490998, "arrival": 1711329842.781807}, "models": {"yolo": {"arrival": 1711329842.3364813, "serving": 1711329842.7589023}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.422726, "arrival": 1711329842.4192128}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4693186, "arrival": 1711329843.0805244}, "models": {"yolo": {"arrival": 1711329842.6924958, "serving": 1711329843.0641727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4498563, "arrival": 1711329842.7005477}, "models": {"yolo": {"arrival": 1711329842.33976, "serving": 1711329842.6852188}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.356276921+00:00\"}\"\n>", "times": {"request": {"sending": 1711329836.4493093}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4854932, "arrival": 1711329843.0845816}, "models": {"yolo": {"arrival": 1711329842.6924958, "serving": 1711329843.0641727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4224513, "arrival": 1711329842.4188008}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4490685, "arrival": 1711329842.420325}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.356315321+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329836.4498227}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4225526, "arrival": 1711329842.3682318}, "models": {"yolo": {"arrival": 1711329842.0186505, "serving": 1711329842.3242552}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.468816, "arrival": 1711329840.9153135}, "models": {"yolo": {"arrival": 1711329840.5859141, "serving": 1711329840.8980837}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4225674, "arrival": 1711329842.3701708}, "models": {"yolo": {"arrival": 1711329842.0108223, "serving": 1711329842.3233461}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4229712, "arrival": 1711329842.0547216}, "models": {"yolo": {"arrival": 1711329841.7652028, "serving": 1711329842.0164268}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4228892, "arrival": 1711329842.9859536}, "models": {"yolo": {"arrival": 1711329842.3354306, "serving": 1711329842.9657896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4860568, "arrival": 1711329841.2913482}, "models": {"yolo": {"arrival": 1711329840.907441, "serving": 1711329841.2672462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.469106, "arrival": 1711329842.3922994}, "models": {"yolo": {"arrival": 1711329842.0257022, "serving": 1711329842.3327627}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4864821, "arrival": 1711329843.700926}, "models": {"yolo": {"arrival": 1711329843.0838504, "serving": 1711329843.6785}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4856431, "arrival": 1711329841.290931}, "models": {"yolo": {"arrival": 1711329840.907441, "serving": 1711329841.2672462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.423051, "arrival": 1711329842.4198318}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4226813, "arrival": 1711329842.390146}, "models": {"yolo": {"arrival": 1711329842.0108223, "serving": 1711329842.3233461}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4230251, "arrival": 1711329843.5410974}, "models": {"yolo": {"arrival": 1711329843.0814016, "serving": 1711329843.5197525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4871874, "arrival": 1711329843.5646071}, "models": {"yolo": {"arrival": 1711329843.073384, "serving": 1711329843.5263715}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.486419, "arrival": 1711329843.866559}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4873903, "arrival": 1711329844.6247077}, "models": {"yolo": {"arrival": 1711329844.39547, "serving": 1711329844.6139708}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.449725, "arrival": 1711329842.9948516}, "models": {"yolo": {"arrival": 1711329842.3354306, "serving": 1711329842.9657896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4229844, "arrival": 1711329840.6042411}, "models": {"yolo": {"arrival": 1711329840.251927, "serving": 1711329840.577104}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4227657, "arrival": 1711329840.6033132}, "models": {"yolo": {"arrival": 1711329840.251927, "serving": 1711329840.577104}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4497898, "arrival": 1711329843.8963826}, "models": {"yolo": {"arrival": 1711329843.5284526, "serving": 1711329843.8727188}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4230645, "arrival": 1711329842.3799436}, "models": {"yolo": {"arrival": 1711329842.023036, "serving": 1711329842.3265717}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4229426, "arrival": 1711329842.419636}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.422582, "arrival": 1711329843.1239862}, "models": {"yolo": {"arrival": 1711329842.686357, "serving": 1711329843.0705736}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4692128, "arrival": 1711329843.1200783}, "models": {"yolo": {"arrival": 1711329842.8767562, "serving": 1711329843.073346}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4492614, "arrival": 1711329843.8959446}, "models": {"yolo": {"arrival": 1711329843.5284526, "serving": 1711329843.8727188}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4228632, "arrival": 1711329842.0504138}, "models": {"yolo": {"arrival": 1711329841.7652028, "serving": 1711329842.0164268}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4503412, "arrival": 1711329842.996188}, "models": {"yolo": {"arrival": 1711329842.3354306, "serving": 1711329842.9657896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.486691, "arrival": 1711329843.1310134}, "models": {"yolo": {"arrival": 1711329842.7683907, "serving": 1711329843.0776596}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4231813, "arrival": 1711329842.380886}, "models": {"yolo": {"arrival": 1711329842.0257022, "serving": 1711329842.3327627}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4500694, "arrival": 1711329843.896773}, "models": {"yolo": {"arrival": 1711329843.5284526, "serving": 1711329843.8727188}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4848957, "arrival": 1711329841.2886584}, "models": {"yolo": {"arrival": 1711329840.907441, "serving": 1711329841.2672462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4499528, "arrival": 1711329840.9128523}, "models": {"yolo": {"arrival": 1711329840.5859141, "serving": 1711329840.8980837}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4689283, "arrival": 1711329843.898768}, "models": {"yolo": {"arrival": 1711329843.5284526, "serving": 1711329843.8727188}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.362541181+00:00\"}\"\n>", "times": {"request": {"sending": 1711329836.4858587}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4693575, "arrival": 1711329843.121637}, "models": {"yolo": {"arrival": 1711329842.7683907, "serving": 1711329843.0776596}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.422739, "arrival": 1711329842.3787088}, "models": {"yolo": {"arrival": 1711329842.023036, "serving": 1711329842.3265717}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.486924, "arrival": 1711329843.5640023}, "models": {"yolo": {"arrival": 1711329843.073384, "serving": 1711329843.5263715}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.422752, "arrival": 1711329842.0499868}, "models": {"yolo": {"arrival": 1711329841.7652028, "serving": 1711329842.0164268}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4859076, "arrival": 1711329843.0854635}, "models": {"yolo": {"arrival": 1711329842.6924958, "serving": 1711329843.0641727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4226522, "arrival": 1711329840.6028168}, "models": {"yolo": {"arrival": 1711329840.251927, "serving": 1711329840.577104}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4226668, "arrival": 1711329842.388463}, "models": {"yolo": {"arrival": 1711329842.0186505, "serving": 1711329842.3242552}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.362518254+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329836.4854348}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.422998, "arrival": 1711329842.9871562}, "models": {"yolo": {"arrival": 1711329842.3354306, "serving": 1711329842.9657896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.486825, "arrival": 1711329843.7017376}, "models": {"yolo": {"arrival": 1711329843.0838504, "serving": 1711329843.6785}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.468768, "arrival": 1711329842.392091}, "models": {"yolo": {"arrival": 1711329842.0257022, "serving": 1711329842.3327627}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4866333, "arrival": 1711329843.0948493}, "models": {"yolo": {"arrival": 1711329842.6924958, "serving": 1711329843.0641727}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.36238684+00:00\"}\"\n>", "times": {"request": {"sending": 1711329836.4689968}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4691422, "arrival": 1711329841.288239}, "models": {"yolo": {"arrival": 1711329840.907441, "serving": 1711329841.2672462}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.356206653+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329836.449034}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4870868, "arrival": 1711329843.7039752}, "models": {"yolo": {"arrival": 1711329843.0838504, "serving": 1711329843.6785}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4496934, "arrival": 1711329840.9121265}, "models": {"yolo": {"arrival": 1711329840.5859141, "serving": 1711329840.8980837}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4868562, "arrival": 1711329844.4160724}, "models": {"yolo": {"arrival": 1711329843.8819537, "serving": 1711329844.3868082}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4847555, "arrival": 1711329842.7552958}, "models": {"yolo": {"arrival": 1711329842.342777, "serving": 1711329842.739653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.362469005+00:00\"}\"\n>", "times": {"request": {"sending": 1711329836.469284}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.362637227+00:00\"}\"\n>", "times": {"request": {"sending": 1711329836.487157}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4227939, "arrival": 1711329842.3907979}, "models": {"yolo": {"arrival": 1711329842.0108223, "serving": 1711329842.3233461}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4852245, "arrival": 1711329842.7561817}, "models": {"yolo": {"arrival": 1711329842.342777, "serving": 1711329842.739653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4686632, "arrival": 1711329842.7833846}, "models": {"yolo": {"arrival": 1711329842.3364813, "serving": 1711329842.7589023}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.486533, "arrival": 1711329844.4119284}, "models": {"yolo": {"arrival": 1711329843.8819537, "serving": 1711329844.3868082}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4688544, "arrival": 1711329843.0957704}, "models": {"yolo": {"arrival": 1711329842.97598, "serving": 1711329843.0703397}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4231684, "arrival": 1711329842.3802612}, "models": {"yolo": {"arrival": 1711329842.023036, "serving": 1711329842.3265717}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4497592, "arrival": 1711329842.897065}, "models": {"yolo": {"arrival": 1711329842.3364322, "serving": 1711329842.8681824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.450428, "arrival": 1711329843.898121}, "models": {"yolo": {"arrival": 1711329843.5284526, "serving": 1711329843.8727188}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4226093, "arrival": 1711329842.4190083}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4850557, "arrival": 1711329844.4029648}, "models": {"yolo": {"arrival": 1711329843.8819537, "serving": 1711329844.3868082}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4230769, "arrival": 1711329842.0549893}, "models": {"yolo": {"arrival": 1711329841.7652028, "serving": 1711329842.0164268}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4499204, "arrival": 1711329842.3819304}, "models": {"yolo": {"arrival": 1711329842.0257022, "serving": 1711329842.3327627}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4851406, "arrival": 1711329843.0834801}, "models": {"yolo": {"arrival": 1711329842.6924958, "serving": 1711329843.0641727}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.485543, "arrival": 1711329843.1228278}, "models": {"yolo": {"arrival": 1711329842.7683907, "serving": 1711329843.0776596}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.350725794+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329836.423038}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4226227, "arrival": 1711329842.3783605}, "models": {"yolo": {"arrival": 1711329842.023036, "serving": 1711329842.3265717}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.350671571+00:00\"}\"\n>", "times": {"request": {"sending": 1711329836.4229295}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4229157, "arrival": 1711329843.5403945}, "models": {"yolo": {"arrival": 1711329843.0814016, "serving": 1711329843.5197525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4496276, "arrival": 1711329842.7821965}, "models": {"yolo": {"arrival": 1711329842.3364813, "serving": 1711329842.7589023}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4858093, "arrival": 1711329844.4111829}, "models": {"yolo": {"arrival": 1711329843.8819537, "serving": 1711329844.3868082}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.422876, "arrival": 1711329840.6038373}, "models": {"yolo": {"arrival": 1711329840.251927, "serving": 1711329840.577104}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4496593, "arrival": 1711329842.3815002}, "models": {"yolo": {"arrival": 1711329842.0257022, "serving": 1711329842.3327627}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.423011, "arrival": 1711329842.8931568}, "models": {"yolo": {"arrival": 1711329842.3364322, "serving": 1711329842.8681824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4690697, "arrival": 1711329842.7837677}, "models": {"yolo": {"arrival": 1711329842.3364813, "serving": 1711329842.7589023}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4491315, "arrival": 1711329842.3811953}, "models": {"yolo": {"arrival": 1711329842.0257022, "serving": 1711329842.3327627}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4860115, "arrival": 1711329842.7708054}, "models": {"yolo": {"arrival": 1711329842.342777, "serving": 1711329842.739653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.469175, "arrival": 1711329843.0993133}, "models": {"yolo": {"arrival": 1711329842.97598, "serving": 1711329843.0703397}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4492295, "arrival": 1711329842.8952093}, "models": {"yolo": {"arrival": 1711329842.3364322, "serving": 1711329842.8681824}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.59 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:56.539182071+00:00\"}\"\n>", "times": {"request": {"sending": 1711329836.4227116}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.422637, "arrival": 1711329842.0495636}, "models": {"yolo": {"arrival": 1711329841.7652028, "serving": 1711329842.0164268}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4491966, "arrival": 1711329842.992223}, "models": {"yolo": {"arrival": 1711329842.3354306, "serving": 1711329842.9657896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.487221, "arrival": 1711329843.5582523}, "models": {"yolo": {"arrival": 1711329843.0877025, "serving": 1711329843.5297189}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4502087, "arrival": 1711329842.783001}, "models": {"yolo": {"arrival": 1711329842.3364813, "serving": 1711329842.7589023}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.356366094+00:00\"}\"\n>", "times": {"request": {"sending": 1711329836.4501216}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4501731, "arrival": 1711329842.702736}, "models": {"yolo": {"arrival": 1711329842.33976, "serving": 1711329842.6852188}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4231558, "arrival": 1711329842.4201257}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4871259, "arrival": 1711329844.4165316}, "models": {"yolo": {"arrival": 1711329843.8819537, "serving": 1711329844.3868082}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4849555, "arrival": 1711329843.1004872}, "models": {"yolo": {"arrival": 1711329842.97598, "serving": 1711329843.0703397}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4500217, "arrival": 1711329842.8977556}, "models": {"yolo": {"arrival": 1711329842.3364322, "serving": 1711329842.8681824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.449585, "arrival": 1711329842.4465845}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4873226, "arrival": 1711329843.8696911}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.485004, "arrival": 1711329843.1211333}, "models": {"yolo": {"arrival": 1711329842.8767562, "serving": 1711329843.073346}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.45038, "arrival": 1711329843.1187358}, "models": {"yolo": {"arrival": 1711329842.8767562, "serving": 1711329843.073346}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.422902, "arrival": 1711329842.889619}, "models": {"yolo": {"arrival": 1711329842.3364322, "serving": 1711329842.8681824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4499893, "arrival": 1711329842.9955533}, "models": {"yolo": {"arrival": 1711329842.3354306, "serving": 1711329842.9657896}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.350774649+00:00\"}\"\n>", "times": {"request": {"sending": 1711329836.4231422}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4231162, "arrival": 1711329842.8939056}, "models": {"yolo": {"arrival": 1711329842.3364322, "serving": 1711329842.8681824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.423194, "arrival": 1711329840.9093852}, "models": {"yolo": {"arrival": 1711329840.5859141, "serving": 1711329840.8980837}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4853942, "arrival": 1711329844.4040096}, "models": {"yolo": {"arrival": 1711329843.8819537, "serving": 1711329844.3868082}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.422836, "arrival": 1711329842.419414}, "models": {"yolo": {"arrival": 1711329839.1999023, "serving": 1711329842.3071141}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4226952, "arrival": 1711329843.532594}, "models": {"yolo": {"arrival": 1711329843.0814016, "serving": 1711329843.5197525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4873598, "arrival": 1711329843.705023}, "models": {"yolo": {"arrival": 1711329843.0838504, "serving": 1711329843.6785}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.42309, "arrival": 1711329840.6082017}, "models": {"yolo": {"arrival": 1711329840.251927, "serving": 1711329840.577104}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4227798, "arrival": 1711329842.3887978}, "models": {"yolo": {"arrival": 1711329842.0186505, "serving": 1711329842.3242552}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4856935, "arrival": 1711329843.8642063}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4504979, "arrival": 1711329842.703406}, "models": {"yolo": {"arrival": 1711329842.33976, "serving": 1711329842.6852188}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.367638083+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329836.487422}}, "outputs": []}, {"times": {"request": {"sending": 1711329836.4688919, "arrival": 1711329843.1195247}, "models": {"yolo": {"arrival": 1711329842.8767562, "serving": 1711329843.073346}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.4489508, "arrival": 1711329842.8945735}, "models": {"yolo": {"arrival": 1711329842.3364322, "serving": 1711329842.8681824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329836.486728, "arrival": 1711329842.7714915}, "models": {"yolo": {"arrival": 1711329842.342777, "serving": 1711329842.739653}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329837.4361534, "arrival": 1711329843.5620914}, "models": {"yolo": {"arrival": 1711329843.0956893, "serving": 1711329843.5284462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4184654, "arrival": 1711329843.881999}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.436108, "arrival": 1711329843.9339154}, "models": {"yolo": {"arrival": 1711329843.5368364, "serving": 1711329843.9070437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4358647, "arrival": 1711329843.8911245}, "models": {"yolo": {"arrival": 1711329843.6886375, "serving": 1711329843.8553238}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4360197, "arrival": 1711329843.893155}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4182189, "arrival": 1711329844.6300843}, "models": {"yolo": {"arrival": 1711329844.39547, "serving": 1711329844.6139708}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.417968, "arrival": 1711329843.5658557}, "models": {"yolo": {"arrival": 1711329843.073384, "serving": 1711329843.5263715}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4175785, "arrival": 1711329843.5652578}, "models": {"yolo": {"arrival": 1711329843.073384, "serving": 1711329843.5263715}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.626452527+00:00\"}\"\n>", "times": {"request": {"sending": 1711329837.4362736}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4180932, "arrival": 1711329841.6764336}, "models": {"yolo": {"arrival": 1711329841.2778516, "serving": 1711329841.6247783}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4533534, "arrival": 1711329844.0667453}, "models": {"yolo": {"arrival": 1711329843.5384712, "serving": 1711329844.0427692}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.453549, "arrival": 1711329844.5374186}, "models": {"yolo": {"arrival": 1711329843.86428, "serving": 1711329844.496366}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4177976, "arrival": 1711329843.8801656}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4531133, "arrival": 1711329842.389033}, "models": {"yolo": {"arrival": 1711329842.0228128, "serving": 1711329842.3352203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4363346, "arrival": 1711329843.936062}, "models": {"yolo": {"arrival": 1711329843.5401025, "serving": 1711329843.9045594}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4686446, "arrival": 1711329842.7727644}, "models": {"yolo": {"arrival": 1711329842.3465676, "serving": 1711329842.7420654}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.435998, "arrival": 1711329842.0434113}, "models": {"yolo": {"arrival": 1711329841.6351824, "serving": 1711329842.013272}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.468105, "arrival": 1711329844.6418595}, "models": {"yolo": {"arrival": 1711329844.202703, "serving": 1711329844.6202192}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4529674, "arrival": 1711329842.053461}, "models": {"yolo": {"arrival": 1711329841.6351824, "serving": 1711329842.013272}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4177547, "arrival": 1711329841.6760464}, "models": {"yolo": {"arrival": 1711329841.2778516, "serving": 1711329841.6247783}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4681938, "arrival": 1711329844.7210336}, "models": {"yolo": {"arrival": 1711329844.3162532, "serving": 1711329844.6933634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.468025, "arrival": 1711329842.3925028}, "models": {"yolo": {"arrival": 1711329842.0228128, "serving": 1711329842.3352203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4530091, "arrival": 1711329844.5261}, "models": {"yolo": {"arrival": 1711329843.86428, "serving": 1711329844.496366}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.453258, "arrival": 1711329844.6370432}, "models": {"yolo": {"arrival": 1711329844.202703, "serving": 1711329844.6202192}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4537237, "arrival": 1711329844.3318026}, "models": {"yolo": {"arrival": 1711329843.913474, "serving": 1711329844.305147}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.611328219+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329837.4179251}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4530802, "arrival": 1711329844.3197849}, "models": {"yolo": {"arrival": 1711329843.913474, "serving": 1711329844.305147}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4533367, "arrival": 1711329844.3237832}, "models": {"yolo": {"arrival": 1711329843.913474, "serving": 1711329844.305147}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.468507, "arrival": 1711329844.0697293}, "models": {"yolo": {"arrival": 1711329843.5384712, "serving": 1711329844.0427692}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.453061, "arrival": 1711329844.3391416}, "models": {"yolo": {"arrival": 1711329843.9171364, "serving": 1711329844.3075583}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.453399, "arrival": 1711329844.5369196}, "models": {"yolo": {"arrival": 1711329843.86428, "serving": 1711329844.496366}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.496137875+00:00\"}\"\n>", "times": {"request": {"sending": 1711329837.4687047}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4319553, "arrival": 1711329841.684573}, "models": {"yolo": {"arrival": 1711329841.2778516, "serving": 1711329841.6247783}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4185169, "arrival": 1711329844.9374666}, "models": {"yolo": {"arrival": 1711329844.6216085, "serving": 1711329844.9118588}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4533837, "arrival": 1711329844.640383}, "models": {"yolo": {"arrival": 1711329844.202703, "serving": 1711329844.6202192}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4687495, "arrival": 1711329844.0721703}, "models": {"yolo": {"arrival": 1711329843.5384712, "serving": 1711329844.0427692}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4358418, "arrival": 1711329843.8927338}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4530966, "arrival": 1711329843.574469}, "models": {"yolo": {"arrival": 1711329843.0956893, "serving": 1711329843.5284462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4686902, "arrival": 1711329846.485977}, "models": {"yolo": {"arrival": 1711329846.0718527, "serving": 1711329846.462087}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4177105, "arrival": 1711329843.1262896}, "models": {"yolo": {"arrival": 1711329842.74912, "serving": 1711329843.0849333}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.418386, "arrival": 1711329843.1301408}, "models": {"yolo": {"arrival": 1711329842.74912, "serving": 1711329843.0849333}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4176629, "arrival": 1711329843.5591917}, "models": {"yolo": {"arrival": 1711329843.0877025, "serving": 1711329843.5297189}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4531636, "arrival": 1711329846.0770109}, "models": {"yolo": {"arrival": 1711329844.923406, "serving": 1711329846.062455}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.468764, "arrival": 1711329842.7733638}, "models": {"yolo": {"arrival": 1711329842.3465676, "serving": 1711329842.7420654}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4686599, "arrival": 1711329844.916263}, "models": {"yolo": {"arrival": 1711329844.6302068, "serving": 1711329844.8931458}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4537075, "arrival": 1711329844.3466663}, "models": {"yolo": {"arrival": 1711329843.9171364, "serving": 1711329844.3075583}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4358864, "arrival": 1711329844.9464853}, "models": {"yolo": {"arrival": 1711329844.6216085, "serving": 1711329844.9118588}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4181356, "arrival": 1711329843.8813357}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.611444391+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329837.4185421}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.435797, "arrival": 1711329843.1351852}, "models": {"yolo": {"arrival": 1711329842.74912, "serving": 1711329843.0849333}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4532738, "arrival": 1711329844.5359242}, "models": {"yolo": {"arrival": 1711329843.86428, "serving": 1711329844.496366}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4318926, "arrival": 1711329843.931893}, "models": {"yolo": {"arrival": 1711329843.5368364, "serving": 1711329843.9070437}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.611566839+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329837.4357283}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4180098, "arrival": 1711329843.5600464}, "models": {"yolo": {"arrival": 1711329843.0877025, "serving": 1711329843.5297189}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4183023, "arrival": 1711329843.5750604}, "models": {"yolo": {"arrival": 1711329843.073384, "serving": 1711329843.5263715}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.626617093+00:00\"}\"\n>", "times": {"request": {"sending": 1711329837.4681785}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.626570644+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329837.453431}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.468522, "arrival": 1711329842.3929074}, "models": {"yolo": {"arrival": 1711329842.0228128, "serving": 1711329842.3352203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4682915, "arrival": 1711329846.0866902}, "models": {"yolo": {"arrival": 1711329844.923406, "serving": 1711329846.062455}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4363563, "arrival": 1711329843.56276}, "models": {"yolo": {"arrival": 1711329843.0956893, "serving": 1711329843.5284462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4531295, "arrival": 1711329844.211546}, "models": {"yolo": {"arrival": 1711329843.8559685, "serving": 1711329844.1960704}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.43593, "arrival": 1711329843.933492}, "models": {"yolo": {"arrival": 1711329843.5368364, "serving": 1711329843.9070437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4185693, "arrival": 1711329843.5762956}, "models": {"yolo": {"arrival": 1711329843.073384, "serving": 1711329843.5263715}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4362955, "arrival": 1711329843.9355888}, "models": {"yolo": {"arrival": 1711329843.5368364, "serving": 1711329843.9070437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4685366, "arrival": 1711329844.646516}, "models": {"yolo": {"arrival": 1711329844.202703, "serving": 1711329844.6202192}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4178412, "arrival": 1711329843.708122}, "models": {"yolo": {"arrival": 1711329843.0838504, "serving": 1711329843.6785}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4358191, "arrival": 1711329842.0430138}, "models": {"yolo": {"arrival": 1711329841.6351824, "serving": 1711329842.013272}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4529908, "arrival": 1711329844.2106266}, "models": {"yolo": {"arrival": 1711329843.8559685, "serving": 1711329844.1960704}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4687347, "arrival": 1711329844.713504}, "models": {"yolo": {"arrival": 1711329844.3119266, "serving": 1711329844.683989}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.626593778+00:00\"}\"\n>", "times": {"request": {"sending": 1711329837.4536915}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4536743, "arrival": 1711329846.0830672}, "models": {"yolo": {"arrival": 1711329844.923406, "serving": 1711329846.062455}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4364016, "arrival": 1711329844.2094078}, "models": {"yolo": {"arrival": 1711329843.8559685, "serving": 1711329844.1960704}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4685986, "arrival": 1711329844.7218273}, "models": {"yolo": {"arrival": 1711329844.3162532, "serving": 1711329844.6933634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4535258, "arrival": 1711329844.6412199}, "models": {"yolo": {"arrival": 1711329844.202703, "serving": 1711329844.6202192}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4682286, "arrival": 1711329844.0691085}, "models": {"yolo": {"arrival": 1711329843.5384712, "serving": 1711329844.0427692}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4317827, "arrival": 1711329841.6837988}, "models": {"yolo": {"arrival": 1711329841.2778516, "serving": 1711329841.6247783}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.432068, "arrival": 1711329843.9241135}, "models": {"yolo": {"arrival": 1711329843.5401025, "serving": 1711329843.9045594}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4356823, "arrival": 1711329843.8876326}, "models": {"yolo": {"arrival": 1711329843.6886375, "serving": 1711329843.8553238}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.431975, "arrival": 1711329843.886329}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.611415033+00:00\"}\"\n>", "times": {"request": {"sending": 1711329837.4182596}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4360642, "arrival": 1711329844.9473586}, "models": {"yolo": {"arrival": 1711329844.6216085, "serving": 1711329844.9118588}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4178832, "arrival": 1711329844.6286736}, "models": {"yolo": {"arrival": 1711329844.39547, "serving": 1711329844.6139708}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4535015, "arrival": 1711329842.3898215}, "models": {"yolo": {"arrival": 1711329842.0228128, "serving": 1711329842.3352203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4532256, "arrival": 1711329843.5755854}, "models": {"yolo": {"arrival": 1711329843.0956893, "serving": 1711329843.5284462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.418052, "arrival": 1711329843.1296334}, "models": {"yolo": {"arrival": 1711329842.74912, "serving": 1711329843.0849333}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4181774, "arrival": 1711329843.7087314}, "models": {"yolo": {"arrival": 1711329843.0838504, "serving": 1711329843.6785}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.4961772+00:00\"}\"\n>", "times": {"request": {"sending": 1711329837.4806828}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4684916, "arrival": 1711329844.7102768}, "models": {"yolo": {"arrival": 1711329844.3119266, "serving": 1711329844.683989}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4362092, "arrival": 1711329843.8935509}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4356282, "arrival": 1711329843.8922775}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.418491, "arrival": 1711329843.7092829}, "models": {"yolo": {"arrival": 1711329843.0838504, "serving": 1711329843.6785}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4183447, "arrival": 1711329843.56072}, "models": {"yolo": {"arrival": 1711329843.0877025, "serving": 1711329843.5297189}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.45329, "arrival": 1711329846.0776544}, "models": {"yolo": {"arrival": 1711329844.923406, "serving": 1711329846.062455}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4361315, "arrival": 1711329843.925854}, "models": {"yolo": {"arrival": 1711329843.5401025, "serving": 1711329843.9045594}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.626477502+00:00\"}\"\n>", "times": {"request": {"sending": 1711329837.4364676}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4534488, "arrival": 1711329844.3458495}, "models": {"yolo": {"arrival": 1711329843.9171364, "serving": 1711329844.3075583}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4360418, "arrival": 1711329843.8918405}, "models": {"yolo": {"arrival": 1711329843.6886375, "serving": 1711329843.8553238}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4533694, "arrival": 1711329842.3896031}, "models": {"yolo": {"arrival": 1711329842.0228128, "serving": 1711329842.3352203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4533198, "arrival": 1711329844.3403842}, "models": {"yolo": {"arrival": 1711329843.9171364, "serving": 1711329844.3075583}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.626501174+00:00\"}\"\n>", "times": {"request": {"sending": 1711329837.453044}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4686136, "arrival": 1711329844.712769}, "models": {"yolo": {"arrival": 1711329844.3119266, "serving": 1711329844.683989}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4363794, "arrival": 1711329842.0508327}, "models": {"yolo": {"arrival": 1711329841.6351824, "serving": 1711329842.013272}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4805932, "arrival": 1711329846.4863303}, "models": {"yolo": {"arrival": 1711329846.0718527, "serving": 1711329846.462087}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4357748, "arrival": 1711329843.9248462}, "models": {"yolo": {"arrival": 1711329843.5401025, "serving": 1711329843.9045594}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4364243, "arrival": 1711329844.5253415}, "models": {"yolo": {"arrival": 1711329843.86428, "serving": 1711329844.496366}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.432106, "arrival": 1711329842.042587}, "models": {"yolo": {"arrival": 1711329841.6351824, "serving": 1711329842.013272}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4528806, "arrival": 1711329843.5633945}, "models": {"yolo": {"arrival": 1711329843.0956893, "serving": 1711329843.5284462}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.626524659+00:00\"}\"\n>", "times": {"request": {"sending": 1711329837.4531786}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.626385716+00:00\"}\"\n>", "times": {"request": {"sending": 1711329837.4360862}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.435751, "arrival": 1711329843.932874}, "models": {"yolo": {"arrival": 1711329843.5368364, "serving": 1711329843.9070437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.436513, "arrival": 1711329843.9366841}, "models": {"yolo": {"arrival": 1711329843.5401025, "serving": 1711329843.9045594}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4531934, "arrival": 1711329844.3397653}, "models": {"yolo": {"arrival": 1711329843.9171364, "serving": 1711329844.3075583}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4682446, "arrival": 1711329842.3927011}, "models": {"yolo": {"arrival": 1711329842.0228128, "serving": 1711329842.3352203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4357057, "arrival": 1711329844.9423757}, "models": {"yolo": {"arrival": 1711329844.6216085, "serving": 1711329844.9118588}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.468276, "arrival": 1711329844.6883}, "models": {"yolo": {"arrival": 1711329844.5051458, "serving": 1711329844.67437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4317546, "arrival": 1711329843.1318586}, "models": {"yolo": {"arrival": 1711329842.74912, "serving": 1711329843.0849333}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.453027, "arrival": 1711329846.07426}, "models": {"yolo": {"arrival": 1711329844.923406, "serving": 1711329846.062455}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.436252, "arrival": 1711329844.9476395}, "models": {"yolo": {"arrival": 1711329844.6216085, "serving": 1711329844.9118588}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4687202, "arrival": 1711329844.723274}, "models": {"yolo": {"arrival": 1711329844.3162532, "serving": 1711329844.6933634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4361877, "arrival": 1711329842.0437944}, "models": {"yolo": {"arrival": 1711329841.6351824, "serving": 1711329842.013272}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4184384, "arrival": 1711329841.676832}, "models": {"yolo": {"arrival": 1711329841.2778516, "serving": 1711329841.6247783}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4532104, "arrival": 1711329844.3229103}, "models": {"yolo": {"arrival": 1711329843.913474, "serving": 1711329844.305147}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4682603, "arrival": 1711329844.6457493}, "models": {"yolo": {"arrival": 1711329844.202703, "serving": 1711329844.6202192}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4320116, "arrival": 1711329844.941863}, "models": {"yolo": {"arrival": 1711329844.6216085, "serving": 1711329844.9118588}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.453739, "arrival": 1711329844.0684824}, "models": {"yolo": {"arrival": 1711329843.5384712, "serving": 1711329844.0427692}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.468675, "arrival": 1711329844.6932147}, "models": {"yolo": {"arrival": 1711329844.5051458, "serving": 1711329844.67437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4359536, "arrival": 1711329843.9253619}, "models": {"yolo": {"arrival": 1711329843.5401025, "serving": 1711329843.9045594}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.43185, "arrival": 1711329844.9414115}, "models": {"yolo": {"arrival": 1711329844.6216085, "serving": 1711329844.9118588}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.611492233+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329837.431871}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4681606, "arrival": 1711329846.0859625}, "models": {"yolo": {"arrival": 1711329844.923406, "serving": 1711329846.062455}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.626639779+00:00\"}\"\n>", "times": {"request": {"sending": 1711329837.468307}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4320865, "arrival": 1711329843.1348846}, "models": {"yolo": {"arrival": 1711329842.74912, "serving": 1711329843.0849333}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.468212, "arrival": 1711329844.3375728}, "models": {"yolo": {"arrival": 1711329843.913474, "serving": 1711329844.305147}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4687824, "arrival": 1711329844.9180374}, "models": {"yolo": {"arrival": 1711329844.6302068, "serving": 1711329844.8931458}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4534705, "arrival": 1711329844.3244529}, "models": {"yolo": {"arrival": 1711329843.913474, "serving": 1711329844.305147}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4686294, "arrival": 1711329844.0715725}, "models": {"yolo": {"arrival": 1711329843.5384712, "serving": 1711329844.0427692}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4685519, "arrival": 1711329844.6922784}, "models": {"yolo": {"arrival": 1711329844.5051458, "serving": 1711329844.67437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4684753, "arrival": 1711329844.7214432}, "models": {"yolo": {"arrival": 1711329844.3162532, "serving": 1711329844.6933634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4534867, "arrival": 1711329844.0677679}, "models": {"yolo": {"arrival": 1711329843.5384712, "serving": 1711329844.0427692}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4534144, "arrival": 1711329846.0822685}, "models": {"yolo": {"arrival": 1711329844.923406, "serving": 1711329846.062455}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4531462, "arrival": 1711329844.5270858}, "models": {"yolo": {"arrival": 1711329843.86428, "serving": 1711329844.496366}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4364457, "arrival": 1711329844.9479258}, "models": {"yolo": {"arrival": 1711329844.6216085, "serving": 1711329844.9118588}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.626548057+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329837.453305}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.431662, "arrival": 1711329843.5725415}, "models": {"yolo": {"arrival": 1711329843.0877025, "serving": 1711329843.5297189}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4318066, "arrival": 1711329843.885557}, "models": {"yolo": {"arrival": 1711329843.082282, "serving": 1711329843.844235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.468127, "arrival": 1711329844.5380404}, "models": {"yolo": {"arrival": 1711329843.86428, "serving": 1711329844.496366}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.431915, "arrival": 1711329843.573399}, "models": {"yolo": {"arrival": 1711329843.0877025, "serving": 1711329843.5297189}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4687982, "arrival": 1711329844.6941237}, "models": {"yolo": {"arrival": 1711329844.5051458, "serving": 1711329844.67437}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:57.626325462+00:00\"}\"\n>", "times": {"request": {"sending": 1711329837.4359078}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4532428, "arrival": 1711329842.389284}, "models": {"yolo": {"arrival": 1711329842.0228128, "serving": 1711329842.3352203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4359758, "arrival": 1711329843.5614426}, "models": {"yolo": {"arrival": 1711329843.0956893, "serving": 1711329843.5284462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4685678, "arrival": 1711329846.4847343}, "models": {"yolo": {"arrival": 1711329846.0718527, "serving": 1711329846.462087}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:57.611529101+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329837.4320302}}, "outputs": []}, {"times": {"request": {"sending": 1711329837.4318283, "arrival": 1711329843.7098265}, "models": {"yolo": {"arrival": 1711329843.0838504, "serving": 1711329843.6785}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4319348, "arrival": 1711329843.1345522}, "models": {"yolo": {"arrival": 1711329842.74912, "serving": 1711329843.0849333}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.432048, "arrival": 1711329843.9324203}, "models": {"yolo": {"arrival": 1711329843.5368364, "serving": 1711329843.9070437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4362307, "arrival": 1711329844.5244515}, "models": {"yolo": {"arrival": 1711329843.86428, "serving": 1711329844.496366}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.43649, "arrival": 1711329844.3384635}, "models": {"yolo": {"arrival": 1711329843.9171364, "serving": 1711329844.3075583}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329837.4319935, "arrival": 1711329843.8870237}, "models": {"yolo": {"arrival": 1711329843.6886375, "serving": 1711329843.8553238}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:58.496053935+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329837.4685836}}, "outputs": []}], [{"times": {"request": {"sending": 1711329838.411956, "arrival": 1711329848.1568449}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4331605, "arrival": 1711329844.847792}, "models": {"yolo": {"arrival": 1711329844.2519171, "serving": 1711329844.8268993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4115586, "arrival": 1711329842.775498}, "models": {"yolo": {"arrival": 1711329842.3465676, "serving": 1711329842.7420654}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.411762, "arrival": 1711329844.7241104}, "models": {"yolo": {"arrival": 1711329844.3162532, "serving": 1711329844.6933634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4379976, "arrival": 1711329846.0584986}, "models": {"yolo": {"arrival": 1711329844.9041383, "serving": 1711329846.02991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.411356, "arrival": 1711329844.7237186}, "models": {"yolo": {"arrival": 1711329844.3162532, "serving": 1711329844.6933634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4467268, "arrival": 1711329843.5686905}, "models": {"yolo": {"arrival": 1711329843.0934505, "serving": 1711329843.528653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4125245, "arrival": 1711329848.1601484}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.44687, "arrival": 1711329848.2048979}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.446504, "arrival": 1711329848.2037563}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4124463, "arrival": 1711329844.2625155}, "models": {"yolo": {"arrival": 1711329844.050457, "serving": 1711329844.241943}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4114616, "arrival": 1711329844.71415}, "models": {"yolo": {"arrival": 1711329844.3119266, "serving": 1711329844.683989}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4464254, "arrival": 1711329848.2276056}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4379308, "arrival": 1711329848.2088046}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.447392, "arrival": 1711329848.248923}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.447364, "arrival": 1711329847.326311}, "models": {"yolo": {"arrival": 1711329847.060201, "serving": 1711329847.2967327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.437747, "arrival": 1711329848.1741455}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.437981, "arrival": 1711329843.1314373}, "models": {"yolo": {"arrival": 1711329842.7506578, "serving": 1711329843.0814888}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:58.660410345+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329838.4472613}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.4464571, "arrival": 1711329844.853555}, "models": {"yolo": {"arrival": 1711329844.2519171, "serving": 1711329844.8268993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4463658, "arrival": 1711329848.2031593}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4127097, "arrival": 1711329846.0519714}, "models": {"yolo": {"arrival": 1711329844.9041383, "serving": 1711329846.02991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4119947, "arrival": 1711329846.6933136}, "models": {"yolo": {"arrival": 1711329846.4700754, "serving": 1711329846.6819446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4115145, "arrival": 1711329844.2598917}, "models": {"yolo": {"arrival": 1711329844.050457, "serving": 1711329844.241943}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4469635, "arrival": 1711329843.5691388}, "models": {"yolo": {"arrival": 1711329843.0934505, "serving": 1711329843.528653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.446246, "arrival": 1711329847.0698357}, "models": {"yolo": {"arrival": 1711329846.6906073, "serving": 1711329847.0496528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4121873, "arrival": 1711329842.7809224}, "models": {"yolo": {"arrival": 1711329842.3465676, "serving": 1711329842.7420654}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4124193, "arrival": 1711329848.1771276}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4330812, "arrival": 1711329846.6993108}, "models": {"yolo": {"arrival": 1711329846.4700754, "serving": 1711329846.6819446}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:58.653119245+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329838.4377828}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.4767516, "arrival": 1711329848.2295291}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.447112, "arrival": 1711329848.2262645}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.447175, "arrival": 1711329848.231148}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4123013, "arrival": 1711329846.6958604}, "models": {"yolo": {"arrival": 1711329846.4700754, "serving": 1711329846.6819446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4116411, "arrival": 1711329844.6951685}, "models": {"yolo": {"arrival": 1711329844.5051458, "serving": 1711329844.67437}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.653165358+00:00\"}\"\n>", "times": {"request": {"sending": 1711329838.4380453}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.4467425, "arrival": 1711329846.484432}, "models": {"yolo": {"arrival": 1711329846.03784, "serving": 1711329846.4609284}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.447189, "arrival": 1711329844.944144}, "models": {"yolo": {"arrival": 1711329844.8369758, "serving": 1711329844.9128976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4464884, "arrival": 1711329846.4837928}, "models": {"yolo": {"arrival": 1711329846.03784, "serving": 1711329846.4609284}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.433124, "arrival": 1711329844.9281533}, "models": {"yolo": {"arrival": 1711329844.6997845, "serving": 1711329844.897544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4466152, "arrival": 1711329846.4841104}, "models": {"yolo": {"arrival": 1711329846.03784, "serving": 1711329846.4609284}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4116805, "arrival": 1711329846.4866366}, "models": {"yolo": {"arrival": 1711329846.0718527, "serving": 1711329846.462087}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.446228, "arrival": 1711329848.17666}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4472904, "arrival": 1711329848.246967}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4122634, "arrival": 1711329848.1580377}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4471602, "arrival": 1711329848.2479577}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4126832, "arrival": 1711329843.1243134}, "models": {"yolo": {"arrival": 1711329842.7506578, "serving": 1711329843.0814888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4118009, "arrival": 1711329844.71987}, "models": {"yolo": {"arrival": 1711329844.3119266, "serving": 1711329844.683989}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4475427, "arrival": 1711329846.6483188}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4461925, "arrival": 1711329843.1325355}, "models": {"yolo": {"arrival": 1711329842.7506578, "serving": 1711329843.0814888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4126573, "arrival": 1711329844.8434463}, "models": {"yolo": {"arrival": 1711329844.2519171, "serving": 1711329844.8268993}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.653187821+00:00\"}\"\n>", "times": {"request": {"sending": 1711329838.4462636}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.4378188, "arrival": 1711329848.2041361}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4474843, "arrival": 1711329847.32742}, "models": {"yolo": {"arrival": 1711329847.060201, "serving": 1711329847.2967327}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.496274236+00:00\"}\"\n>", "times": {"request": {"sending": 1711329838.4120328}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.4462812, "arrival": 1711329848.2094564}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4115999, "arrival": 1711329844.9219499}, "models": {"yolo": {"arrival": 1711329844.6302068, "serving": 1711329844.8931458}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4472177, "arrival": 1711329846.862196}, "models": {"yolo": {"arrival": 1711329846.4722116, "serving": 1711329846.83098}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4822357, "arrival": 1711329847.3283696}, "models": {"yolo": {"arrival": 1711329847.060201, "serving": 1711329847.2967327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.437729, "arrival": 1711329846.0575817}, "models": {"yolo": {"arrival": 1711329844.9041383, "serving": 1711329846.02991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.446314, "arrival": 1711329844.8531024}, "models": {"yolo": {"arrival": 1711329844.2519171, "serving": 1711329844.8268993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.447378, "arrival": 1711329839.9040048}, "models": {"yolo": {"arrival": 1711329839.447196, "serving": 1711329839.8823073}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4375815, "arrival": 1711329844.9318464}, "models": {"yolo": {"arrival": 1711329844.6997845, "serving": 1711329844.897544}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.653076254+00:00\"}\"\n>", "times": {"request": {"sending": 1711329838.433242}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.432987, "arrival": 1711329848.1606517}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4475713, "arrival": 1711329846.8641076}, "models": {"yolo": {"arrival": 1711329846.4722116, "serving": 1711329846.83098}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.411879, "arrival": 1711329842.7762032}, "models": {"yolo": {"arrival": 1711329842.3465676, "serving": 1711329842.7420654}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4380288, "arrival": 1711329847.068567}, "models": {"yolo": {"arrival": 1711329846.6906073, "serving": 1711329847.0496528}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.496244467+00:00\"}\"\n>", "times": {"request": {"sending": 1711329838.4117212}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.4474225, "arrival": 1711329846.6441307}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.660387485+00:00\"}\"\n>", "times": {"request": {"sending": 1711329838.4471455}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.647970222+00:00\"}\"\n>", "times": {"request": {"sending": 1711329838.4125767}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.411918, "arrival": 1711329844.9325676}, "models": {"yolo": {"arrival": 1711329844.6302068, "serving": 1711329844.8931458}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.660291174+00:00\"}\"\n>", "times": {"request": {"sending": 1711329838.446535}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.4121492, "arrival": 1711329844.2616053}, "models": {"yolo": {"arrival": 1711329844.050457, "serving": 1711329844.241943}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.446298, "arrival": 1711329848.2062798}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4465683, "arrival": 1711329848.227263}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4122255, "arrival": 1711329844.9330173}, "models": {"yolo": {"arrival": 1711329844.6302068, "serving": 1711329844.8931458}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.412073, "arrival": 1711329844.9232485}, "models": {"yolo": {"arrival": 1711329844.6997845, "serving": 1711329844.897544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4466932, "arrival": 1711329848.2298698}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.412394, "arrival": 1711329844.924729}, "models": {"yolo": {"arrival": 1711329844.6997845, "serving": 1711329844.897544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4463468, "arrival": 1711329846.483267}, "models": {"yolo": {"arrival": 1711329846.03784, "serving": 1711329846.4609284}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4463818, "arrival": 1711329847.0750072}, "models": {"yolo": {"arrival": 1711329846.6906073, "serving": 1711329847.0496528}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:58.647903685+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329838.4123397}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.4378986, "arrival": 1711329847.0680666}, "models": {"yolo": {"arrival": 1711329846.6906073, "serving": 1711329847.0496528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.412111, "arrival": 1711329844.7205439}, "models": {"yolo": {"arrival": 1711329844.3119266, "serving": 1711329844.683989}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4469347, "arrival": 1711329848.230244}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4470382, "arrival": 1711329848.2328286}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4472036, "arrival": 1711329843.5738297}, "models": {"yolo": {"arrival": 1711329843.0934505, "serving": 1711329843.528653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4118404, "arrival": 1711329844.2608252}, "models": {"yolo": {"arrival": 1711329844.050457, "serving": 1711329844.241943}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4125504, "arrival": 1711329846.6983066}, "models": {"yolo": {"arrival": 1711329846.4700754, "serving": 1711329846.6819446}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.446677, "arrival": 1711329848.23206}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.447469, "arrival": 1711329848.2290761}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4124982, "arrival": 1711329846.050754}, "models": {"yolo": {"arrival": 1711329844.9041383, "serving": 1711329846.02991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.412604, "arrival": 1711329844.927223}, "models": {"yolo": {"arrival": 1711329844.6997845, "serving": 1711329844.897544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.447082, "arrival": 1711329843.5729992}, "models": {"yolo": {"arrival": 1711329843.0934505, "serving": 1711329843.528653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4377646, "arrival": 1711329847.0672028}, "models": {"yolo": {"arrival": 1711329846.6906073, "serving": 1711329847.0496528}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:58.660342914+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329838.4469042}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.4473202, "arrival": 1711329843.9124823}, "models": {"yolo": {"arrival": 1711329843.537913, "serving": 1711329843.9023178}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4378653, "arrival": 1711329846.0580428}, "models": {"yolo": {"arrival": 1711329844.9041383, "serving": 1711329846.02991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4466453, "arrival": 1711329847.0775695}, "models": {"yolo": {"arrival": 1711329846.6906073, "serving": 1711329847.0496528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4378824, "arrival": 1711329848.1749344}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.446584, "arrival": 1711329844.8541393}, "models": {"yolo": {"arrival": 1711329844.2519171, "serving": 1711329844.8268993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.412472, "arrival": 1711329842.7813947}, "models": {"yolo": {"arrival": 1711329842.3465676, "serving": 1711329842.7420654}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4474382, "arrival": 1711329843.9147918}, "models": {"yolo": {"arrival": 1711329843.537913, "serving": 1711329843.9023178}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.660235708+00:00\"}\"\n>", "times": {"request": {"sending": 1711329838.4464083}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.4376671, "arrival": 1711329848.181074}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4471283, "arrival": 1711329847.3210747}, "models": {"yolo": {"arrival": 1711329847.060201, "serving": 1711329847.2967327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4472759, "arrival": 1711329848.248679}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4467106, "arrival": 1711329844.9429264}, "models": {"yolo": {"arrival": 1711329844.8369758, "serving": 1711329844.9128976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4331422, "arrival": 1711329848.1805007}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4380128, "arrival": 1711329848.1759644}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4462106, "arrival": 1711329846.058898}, "models": {"yolo": {"arrival": 1711329844.9041383, "serving": 1711329846.02991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4470527, "arrival": 1711329848.2306607}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4332108, "arrival": 1711329848.1611662}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4331787, "arrival": 1711329843.1246488}, "models": {"yolo": {"arrival": 1711329842.7506578, "serving": 1711329843.0814888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4332268, "arrival": 1711329847.0638483}, "models": {"yolo": {"arrival": 1711329846.6906073, "serving": 1711329847.0496528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4465518, "arrival": 1711329848.2316198}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.660365628+00:00\"}\"\n>", "times": {"request": {"sending": 1711329838.4470239}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.41263, "arrival": 1711329848.1796665}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:23:58.653143978+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329838.4379156}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.446056, "arrival": 1711329848.2091305}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4466002, "arrival": 1711329843.5671086}, "models": {"yolo": {"arrival": 1711329843.0934505, "serving": 1711329843.528653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4474082, "arrival": 1711329848.2472138}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4473505, "arrival": 1711329848.2287462}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4464414, "arrival": 1711329848.2067757}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4469774, "arrival": 1711329846.855698}, "models": {"yolo": {"arrival": 1711329846.4722116, "serving": 1711329846.83098}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4461732, "arrival": 1711329844.8511946}, "models": {"yolo": {"arrival": 1711329844.2519171, "serving": 1711329844.8268993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4379478, "arrival": 1711329848.2055962}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4475145, "arrival": 1711329848.2491693}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4474993, "arrival": 1711329839.904758}, "models": {"yolo": {"arrival": 1711329839.447196, "serving": 1711329839.8823073}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4469197, "arrival": 1711329848.2325008}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.446949, "arrival": 1711329844.9433484}, "models": {"yolo": {"arrival": 1711329844.8369758, "serving": 1711329844.9128976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4378033, "arrival": 1711329848.2084675}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.64799967+00:00\"}\"\n>", "times": {"request": {"sending": 1711329838.4331036}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.4475565, "arrival": 1711329843.9153566}, "models": {"yolo": {"arrival": 1711329843.537913, "serving": 1711329843.9023178}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4469938, "arrival": 1711329848.205253}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4379644, "arrival": 1711329844.8507428}, "models": {"yolo": {"arrival": 1711329844.2519171, "serving": 1711329844.8268993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.482308, "arrival": 1711329839.9068527}, "models": {"yolo": {"arrival": 1711329839.447196, "serving": 1711329839.8823073}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4463305, "arrival": 1711329843.1328418}, "models": {"yolo": {"arrival": 1711329842.7506578, "serving": 1711329843.0814888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.446887, "arrival": 1711329847.0781145}, "models": {"yolo": {"arrival": 1711329846.6906073, "serving": 1711329847.0496528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4473062, "arrival": 1711329844.9469435}, "models": {"yolo": {"arrival": 1711329844.8369758, "serving": 1711329844.9128976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4464734, "arrival": 1711329843.5665085}, "models": {"yolo": {"arrival": 1711329843.0934505, "serving": 1711329843.528653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4331954, "arrival": 1711329846.0568252}, "models": {"yolo": {"arrival": 1711329844.9041383, "serving": 1711329846.02991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4472327, "arrival": 1711329848.226925}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4474547, "arrival": 1711329846.8634884}, "models": {"yolo": {"arrival": 1711329846.4722116, "serving": 1711329846.83098}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.447529, "arrival": 1711329848.2474606}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4461474, "arrival": 1711329848.2059407}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4470088, "arrival": 1711329847.317547}, "models": {"yolo": {"arrival": 1711329847.060201, "serving": 1711329847.2967327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.447067, "arrival": 1711329844.9437237}, "models": {"yolo": {"arrival": 1711329844.8369758, "serving": 1711329844.9128976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4473352, "arrival": 1711329846.8628461}, "models": {"yolo": {"arrival": 1711329846.4722116, "serving": 1711329846.83098}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4465196, "arrival": 1711329847.0756633}, "models": {"yolo": {"arrival": 1711329846.6906073, "serving": 1711329847.0496528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4466302, "arrival": 1711329848.2045035}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4470968, "arrival": 1711329846.861385}, "models": {"yolo": {"arrival": 1711329846.4722116, "serving": 1711329846.83098}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.437849, "arrival": 1711329843.1254287}, "models": {"yolo": {"arrival": 1711329842.7506578, "serving": 1711329843.0814888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4472468, "arrival": 1711329847.322265}, "models": {"yolo": {"arrival": 1711329847.060201, "serving": 1711329847.2967327}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.58 GiB already allocated; 80.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:23:58.660318389+00:00\"}\"\n>", "times": {"request": {"sending": 1711329838.4466615}}, "outputs": []}, {"times": {"request": {"sending": 1711329838.4377086, "arrival": 1711329843.1250386}, "models": {"yolo": {"arrival": 1711329842.7506578, "serving": 1711329843.0814888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4376888, "arrival": 1711329844.8486826}, "models": {"yolo": {"arrival": 1711329844.2519171, "serving": 1711329844.8268993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329838.4378338, "arrival": 1711329844.84913}, "models": {"yolo": {"arrival": 1711329844.2519171, "serving": 1711329844.8268993}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329839.415429, "arrival": 1711329848.2463303}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4581597, "arrival": 1711329844.9342391}, "models": {"yolo": {"arrival": 1711329844.6907456, "serving": 1711329844.8968353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4155896, "arrival": 1711329846.6605418}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4161036, "arrival": 1711329846.6620636}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4159083, "arrival": 1711329847.0579622}, "models": {"yolo": {"arrival": 1711329846.8409312, "serving": 1711329847.0360005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.446511, "arrival": 1711329847.9042633}, "models": {"yolo": {"arrival": 1711329847.747887, "serving": 1711329847.8614392}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4584222, "arrival": 1711329847.6308718}, "models": {"yolo": {"arrival": 1711329847.3505187, "serving": 1711329847.6053379}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4161692, "arrival": 1711329847.0585322}, "models": {"yolo": {"arrival": 1711329846.8409312, "serving": 1711329847.0360005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4145706, "arrival": 1711329843.9277039}, "models": {"yolo": {"arrival": 1711329843.537913, "serving": 1711329843.9023178}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4143777, "arrival": 1711329848.249511}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.414639, "arrival": 1711329848.2454305}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4581292, "arrival": 1711329848.2861907}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4150379, "arrival": 1711329848.2573018}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.44628, "arrival": 1711329847.9036784}, "models": {"yolo": {"arrival": 1711329847.747887, "serving": 1711329847.8614392}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4149706, "arrival": 1711329839.9080575}, "models": {"yolo": {"arrival": 1711329839.447196, "serving": 1711329839.8823073}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4146056, "arrival": 1711329846.8647237}, "models": {"yolo": {"arrival": 1711329846.4722116, "serving": 1711329846.83098}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:00.530597936+00:00\"}\"\n>", "times": {"request": {"sending": 1711329839.4460964}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4578853, "arrival": 1711329848.280267}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4153972, "arrival": 1711329847.0545404}, "models": {"yolo": {"arrival": 1711329846.8409312, "serving": 1711329847.0360005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4433103, "arrival": 1711329846.6669714}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4165456, "arrival": 1711329848.2688863}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4158134, "arrival": 1711329848.2587512}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4145327, "arrival": 1711329846.6494}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.415202, "arrival": 1711329847.7701504}, "models": {"yolo": {"arrival": 1711329847.3086545, "serving": 1711329847.7383235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4433634, "arrival": 1711329848.2556422}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4150054, "arrival": 1711329848.256338}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.416202, "arrival": 1711329848.255036}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:00.521079062+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329839.4157488}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4157817, "arrival": 1711329848.257681}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:00.52111102+00:00\"}\"\n>", "times": {"request": {"sending": 1711329839.4160047}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4583716, "arrival": 1711329847.251355}, "models": {"yolo": {"arrival": 1711329846.7594318, "serving": 1711329847.2350764}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4458692, "arrival": 1711329848.265955}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4156852, "arrival": 1711329848.2466004}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4460492, "arrival": 1711329848.2661924}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4576693, "arrival": 1711329847.3662848}, "models": {"yolo": {"arrival": 1711329847.0450435, "serving": 1711329847.3390503}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4148724, "arrival": 1711329846.865125}, "models": {"yolo": {"arrival": 1711329846.4722116, "serving": 1711329846.83098}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4584548, "arrival": 1711329848.2352107}, "models": {"yolo": {"arrival": 1711329847.8746598, "serving": 1711329848.1814575}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:00.53065441+00:00\"}\"\n>", "times": {"request": {"sending": 1711329839.4465284}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4578345, "arrival": 1711329847.9104111}, "models": {"yolo": {"arrival": 1711329847.747887, "serving": 1711329847.8614392}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.457689, "arrival": 1711329848.2670186}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.415845, "arrival": 1711329846.661275}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.415365, "arrival": 1711329844.3409882}, "models": {"yolo": {"arrival": 1711329843.9105089, "serving": 1711329844.3077958}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:00.521049967+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329839.4154925}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4580834, "arrival": 1711329848.2343113}, "models": {"yolo": {"arrival": 1711329847.8746598, "serving": 1711329848.1814575}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4160385, "arrival": 1711329848.267246}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.443329, "arrival": 1711329844.3470664}, "models": {"yolo": {"arrival": 1711329843.9105089, "serving": 1711329844.3077958}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4150703, "arrival": 1711329846.650762}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4584842, "arrival": 1711329848.2854838}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.458503, "arrival": 1711329848.2869732}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4462543, "arrival": 1711329848.2665534}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.458207, "arrival": 1711329848.234771}, "models": {"yolo": {"arrival": 1711329847.8746598, "serving": 1711329848.1814575}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4161363, "arrival": 1711329844.3454435}, "models": {"yolo": {"arrival": 1711329843.9105089, "serving": 1711329844.3077958}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4579163, "arrival": 1711329844.9334304}, "models": {"yolo": {"arrival": 1711329844.6907456, "serving": 1711329844.8968353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.457772, "arrival": 1711329846.7795825}, "models": {"yolo": {"arrival": 1711329846.6353252, "serving": 1711329846.748107}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.416071, "arrival": 1711329848.2674813}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.414839, "arrival": 1711329843.9282885}, "models": {"yolo": {"arrival": 1711329843.537913, "serving": 1711329843.9023178}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4146729, "arrival": 1711329847.7659023}, "models": {"yolo": {"arrival": 1711329847.3086545, "serving": 1711329847.7383235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4579911, "arrival": 1711329848.2843354}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:00.530837211+00:00\"}\"\n>", "times": {"request": {"sending": 1711329839.4765103}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4156537, "arrival": 1711329847.0573199}, "models": {"yolo": {"arrival": 1711329846.8409312, "serving": 1711329847.0360005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:00.530518728+00:00\"}\"\n>", "times": {"request": {"sending": 1711329839.4459152}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.414938, "arrival": 1711329847.767234}, "models": {"yolo": {"arrival": 1711329847.3086545, "serving": 1711329847.7383235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.41517, "arrival": 1711329848.24608}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4147408, "arrival": 1711329848.2559848}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4144878, "arrival": 1711329848.2477114}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4162345, "arrival": 1711329847.7802918}, "models": {"yolo": {"arrival": 1711329847.3086545, "serving": 1711329847.7383235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.457868, "arrival": 1711329848.283894}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4583557, "arrival": 1711329848.2866125}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.458387, "arrival": 1711329844.9346595}, "models": {"yolo": {"arrival": 1711329844.6907456, "serving": 1711329844.8968353}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:00.530758724+00:00\"}\"\n>", "times": {"request": {"sending": 1711329839.4580984}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4432878, "arrival": 1711329848.2692382}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:00.530813331+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329839.45847}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4155576, "arrival": 1711329848.2583811}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.414905, "arrival": 1711329848.245816}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4156215, "arrival": 1711329844.341596}, "models": {"yolo": {"arrival": 1711329843.9105089, "serving": 1711329844.3077958}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4154606, "arrival": 1711329847.7770305}, "models": {"yolo": {"arrival": 1711329847.3086545, "serving": 1711329847.7383235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.415137, "arrival": 1711329847.053536}, "models": {"yolo": {"arrival": 1711329846.8409312, "serving": 1711329847.0360005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4153328, "arrival": 1711329846.6598496}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4576466, "arrival": 1711329844.708595}, "models": {"yolo": {"arrival": 1711329844.3168905, "serving": 1711329844.6816669}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4462242, "arrival": 1711329847.3626812}, "models": {"yolo": {"arrival": 1711329847.0450435, "serving": 1711329847.3390503}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:00.520930406+00:00\"}\"\n>", "times": {"request": {"sending": 1711329839.4152339}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4433472, "arrival": 1711329847.3505638}, "models": {"yolo": {"arrival": 1711329847.0450435, "serving": 1711329847.3390503}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.416611, "arrival": 1711329844.3462465}, "models": {"yolo": {"arrival": 1711329843.9105089, "serving": 1711329844.3077958}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4148061, "arrival": 1711329846.6500697}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4459593, "arrival": 1711329848.2698343}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:00.530785841+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329839.4582224}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.446382, "arrival": 1711329848.2794425}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4465444, "arrival": 1711329848.2787497}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4147062, "arrival": 1711329839.9075074}, "models": {"yolo": {"arrival": 1711329839.447196, "serving": 1711329839.8823073}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.415526, "arrival": 1711329848.256923}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4458065, "arrival": 1711329844.698748}, "models": {"yolo": {"arrival": 1711329844.3168905, "serving": 1711329844.6816669}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4159403, "arrival": 1711329848.2547321}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4463482, "arrival": 1711329848.2780237}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4151027, "arrival": 1711329843.9287846}, "models": {"yolo": {"arrival": 1711329843.537913, "serving": 1711329843.9023178}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.416708, "arrival": 1711329847.7821856}, "models": {"yolo": {"arrival": 1711329847.3086545, "serving": 1711329847.7383235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4153004, "arrival": 1711329848.2580311}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4461184, "arrival": 1711329848.2776642}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4577558, "arrival": 1711329848.2800355}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4152682, "arrival": 1711329848.2566845}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.443381, "arrival": 1711329847.7827516}, "models": {"yolo": {"arrival": 1711329847.3086545, "serving": 1711329847.7383235}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:00.521180411+00:00\"}\"\n>", "times": {"request": {"sending": 1711329839.4433963}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4166439, "arrival": 1711329847.3495653}, "models": {"yolo": {"arrival": 1711329847.0450435, "serving": 1711329847.3390503}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.415972, "arrival": 1711329847.7797215}, "models": {"yolo": {"arrival": 1711329847.3086545, "serving": 1711329847.7383235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4147732, "arrival": 1711329848.248447}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4157166, "arrival": 1711329847.7775517}, "models": {"yolo": {"arrival": 1711329847.3086545, "serving": 1711329847.7383235}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4583392, "arrival": 1711329848.285121}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4580233, "arrival": 1711329847.2496514}, "models": {"yolo": {"arrival": 1711329846.7594318, "serving": 1711329847.2350764}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4158769, "arrival": 1711329844.344949}, "models": {"yolo": {"arrival": 1711329843.9105089, "serving": 1711329844.3077958}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4580076, "arrival": 1711329848.285839}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4459379, "arrival": 1711329848.2769659}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4165792, "arrival": 1711329846.6629372}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4464936, "arrival": 1711329848.2667873}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:00.530709556+00:00\"}\"\n>", "times": {"request": {"sending": 1711329839.4578521}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4584363, "arrival": 1711329848.2773159}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:00.521133877+00:00\"}\"\n>", "times": {"request": {"sending": 1711329839.416268}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4585528, "arrival": 1711329847.6315918}, "models": {"yolo": {"arrival": 1711329847.3505187, "serving": 1711329847.6053379}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4577386, "arrival": 1711329848.2790997}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.457624, "arrival": 1711329846.7788398}, "models": {"yolo": {"arrival": 1711329846.6353252, "serving": 1711329846.748107}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4760573, "arrival": 1711329848.2482054}, "models": {"yolo": {"arrival": 1711329847.8746598, "serving": 1711329848.1814575}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4577887, "arrival": 1711329844.7097144}, "models": {"yolo": {"arrival": 1711329844.3168905, "serving": 1711329844.6816669}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4456453, "arrival": 1711329848.2685316}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4165075, "arrival": 1711329848.2678356}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.457539, "arrival": 1711329848.2797906}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:00.53062482+00:00\"}\"\n>", "times": {"request": {"sending": 1711329839.4463153}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.445838, "arrival": 1711329847.3596008}, "models": {"yolo": {"arrival": 1711329847.0450435, "serving": 1711329847.3390503}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.446164, "arrival": 1711329846.6689475}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4581757, "arrival": 1711329847.6291637}, "models": {"yolo": {"arrival": 1711329847.3505187, "serving": 1711329847.6053379}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4464257, "arrival": 1711329846.7767522}, "models": {"yolo": {"arrival": 1711329846.6353252, "serving": 1711329846.748107}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:00.521157018+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329839.4167404}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4460263, "arrival": 1711329847.3609257}, "models": {"yolo": {"arrival": 1711329847.0450435, "serving": 1711329847.3390503}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4461393, "arrival": 1711329848.2783747}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4578195, "arrival": 1711329848.2755208}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4457364, "arrival": 1711329848.2695835}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4464567, "arrival": 1711329844.702402}, "models": {"yolo": {"arrival": 1711329844.3168905, "serving": 1711329844.6816669}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4460044, "arrival": 1711329844.700984}, "models": {"yolo": {"arrival": 1711329844.3168905, "serving": 1711329844.6816669}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:00.530681948+00:00\"}\"\n>", "times": {"request": {"sending": 1711329839.4577239}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.445893, "arrival": 1711329847.901761}, "models": {"yolo": {"arrival": 1711329847.747887, "serving": 1711329847.8614392}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4579623, "arrival": 1711329848.2339911}, "models": {"yolo": {"arrival": 1711329847.8746598, "serving": 1711329848.1814575}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4580376, "arrival": 1711329844.9338374}, "models": {"yolo": {"arrival": 1711329844.6907456, "serving": 1711329844.8968353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4577074, "arrival": 1711329847.9097836}, "models": {"yolo": {"arrival": 1711329847.747887, "serving": 1711329847.8614392}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:00.53073281+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329839.4579775}}, "outputs": []}, {"times": {"request": {"sending": 1711329839.4579003, "arrival": 1711329846.7802227}, "models": {"yolo": {"arrival": 1711329846.6353252, "serving": 1711329846.748107}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4432025, "arrival": 1711329848.2681847}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.458144, "arrival": 1711329847.250642}, "models": {"yolo": {"arrival": 1711329846.7594318, "serving": 1711329847.2350764}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.458113, "arrival": 1711329848.2847242}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4579473, "arrival": 1711329848.27598}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4581916, "arrival": 1711329848.2766118}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4459822, "arrival": 1711329846.668036}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4580529, "arrival": 1711329847.628496}, "models": {"yolo": {"arrival": 1711329847.3505187, "serving": 1711329847.6053379}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.446193, "arrival": 1711329844.7017426}, "models": {"yolo": {"arrival": 1711329844.3168905, "serving": 1711329844.6816669}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4460723, "arrival": 1711329847.902323}, "models": {"yolo": {"arrival": 1711329847.747887, "serving": 1711329847.8614392}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.458568, "arrival": 1711329848.291169}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4585204, "arrival": 1711329847.2725878}, "models": {"yolo": {"arrival": 1711329846.7594318, "serving": 1711329847.2350764}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4579318, "arrival": 1711329847.627341}, "models": {"yolo": {"arrival": 1711329847.3505187, "serving": 1711329847.6053379}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4585369, "arrival": 1711329844.9369872}, "models": {"yolo": {"arrival": 1711329844.6907456, "serving": 1711329844.8968353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4166756, "arrival": 1711329848.2552898}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4580677, "arrival": 1711329848.276249}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4457731, "arrival": 1711329846.6675503}, "models": {"yolo": {"arrival": 1711329844.9260876, "serving": 1711329846.6231706}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4578042, "arrival": 1711329847.3674116}, "models": {"yolo": {"arrival": 1711329847.0450435, "serving": 1711329847.3390503}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329839.4464753, "arrival": 1711329847.3654137}, "models": {"yolo": {"arrival": 1711329847.0450435, "serving": 1711329847.3390503}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329840.4813285, "arrival": 1711329848.324186}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.414555, "arrival": 1711329846.2357748}, "models": {"yolo": {"arrival": 1711329844.9062595, "serving": 1711329846.2102892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4142516, "arrival": 1711329847.6328778}, "models": {"yolo": {"arrival": 1711329847.3505187, "serving": 1711329847.6053379}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.414211, "arrival": 1711329846.2348568}, "models": {"yolo": {"arrival": 1711329844.9062595, "serving": 1711329846.2102892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4556785, "arrival": 1711329848.3084629}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4151683, "arrival": 1711329846.240758}, "models": {"yolo": {"arrival": 1711329844.9062595, "serving": 1711329846.2102892}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.480327738+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329840.4817474}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4144804, "arrival": 1711329848.2969954}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.455876, "arrival": 1711329846.7721102}, "models": {"yolo": {"arrival": 1711329846.4787714, "serving": 1711329846.7455325}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4139893, "arrival": 1711329848.2922356}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.481544, "arrival": 1711329848.5573642}, "models": {"yolo": {"arrival": 1711329848.1687624, "serving": 1711329848.532028}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4146686, "arrival": 1711329848.2499857}, "models": {"yolo": {"arrival": 1711329847.8746598, "serving": 1711329848.1814575}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4322982, "arrival": 1711329848.2654202}, "models": {"yolo": {"arrival": 1711329847.8746598, "serving": 1711329848.1814575}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4141095, "arrival": 1711329848.2872248}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4816945, "arrival": 1711329848.5581584}, "models": {"yolo": {"arrival": 1711329848.1687624, "serving": 1711329848.532028}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4324882, "arrival": 1711329848.3067243}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4320636, "arrival": 1711329847.2797725}, "models": {"yolo": {"arrival": 1711329846.7594318, "serving": 1711329847.2350764}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.48139, "arrival": 1711329846.775194}, "models": {"yolo": {"arrival": 1711329846.4787714, "serving": 1711329846.7455325}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4558623, "arrival": 1711329847.892573}, "models": {"yolo": {"arrival": 1711329847.567168, "serving": 1711329847.8534331}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.455603, "arrival": 1711329848.3155644}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4150887, "arrival": 1711329848.2975519}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4142919, "arrival": 1711329848.2915013}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4148214, "arrival": 1711329847.2782667}, "models": {"yolo": {"arrival": 1711329846.7594318, "serving": 1711329847.2350764}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.414858, "arrival": 1711329846.2400165}, "models": {"yolo": {"arrival": 1711329844.9062595, "serving": 1711329846.2102892}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.470840721+00:00\"}\"\n>", "times": {"request": {"sending": 1711329840.4324708}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4148948, "arrival": 1711329847.9059784}, "models": {"yolo": {"arrival": 1711329847.6170814, "serving": 1711329847.8652205}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4815862, "arrival": 1711329848.9502184}, "models": {"yolo": {"arrival": 1711329848.5401397, "serving": 1711329848.900771}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4817119, "arrival": 1711329848.5375478}, "models": {"yolo": {"arrival": 1711329848.1632314, "serving": 1711329848.5134635}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4143317, "arrival": 1711329848.2497582}, "models": {"yolo": {"arrival": 1711329847.8746598, "serving": 1711329848.1814575}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4145925, "arrival": 1711329847.9057012}, "models": {"yolo": {"arrival": 1711329847.6170814, "serving": 1711329847.8652205}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4328604, "arrival": 1711329847.911346}, "models": {"yolo": {"arrival": 1711329847.6170814, "serving": 1711329847.8652205}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.432381, "arrival": 1711329847.2834067}, "models": {"yolo": {"arrival": 1711329846.7594318, "serving": 1711329847.2350764}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4556623, "arrival": 1711329848.214434}, "models": {"yolo": {"arrival": 1711329847.875249, "serving": 1711329848.1589327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4816253, "arrival": 1711329848.3246264}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4320219, "arrival": 1711329848.294042}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4554894, "arrival": 1711329848.320105}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4555197, "arrival": 1711329846.4924254}, "models": {"yolo": {"arrival": 1711329846.220493, "serving": 1711329846.4691665}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.455372, "arrival": 1711329848.3193164}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4552927, "arrival": 1711329848.2132185}, "models": {"yolo": {"arrival": 1711329847.875249, "serving": 1711329848.1589327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4817705, "arrival": 1711329848.327943}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4321802, "arrival": 1711329848.3021302}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.481137, "arrival": 1711329848.3239636}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4318984, "arrival": 1711329848.2504497}, "models": {"yolo": {"arrival": 1711329847.8746598, "serving": 1711329848.1814575}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.475522531+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329840.4557073}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4141634, "arrival": 1711329847.2735395}, "models": {"yolo": {"arrival": 1711329846.7594318, "serving": 1711329847.2350764}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4145184, "arrival": 1711329847.2772684}, "models": {"yolo": {"arrival": 1711329846.7594318, "serving": 1711329847.2350764}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4555035, "arrival": 1711329847.5807683}, "models": {"yolo": {"arrival": 1711329847.2497609, "serving": 1711329847.5579898}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4553103, "arrival": 1711329848.3075542}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4815104, "arrival": 1711329847.8965614}, "models": {"yolo": {"arrival": 1711329847.567168, "serving": 1711329847.8534331}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4818158, "arrival": 1711329847.1187267}, "models": {"yolo": {"arrival": 1711329846.7562096, "serving": 1711329847.09955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.455474, "arrival": 1711329848.3153276}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4811907, "arrival": 1711329847.8931694}, "models": {"yolo": {"arrival": 1711329847.567168, "serving": 1711329847.8534331}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.475499656+00:00\"}\"\n>", "times": {"request": {"sending": 1711329840.4555764}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4814315, "arrival": 1711329848.319542}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4550738, "arrival": 1711329848.3121428}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4146314, "arrival": 1711329848.2918766}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4555476, "arrival": 1711329848.3082469}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:00.530864068+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329840.414371}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4321418, "arrival": 1711329848.264932}, "models": {"yolo": {"arrival": 1711329847.8746598, "serving": 1711329848.1814575}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4147837, "arrival": 1711329848.297304}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.481411, "arrival": 1711329848.55676}, "models": {"yolo": {"arrival": 1711329848.1687624, "serving": 1711329848.532028}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.45562, "arrival": 1711329848.3204494}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4322758, "arrival": 1711329848.2982848}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.414442, "arrival": 1711329848.2930746}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4816408, "arrival": 1711329848.326499}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4551098, "arrival": 1711329847.5792737}, "models": {"yolo": {"arrival": 1711329847.2497609, "serving": 1711329847.5579898}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4325845, "arrival": 1711329848.2987843}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.475477406+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329840.4554594}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4322002, "arrival": 1711329848.3077872}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4557369, "arrival": 1711329848.3207753}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4325662, "arrival": 1711329847.9110856}, "models": {"yolo": {"arrival": 1711329847.6170814, "serving": 1711329847.8652205}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4149764, "arrival": 1711329848.2502046}, "models": {"yolo": {"arrival": 1711329847.8746598, "serving": 1711329848.1814575}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4325228, "arrival": 1711329847.2840822}, "models": {"yolo": {"arrival": 1711329846.7594318, "serving": 1711329847.2350764}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.475545148+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329840.4558208}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4557924, "arrival": 1711329848.3160105}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4151247, "arrival": 1711329847.279088}, "models": {"yolo": {"arrival": 1711329846.7594318, "serving": 1711329847.2350764}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4152415, "arrival": 1711329848.2928376}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.470764026+00:00\"}\"\n>", "times": {"request": {"sending": 1711329840.431992}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4320433, "arrival": 1711329848.2977912}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4149332, "arrival": 1711329848.292601}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:00.530900293+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329840.4147062}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4322565, "arrival": 1711329847.9101179}, "models": {"yolo": {"arrival": 1711329847.6170814, "serving": 1711329847.8652205}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.470695025+00:00\"}\"\n>", "times": {"request": {"sending": 1711329840.415013}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4810784, "arrival": 1711329848.946683}, "models": {"yolo": {"arrival": 1711329848.5401397, "serving": 1711329848.900771}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4814465, "arrival": 1711329848.9474392}, "models": {"yolo": {"arrival": 1711329848.5401397, "serving": 1711329848.900771}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4813468, "arrival": 1711329848.325049}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.432083, "arrival": 1711329846.2414415}, "models": {"yolo": {"arrival": 1711329844.9062595, "serving": 1711329846.2102892}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.480272146+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329840.4814608}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.432101, "arrival": 1711329847.9075634}, "models": {"yolo": {"arrival": 1711329847.6170814, "serving": 1711329847.8652205}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.475388136+00:00\"}\"\n>", "times": {"request": {"sending": 1711329840.4550476}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4147463, "arrival": 1711329848.2934325}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.480788, "arrival": 1711329848.23355}, "models": {"yolo": {"arrival": 1711329847.875249, "serving": 1711329848.1589327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4814954, "arrival": 1711329848.326175}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.481785, "arrival": 1711329848.3267457}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.455357, "arrival": 1711329848.3150356}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4557638, "arrival": 1711329846.7714384}, "models": {"yolo": {"arrival": 1711329846.4787714, "serving": 1711329846.7455325}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.481726, "arrival": 1711329848.9508452}, "models": {"yolo": {"arrival": 1711329848.5401397, "serving": 1711329848.900771}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4553256, "arrival": 1711329848.5629976}, "models": {"yolo": {"arrival": 1711329848.1963868, "serving": 1711329848.5312662}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.455723, "arrival": 1711329848.3157895}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4812152, "arrival": 1711329846.7727327}, "models": {"yolo": {"arrival": 1711329846.4787714, "serving": 1711329846.7455325}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4556336, "arrival": 1711329847.5857723}, "models": {"yolo": {"arrival": 1711329847.2497609, "serving": 1711329847.5579898}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.432796, "arrival": 1711329848.309651}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.475450491+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329840.4553409}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4327698, "arrival": 1711329848.307231}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.481674, "arrival": 1711329847.1174235}, "models": {"yolo": {"arrival": 1711329846.7562096, "serving": 1711329847.09955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4812396, "arrival": 1711329848.5562174}, "models": {"yolo": {"arrival": 1711329848.1687624, "serving": 1711329848.532028}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4550915, "arrival": 1711329848.309901}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4328167, "arrival": 1711329847.5688312}, "models": {"yolo": {"arrival": 1711329847.2497609, "serving": 1711329847.5579898}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4328818, "arrival": 1711329848.306984}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4816568, "arrival": 1711329847.9030752}, "models": {"yolo": {"arrival": 1711329847.567168, "serving": 1711329847.8534331}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4555333, "arrival": 1711329848.2141051}, "models": {"yolo": {"arrival": 1711329847.875249, "serving": 1711329848.1589327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4549756, "arrival": 1711329848.5626209}, "models": {"yolo": {"arrival": 1711329848.1963868, "serving": 1711329848.5312662}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.470792226+00:00\"}\"\n>", "times": {"request": {"sending": 1711329840.4321609}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4812825, "arrival": 1711329848.9470634}, "models": {"yolo": {"arrival": 1711329848.5401397, "serving": 1711329848.900771}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4553869, "arrival": 1711329847.5801225}, "models": {"yolo": {"arrival": 1711329847.2497609, "serving": 1711329847.5579898}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4323988, "arrival": 1711329846.2430246}, "models": {"yolo": {"arrival": 1711329844.9062595, "serving": 1711329846.2102892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4557786, "arrival": 1711329848.2283623}, "models": {"yolo": {"arrival": 1711329847.875249, "serving": 1711329848.1589327}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.470817892+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329840.4323194}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.455431, "arrival": 1711329848.3080127}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4812613, "arrival": 1711329848.3189445}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4556465, "arrival": 1711329846.7704206}, "models": {"yolo": {"arrival": 1711329846.4787714, "serving": 1711329846.7455325}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4554443, "arrival": 1711329848.5633547}, "models": {"yolo": {"arrival": 1711329848.1963868, "serving": 1711329848.5312662}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.475566387+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329840.4811096}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.481525, "arrival": 1711329846.7760293}, "models": {"yolo": {"arrival": 1711329846.4787714, "serving": 1711329846.7455325}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.432839, "arrival": 1711329846.4914913}, "models": {"yolo": {"arrival": 1711329846.220493, "serving": 1711329846.4691665}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.481799, "arrival": 1711329847.904838}, "models": {"yolo": {"arrival": 1711329847.567168, "serving": 1711329847.8534331}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.481831, "arrival": 1711329848.5636804}, "models": {"yolo": {"arrival": 1711329848.1687624, "serving": 1711329848.532028}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4813645, "arrival": 1711329847.8960676}, "models": {"yolo": {"arrival": 1711329847.567168, "serving": 1711329847.8534331}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.432727, "arrival": 1711329848.5619338}, "models": {"yolo": {"arrival": 1711329848.1963868, "serving": 1711329848.5312662}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4152043, "arrival": 1711329847.907238}, "models": {"yolo": {"arrival": 1711329847.6170814, "serving": 1711329847.8652205}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4150517, "arrival": 1711329848.2938004}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4555619, "arrival": 1711329848.5668383}, "models": {"yolo": {"arrival": 1711329848.1963868, "serving": 1711329848.5312662}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.480355909+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329840.5010183}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4558492, "arrival": 1711329848.321093}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.455836, "arrival": 1711329848.3237026}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4556916, "arrival": 1711329848.5674112}, "models": {"yolo": {"arrival": 1711329848.1963868, "serving": 1711329848.5312662}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.455401, "arrival": 1711329846.4921756}, "models": {"yolo": {"arrival": 1711329846.220493, "serving": 1711329846.4691665}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.481043, "arrival": 1711329848.31623}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4323623, "arrival": 1711329848.308678}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.480199777+00:00\"}\"\n>", "times": {"request": {"sending": 1711329840.4812975}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.455416, "arrival": 1711329848.2137828}, "models": {"yolo": {"arrival": 1711329847.875249, "serving": 1711329848.1589327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4324162, "arrival": 1711329847.9108167}, "models": {"yolo": {"arrival": 1711329847.6170814, "serving": 1711329847.8652205}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4557505, "arrival": 1711329847.5864441}, "models": {"yolo": {"arrival": 1711329847.2497609, "serving": 1711329847.5579898}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.432119, "arrival": 1711329848.298028}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4558055, "arrival": 1711329848.9462905}, "models": {"yolo": {"arrival": 1711329848.5401397, "serving": 1711329848.900771}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.480299457+00:00\"}\"\n>", "times": {"request": {"sending": 1711329840.4816036}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.432505, "arrival": 1711329848.3089042}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4551263, "arrival": 1711329846.4919176}, "models": {"yolo": {"arrival": 1711329846.220493, "serving": 1711329846.4691665}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4325442, "arrival": 1711329846.24362}, "models": {"yolo": {"arrival": 1711329844.9062595, "serving": 1711329846.2102892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.432435, "arrival": 1711329848.2985427}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.500251, "arrival": 1711329849.3424404}, "models": {"yolo": {"arrival": 1711329848.9078658, "serving": 1711329849.309154}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4322186, "arrival": 1711329847.2826288}, "models": {"yolo": {"arrival": 1711329846.7594318, "serving": 1711329847.2350764}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4811654, "arrival": 1711329848.3248374}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4324517, "arrival": 1711329848.265672}, "models": {"yolo": {"arrival": 1711329847.8746598, "serving": 1711329848.1814575}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.500171, "arrival": 1711329848.5383558}, "models": {"yolo": {"arrival": 1711329848.1632314, "serving": 1711329848.5134635}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4814777, "arrival": 1711329848.3244066}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.470863459+00:00\"}\"\n>", "times": {"request": {"sending": 1711329840.432749}}, "outputs": []}, {"times": {"request": {"sending": 1711329840.4322376, "arrival": 1711329846.242326}, "models": {"yolo": {"arrival": 1711329844.9062595, "serving": 1711329846.2102892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.481564, "arrival": 1711329848.3197632}, "models": {"yolo": {"arrival": 1711329844.7189527, "serving": 1711329848.1312566}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329840.4323404, "arrival": 1711329848.30628}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329841.4111524, "arrival": 1711329848.5388849}, "models": {"yolo": {"arrival": 1711329848.1632314, "serving": 1711329848.5134635}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4120822, "arrival": 1711329848.9079008}, "models": {"yolo": {"arrival": 1711329848.5242481, "serving": 1711329848.8922873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4114268, "arrival": 1711329848.9337342}, "models": {"yolo": {"arrival": 1711329848.544357, "serving": 1711329848.9017153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4111927, "arrival": 1711329849.3432405}, "models": {"yolo": {"arrival": 1711329848.9078658, "serving": 1711329849.309154}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.448225, "arrival": 1711329848.7584493}, "models": {"yolo": {"arrival": 1711329848.1687596, "serving": 1711329848.7305713}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4109702, "arrival": 1711329848.3304338}, "models": {"yolo": {"arrival": 1711329844.731813, "serving": 1711329848.137022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4487994, "arrival": 1711329848.93087}, "models": {"yolo": {"arrival": 1711329848.739787, "serving": 1711329848.8997188}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4108677, "arrival": 1711329848.328307}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4357193, "arrival": 1711329848.9127023}, "models": {"yolo": {"arrival": 1711329848.5242481, "serving": 1711329848.8922873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4120448, "arrival": 1711329848.9455276}, "models": {"yolo": {"arrival": 1711329848.544357, "serving": 1711329848.9017153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4481058, "arrival": 1711329848.75514}, "models": {"yolo": {"arrival": 1711329848.1687596, "serving": 1711329848.7305713}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.992706721+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.4486394}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.4365375, "arrival": 1711329849.3576357}, "models": {"yolo": {"arrival": 1711329848.9006736, "serving": 1711329849.3189824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4113882, "arrival": 1711329847.1227906}, "models": {"yolo": {"arrival": 1711329846.7562096, "serving": 1711329847.09955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4115806, "arrival": 1711329848.3298402}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4482894, "arrival": 1711329852.1650043}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4489753, "arrival": 1711329852.1819518}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4362345, "arrival": 1711329849.3571482}, "models": {"yolo": {"arrival": 1711329848.9006736, "serving": 1711329849.3189824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4482412, "arrival": 1711329847.3350985}, "models": {"yolo": {"arrival": 1711329847.1123219, "serving": 1711329847.314664}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4116566, "arrival": 1711329848.2074623}, "models": {"yolo": {"arrival": 1711329847.8624916, "serving": 1711329848.15689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4111109, "arrival": 1711329848.5641525}, "models": {"yolo": {"arrival": 1711329848.1687624, "serving": 1711329848.532028}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4120066, "arrival": 1711329847.127512}, "models": {"yolo": {"arrival": 1711329846.7562096, "serving": 1711329847.09955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4122345, "arrival": 1711329848.5427318}, "models": {"yolo": {"arrival": 1711329848.1724594, "serving": 1711329848.516461}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.992606999+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329841.4477854}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.4355829, "arrival": 1711329847.1291275}, "models": {"yolo": {"arrival": 1711329846.7562096, "serving": 1711329847.09955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.411023, "arrival": 1711329847.9053874}, "models": {"yolo": {"arrival": 1711329847.567168, "serving": 1711329847.8534331}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4118876, "arrival": 1711329848.3301964}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4119253, "arrival": 1711329848.542283}, "models": {"yolo": {"arrival": 1711329848.1724594, "serving": 1711329848.516461}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4361963, "arrival": 1711329849.3585463}, "models": {"yolo": {"arrival": 1711329848.9121032, "serving": 1711329849.3245845}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4123878, "arrival": 1711329848.9101126}, "models": {"yolo": {"arrival": 1711329848.5242481, "serving": 1711329848.8922873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4362698, "arrival": 1711329852.1498227}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.479146, "arrival": 1711329849.9234552}, "models": {"yolo": {"arrival": 1711329849.330518, "serving": 1711329849.8919396}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4113498, "arrival": 1711329848.2071142}, "models": {"yolo": {"arrival": 1711329847.8624916, "serving": 1711329848.15689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4332683, "arrival": 1711329848.22797}, "models": {"yolo": {"arrival": 1711329847.8624916, "serving": 1711329848.15689}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.975826671+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.411232}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.992658403+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.4481812}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.4123502, "arrival": 1711329848.9459476}, "models": {"yolo": {"arrival": 1711329848.544357, "serving": 1711329848.9017153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4485521, "arrival": 1711329849.351225}, "models": {"yolo": {"arrival": 1711329848.8924253, "serving": 1711329849.3087225}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.992750741+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.4489915}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.447976, "arrival": 1711329848.754456}, "models": {"yolo": {"arrival": 1711329848.1687596, "serving": 1711329848.7305713}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.411963, "arrival": 1711329848.2077928}, "models": {"yolo": {"arrival": 1711329847.8624916, "serving": 1711329848.15689}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.992554536+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.4358044}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.4482563, "arrival": 1711329849.3614984}, "models": {"yolo": {"arrival": 1711329848.9121032, "serving": 1711329849.3245845}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.411504, "arrival": 1711329849.3438282}, "models": {"yolo": {"arrival": 1711329848.9078658, "serving": 1711329849.309154}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4116943, "arrival": 1711329847.1237774}, "models": {"yolo": {"arrival": 1711329846.7562096, "serving": 1711329847.09955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4478607, "arrival": 1711329847.332097}, "models": {"yolo": {"arrival": 1711329847.1123219, "serving": 1711329847.314664}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4110684, "arrival": 1711329847.119818}, "models": {"yolo": {"arrival": 1711329846.7562096, "serving": 1711329847.09955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4490309, "arrival": 1711329848.9329164}, "models": {"yolo": {"arrival": 1711329848.739787, "serving": 1711329848.8997188}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4116185, "arrival": 1711329848.5417075}, "models": {"yolo": {"arrival": 1711329848.1724594, "serving": 1711329848.516461}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.411771, "arrival": 1711329848.907097}, "models": {"yolo": {"arrival": 1711329848.5242481, "serving": 1711329848.8922873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4483664, "arrival": 1711329847.6528978}, "models": {"yolo": {"arrival": 1711329847.3265052, "serving": 1711329847.634175}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4483194, "arrival": 1711329849.1868012}, "models": {"yolo": {"arrival": 1711329848.5231192, "serving": 1711329849.1525064}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.448122, "arrival": 1711329847.334551}, "models": {"yolo": {"arrival": 1711329847.1123219, "serving": 1711329847.314664}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4478936, "arrival": 1711329849.3580012}, "models": {"yolo": {"arrival": 1711329848.9006736, "serving": 1711329849.3189824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4481523, "arrival": 1711329849.362152}, "models": {"yolo": {"arrival": 1711329848.9006736, "serving": 1711329849.3189824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4483805, "arrival": 1711329849.361793}, "models": {"yolo": {"arrival": 1711329848.9121032, "serving": 1711329849.3245845}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.448027, "arrival": 1711329849.3582876}, "models": {"yolo": {"arrival": 1711329848.9006736, "serving": 1711329849.3189824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.447844, "arrival": 1711329848.7521946}, "models": {"yolo": {"arrival": 1711329848.1687596, "serving": 1711329848.7305713}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4358802, "arrival": 1711329848.902745}, "models": {"yolo": {"arrival": 1711329848.525948, "serving": 1711329848.8846505}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.448686, "arrival": 1711329848.9268672}, "models": {"yolo": {"arrival": 1711329848.739787, "serving": 1711329848.8997188}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4486535, "arrival": 1711329849.1876469}, "models": {"yolo": {"arrival": 1711329848.5231192, "serving": 1711329849.1525064}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4480417, "arrival": 1711329852.164262}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.433245, "arrival": 1711329848.8998055}, "models": {"yolo": {"arrival": 1711329848.525948, "serving": 1711329848.8846505}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4112725, "arrival": 1711329848.328528}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4486682, "arrival": 1711329849.890515}, "models": {"yolo": {"arrival": 1711329849.3173602, "serving": 1711329849.8778849}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4113111, "arrival": 1711329848.5396476}, "models": {"yolo": {"arrival": 1711329848.1724594, "serving": 1711329848.516461}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4481957, "arrival": 1711329849.186062}, "models": {"yolo": {"arrival": 1711329848.5231192, "serving": 1711329849.1525064}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.975959058+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329841.411542}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.4477031, "arrival": 1711329852.15128}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4791102, "arrival": 1711329847.660788}, "models": {"yolo": {"arrival": 1711329847.3265052, "serving": 1711329847.634175}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4114652, "arrival": 1711329848.539269}, "models": {"yolo": {"arrival": 1711329848.1632314, "serving": 1711329848.5134635}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4487848, "arrival": 1711329849.8927934}, "models": {"yolo": {"arrival": 1711329849.3173602, "serving": 1711329849.8778849}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.448395, "arrival": 1711329849.3627229}, "models": {"yolo": {"arrival": 1711329848.9006736, "serving": 1711329849.3189824}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.992782802+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.4791794}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.448743, "arrival": 1711329852.1668093}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4479089, "arrival": 1711329852.1636608}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.433215, "arrival": 1711329848.3314793}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4479933, "arrival": 1711329847.3339438}, "models": {"yolo": {"arrival": 1711329847.1123219, "serving": 1711329847.314664}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4364786, "arrival": 1711329847.3315706}, "models": {"yolo": {"arrival": 1711329847.1123219, "serving": 1711329847.314664}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4363866, "arrival": 1711329848.9048338}, "models": {"yolo": {"arrival": 1711329848.525948, "serving": 1711329848.8846505}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4479582, "arrival": 1711329849.3454573}, "models": {"yolo": {"arrival": 1711329848.8924253, "serving": 1711329849.3087225}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.448958, "arrival": 1711329849.9185305}, "models": {"yolo": {"arrival": 1711329849.330518, "serving": 1711329849.8919396}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4487283, "arrival": 1711329849.917133}, "models": {"yolo": {"arrival": 1711329849.330518, "serving": 1711329849.8919396}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.448916, "arrival": 1711329848.932055}, "models": {"yolo": {"arrival": 1711329848.739787, "serving": 1711329848.8997188}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4490447, "arrival": 1711329847.6600916}, "models": {"yolo": {"arrival": 1711329847.3265052, "serving": 1711329847.634175}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4481661, "arrival": 1711329852.164657}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.992638194+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329841.448058}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.4361098, "arrival": 1711329848.9038918}, "models": {"yolo": {"arrival": 1711329848.525948, "serving": 1711329848.8846505}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4359715, "arrival": 1711329848.949911}, "models": {"yolo": {"arrival": 1711329848.544357, "serving": 1711329848.9017153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4790487, "arrival": 1711329849.8951504}, "models": {"yolo": {"arrival": 1711329849.3173602, "serving": 1711329849.8778849}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.412312, "arrival": 1711329847.128392}, "models": {"yolo": {"arrival": 1711329846.7562096, "serving": 1711329847.09955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4490592, "arrival": 1711329849.9244893}, "models": {"yolo": {"arrival": 1711329849.3348093, "serving": 1711329849.8943362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4117322, "arrival": 1711329848.9357507}, "models": {"yolo": {"arrival": 1711329848.544357, "serving": 1711329848.9017153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.436515, "arrival": 1711329849.358789}, "models": {"yolo": {"arrival": 1711329848.9121032, "serving": 1711329849.3245845}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.992474719+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.4118485}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.436168, "arrival": 1711329847.3309903}, "models": {"yolo": {"arrival": 1711329847.1123219, "serving": 1711329847.314664}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.992673396+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.448305}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.99262287+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.4479249}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.4485676, "arrival": 1711329848.7600417}, "models": {"yolo": {"arrival": 1711329848.1687596, "serving": 1711329848.7305713}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4478085, "arrival": 1711329848.5294583}, "models": {"yolo": {"arrival": 1711329848.1892612, "serving": 1711329848.5141776}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4118094, "arrival": 1711329849.344396}, "models": {"yolo": {"arrival": 1711329848.9078658, "serving": 1711329849.309154}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4363372, "arrival": 1711329848.5289664}, "models": {"yolo": {"arrival": 1711329848.1892612, "serving": 1711329848.5141776}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.435998, "arrival": 1711329848.9134963}, "models": {"yolo": {"arrival": 1711329848.5242481, "serving": 1711329848.8922873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4484093, "arrival": 1711329852.1653469}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4488616, "arrival": 1711329852.1672177}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4485965, "arrival": 1711329849.9100866}, "models": {"yolo": {"arrival": 1711329849.3348093, "serving": 1711329849.8943362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.448334, "arrival": 1711329849.3507097}, "models": {"yolo": {"arrival": 1711329848.8924253, "serving": 1711329849.3087225}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4489431, "arrival": 1711329849.924092}, "models": {"yolo": {"arrival": 1711329849.3348093, "serving": 1711329849.8943362}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.992764733+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.4491029}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.4359136, "arrival": 1711329848.23323}, "models": {"yolo": {"arrival": 1711329847.8624916, "serving": 1711329848.15689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4483502, "arrival": 1711329848.759429}, "models": {"yolo": {"arrival": 1711329848.1687596, "serving": 1711329848.7305713}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.448813, "arrival": 1711329847.6576357}, "models": {"yolo": {"arrival": 1711329847.3265052, "serving": 1711329847.634175}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4486105, "arrival": 1711329849.9146817}, "models": {"yolo": {"arrival": 1711329849.330518, "serving": 1711329849.8919396}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.448889, "arrival": 1711329849.1884656}, "models": {"yolo": {"arrival": 1711329848.5231192, "serving": 1711329849.1525064}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4791265, "arrival": 1711329849.9250638}, "models": {"yolo": {"arrival": 1711329849.3348093, "serving": 1711329849.8943362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4360812, "arrival": 1711329848.5284357}, "models": {"yolo": {"arrival": 1711329848.1892612, "serving": 1711329848.5141776}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.41212, "arrival": 1711329849.3515654}, "models": {"yolo": {"arrival": 1711329848.9078658, "serving": 1711329849.309154}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4488313, "arrival": 1711329849.9162192}, "models": {"yolo": {"arrival": 1711329849.3348093, "serving": 1711329849.8943362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4121966, "arrival": 1711329848.3311303}, "models": {"yolo": {"arrival": 1711329844.9449763, "serving": 1711329848.152761}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4487705, "arrival": 1711329849.1880586}, "models": {"yolo": {"arrival": 1711329848.5231192, "serving": 1711329849.1525064}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4359422, "arrival": 1711329847.1296427}, "models": {"yolo": {"arrival": 1711329846.7562096, "serving": 1711329847.09955}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.436139, "arrival": 1711329848.7486494}, "models": {"yolo": {"arrival": 1711329848.1687596, "serving": 1711329848.7305713}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4478774, "arrival": 1711329849.3590345}, "models": {"yolo": {"arrival": 1711329848.9121032, "serving": 1711329849.3245845}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.435844, "arrival": 1711329848.5278351}, "models": {"yolo": {"arrival": 1711329848.1892612, "serving": 1711329848.5141776}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4122732, "arrival": 1711329848.20813}, "models": {"yolo": {"arrival": 1711329847.8624916, "serving": 1711329848.15689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4487004, "arrival": 1711329847.6558244}, "models": {"yolo": {"arrival": 1711329847.3265052, "serving": 1711329847.634175}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4790893, "arrival": 1711329848.949291}, "models": {"yolo": {"arrival": 1711329848.739787, "serving": 1711329848.8997188}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4364324, "arrival": 1711329848.7514017}, "models": {"yolo": {"arrival": 1711329848.1687596, "serving": 1711329848.7305713}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4486253, "arrival": 1711329852.1656797}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.448846, "arrival": 1711329849.9177475}, "models": {"yolo": {"arrival": 1711329849.330518, "serving": 1711329849.8919396}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4478269, "arrival": 1711329849.344925}, "models": {"yolo": {"arrival": 1711329848.8924253, "serving": 1711329849.3087225}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.992517775+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329841.412158}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.992721855+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.448757}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.4124255, "arrival": 1711329849.3519132}, "models": {"yolo": {"arrival": 1711329848.9078658, "serving": 1711329849.309154}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4479425, "arrival": 1711329849.1774783}, "models": {"yolo": {"arrival": 1711329848.5231192, "serving": 1711329849.1525064}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4480877, "arrival": 1711329849.3494673}, "models": {"yolo": {"arrival": 1711329848.8924253, "serving": 1711329849.3087225}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4482727, "arrival": 1711329849.3623865}, "models": {"yolo": {"arrival": 1711329848.9006736, "serving": 1711329849.3189824}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.99253767+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.433119}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.992591014+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.436305}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.4356759, "arrival": 1711329848.9496074}, "models": {"yolo": {"arrival": 1711329848.544357, "serving": 1711329848.9017153}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.992736686+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.4488754}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.4482102, "arrival": 1711329849.350139}, "models": {"yolo": {"arrival": 1711329848.8924253, "serving": 1711329849.3087225}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:01.992688189+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329841.448425}}, "outputs": []}, {"times": {"request": {"sending": 1711329841.4480124, "arrival": 1711329849.359388}, "models": {"yolo": {"arrival": 1711329848.9121032, "serving": 1711329849.3245845}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4785655, "arrival": 1711329849.3296342}, "models": {"yolo": {"arrival": 1711329849.162482, "serving": 1711329849.3050587}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4487143, "arrival": 1711329849.915523}, "models": {"yolo": {"arrival": 1711329849.3348093, "serving": 1711329849.8943362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4490721, "arrival": 1711329849.9205558}, "models": {"yolo": {"arrival": 1711329849.330518, "serving": 1711329849.8919396}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4485824, "arrival": 1711329847.6550293}, "models": {"yolo": {"arrival": 1711329847.3265052, "serving": 1711329847.634175}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4490175, "arrival": 1711329849.8936698}, "models": {"yolo": {"arrival": 1711329849.3173602, "serving": 1711329849.8778849}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4489026, "arrival": 1711329849.8932703}, "models": {"yolo": {"arrival": 1711329849.3173602, "serving": 1711329849.8778849}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4489293, "arrival": 1711329847.6583335}, "models": {"yolo": {"arrival": 1711329847.3265052, "serving": 1711329847.634175}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.448536, "arrival": 1711329849.1872292}, "models": {"yolo": {"arrival": 1711329848.5231192, "serving": 1711329849.1525064}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4490051, "arrival": 1711329849.327388}, "models": {"yolo": {"arrival": 1711329849.162482, "serving": 1711329849.3050587}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4357631, "arrival": 1711329852.148774}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4480727, "arrival": 1711329849.1849697}, "models": {"yolo": {"arrival": 1711329848.5231192, "serving": 1711329849.1525064}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4791636, "arrival": 1711329852.1876307}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4481373, "arrival": 1711329849.3596337}, "models": {"yolo": {"arrival": 1711329848.9121032, "serving": 1711329849.3245845}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.449089, "arrival": 1711329852.1865337}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329841.4360263, "arrival": 1711329852.1493752}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 24.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:01.992575043+00:00\"}\"\n>", "times": {"request": {"sending": 1711329841.4360538}}, "outputs": []}], [{"times": {"request": {"sending": 1711329842.4169147, "arrival": 1711329849.330188}, "models": {"yolo": {"arrival": 1711329849.162482, "serving": 1711329849.3050587}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4170296, "arrival": 1711329850.304384}, "models": {"yolo": {"arrival": 1711329849.9030762, "serving": 1711329850.2805562}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4459176, "arrival": 1711329850.654194}, "models": {"yolo": {"arrival": 1711329850.2728446, "serving": 1711329850.6129186}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4174676, "arrival": 1711329849.5912805}, "models": {"yolo": {"arrival": 1711329848.9100993, "serving": 1711329849.572614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:02.607940777+00:00\"}\"\n>", "times": {"request": {"sending": 1711329842.4828494}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4460409, "arrival": 1711329850.300885}, "models": {"yolo": {"arrival": 1711329849.3165832, "serving": 1711329850.270371}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4684265, "arrival": 1711329853.5861077}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4575338, "arrival": 1711329848.237904}, "models": {"yolo": {"arrival": 1711329847.8613708, "serving": 1711329848.1635919}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4651616, "arrival": 1711329850.5066302}, "models": {"yolo": {"arrival": 1711329849.8551962, "serving": 1711329850.4831672}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.468603, "arrival": 1711329851.8427598}, "models": {"yolo": {"arrival": 1711329851.3575854, "serving": 1711329851.8055372}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:02.601004225+00:00\"}\"\n>", "times": {"request": {"sending": 1711329842.4461823}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:02.604566107+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329842.4579327}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.458154, "arrival": 1711329853.6242685}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.445988, "arrival": 1711329850.6609907}, "models": {"yolo": {"arrival": 1711329850.2837737, "serving": 1711329850.6183245}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4181235, "arrival": 1711329850.6560144}, "models": {"yolo": {"arrival": 1711329850.2900467, "serving": 1711329850.6196992}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.457259, "arrival": 1711329850.66299}, "models": {"yolo": {"arrival": 1711329850.2900467, "serving": 1711329850.6196992}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4459014, "arrival": 1711329850.300447}, "models": {"yolo": {"arrival": 1711329849.3165832, "serving": 1711329850.270371}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4461582, "arrival": 1711329852.2024035}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.417637, "arrival": 1711329852.1915212}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:02.600580175+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329842.4180539}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4573944, "arrival": 1711329852.2081144}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4461176, "arrival": 1711329850.66275}, "models": {"yolo": {"arrival": 1711329850.2900467, "serving": 1711329850.6196992}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.465001, "arrival": 1711329850.4975874}, "models": {"yolo": {"arrival": 1711329849.8551962, "serving": 1711329850.4831672}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4169784, "arrival": 1711329849.8956263}, "models": {"yolo": {"arrival": 1711329849.3173602, "serving": 1711329849.8778849}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4652, "arrival": 1711329851.6196048}, "models": {"yolo": {"arrival": 1711329850.631455, "serving": 1711329851.574152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4171495, "arrival": 1711329850.3047237}, "models": {"yolo": {"arrival": 1711329849.9030762, "serving": 1711329850.2805562}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4457848, "arrival": 1711329849.5988045}, "models": {"yolo": {"arrival": 1711329848.9100993, "serving": 1711329849.572614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:02.598051787+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329842.4173086}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.468564, "arrival": 1711329850.5079348}, "models": {"yolo": {"arrival": 1711329849.8551962, "serving": 1711329850.4831672}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4460738, "arrival": 1711329849.8619602}, "models": {"yolo": {"arrival": 1711329849.5820694, "serving": 1711329849.8469791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4652975, "arrival": 1711329850.5071826}, "models": {"yolo": {"arrival": 1711329849.8551962, "serving": 1711329850.4831672}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4649832, "arrival": 1711329853.6394713}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4458013, "arrival": 1711329848.2153878}, "models": {"yolo": {"arrival": 1711329847.8613708, "serving": 1711329848.1635919}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4685767, "arrival": 1711329848.947775}, "models": {"yolo": {"arrival": 1711329848.5387735, "serving": 1711329848.900985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4171345, "arrival": 1711329847.8885427}, "models": {"yolo": {"arrival": 1711329847.6427503, "serving": 1711329847.8518157}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.446094, "arrival": 1711329848.2372162}, "models": {"yolo": {"arrival": 1711329847.8613708, "serving": 1711329848.1635919}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.465178, "arrival": 1711329848.555562}, "models": {"yolo": {"arrival": 1711329848.1754375, "serving": 1711329848.529361}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:02.607117139+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329842.4684126}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4457486, "arrival": 1711329850.2976315}, "models": {"yolo": {"arrival": 1711329849.3165832, "serving": 1711329850.270371}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4171636, "arrival": 1711329850.2929776}, "models": {"yolo": {"arrival": 1711329849.9005039, "serving": 1711329850.2740827}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.468455, "arrival": 1711329850.5075305}, "models": {"yolo": {"arrival": 1711329849.8551962, "serving": 1711329850.4831672}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4172945, "arrival": 1711329852.1896927}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4174092, "arrival": 1711329852.1902897}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.417178, "arrival": 1711329852.1890423}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4180667, "arrival": 1711329850.2973483}, "models": {"yolo": {"arrival": 1711329849.3165832, "serving": 1711329850.270371}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4579911, "arrival": 1711329853.5914145}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:02.597993932+00:00\"}\"\n>", "times": {"request": {"sending": 1711329842.4171922}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4575663, "arrival": 1711329851.617322}, "models": {"yolo": {"arrival": 1711329850.631455, "serving": 1711329851.574152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4684837, "arrival": 1711329851.6203918}, "models": {"yolo": {"arrival": 1711329850.631455, "serving": 1711329851.574152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4582276, "arrival": 1711329851.3701768}, "models": {"yolo": {"arrival": 1711329850.627392, "serving": 1711329851.348155}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4462008, "arrival": 1711329850.3012104}, "models": {"yolo": {"arrival": 1711329849.3165832, "serving": 1711329850.270371}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4176667, "arrival": 1711329850.2970543}, "models": {"yolo": {"arrival": 1711329849.3165832, "serving": 1711329850.270371}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.445821, "arrival": 1711329850.6564748}, "models": {"yolo": {"arrival": 1711329850.2900467, "serving": 1711329850.6196992}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:02.607065411+00:00\"}\"\n>", "times": {"request": {"sending": 1711329842.4652493}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4460068, "arrival": 1711329852.2020016}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4171042, "arrival": 1711329850.2821484}, "models": {"yolo": {"arrival": 1711329849.8866286, "serving": 1711329850.2667234}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4575095, "arrival": 1711329849.8651958}, "models": {"yolo": {"arrival": 1711329849.5820694, "serving": 1711329849.8469791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.468469, "arrival": 1711329848.5681021}, "models": {"yolo": {"arrival": 1711329848.1754375, "serving": 1711329848.529361}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4179983, "arrival": 1711329848.2147586}, "models": {"yolo": {"arrival": 1711329847.8613708, "serving": 1711329848.1635919}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4172804, "arrival": 1711329850.293629}, "models": {"yolo": {"arrival": 1711329849.9005039, "serving": 1711329850.2740827}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.445934, "arrival": 1711329849.5995145}, "models": {"yolo": {"arrival": 1711329848.9100993, "serving": 1711329849.572614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4173958, "arrival": 1711329850.3017628}, "models": {"yolo": {"arrival": 1711329849.9005039, "serving": 1711329850.2740827}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4176226, "arrival": 1711329850.6496217}, "models": {"yolo": {"arrival": 1711329850.2837737, "serving": 1711329850.6183245}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.465265, "arrival": 1711329853.5856972}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.458032, "arrival": 1711329848.5507483}, "models": {"yolo": {"arrival": 1711329848.1754375, "serving": 1711329848.529361}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:02.604510603+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329842.4574256}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4169974, "arrival": 1711329849.5825503}, "models": {"yolo": {"arrival": 1711329848.9100993, "serving": 1711329849.572614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4173384, "arrival": 1711329850.28608}, "models": {"yolo": {"arrival": 1711329849.8866286, "serving": 1711329850.2667234}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4683087, "arrival": 1711329848.5677571}, "models": {"yolo": {"arrival": 1711329848.1754375, "serving": 1711329848.529361}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4174528, "arrival": 1711329850.2921627}, "models": {"yolo": {"arrival": 1711329849.8866286, "serving": 1711329850.2667234}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4575946, "arrival": 1711329851.3687267}, "models": {"yolo": {"arrival": 1711329850.627392, "serving": 1711329851.348155}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:02.600638553+00:00\"}\"\n>", "times": {"request": {"sending": 1711329842.4458814}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4176085, "arrival": 1711329850.3058064}, "models": {"yolo": {"arrival": 1711329849.9030762, "serving": 1711329850.2805562}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:02.604594233+00:00\"}\"\n>", "times": {"request": {"sending": 1711329842.4581237}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.446141, "arrival": 1711329850.661294}, "models": {"yolo": {"arrival": 1711329850.2837737, "serving": 1711329850.6183245}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4459643, "arrival": 1711329850.6569395}, "models": {"yolo": {"arrival": 1711329850.2900467, "serving": 1711329850.6196992}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4574833, "arrival": 1711329853.591022}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4175806, "arrival": 1711329849.5927663}, "models": {"yolo": {"arrival": 1711329848.9100993, "serving": 1711329849.572614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.465232, "arrival": 1711329852.2139869}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.465065, "arrival": 1711329851.3725855}, "models": {"yolo": {"arrival": 1711329850.627392, "serving": 1711329851.348155}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.417235, "arrival": 1711329849.5875986}, "models": {"yolo": {"arrival": 1711329848.9100993, "serving": 1711329849.572614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.457457, "arrival": 1711329850.3014941}, "models": {"yolo": {"arrival": 1711329849.3165832, "serving": 1711329850.270371}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4684968, "arrival": 1711329851.84171}, "models": {"yolo": {"arrival": 1711329851.3575854, "serving": 1711329851.8055372}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4581087, "arrival": 1711329852.208821}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4174948, "arrival": 1711329850.3055413}, "models": {"yolo": {"arrival": 1711329849.9030762, "serving": 1711329850.2805562}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4173813, "arrival": 1711329850.305278}, "models": {"yolo": {"arrival": 1711329849.9030762, "serving": 1711329850.2805562}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.417566, "arrival": 1711329850.6432571}, "models": {"yolo": {"arrival": 1711329850.2728446, "serving": 1711329850.6129186}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.458212, "arrival": 1711329851.6180246}, "models": {"yolo": {"arrival": 1711329850.631455, "serving": 1711329851.574152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4175947, "arrival": 1711329847.8919694}, "models": {"yolo": {"arrival": 1711329847.6427503, "serving": 1711329847.8518157}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.417324, "arrival": 1711329850.2912912}, "models": {"yolo": {"arrival": 1711329849.3165832, "serving": 1711329850.270371}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4460564, "arrival": 1711329853.5901363}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4458618, "arrival": 1711329852.2017329}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4685369, "arrival": 1711329853.5865135}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4580867, "arrival": 1711329851.3695328}, "models": {"yolo": {"arrival": 1711329850.627392, "serving": 1711329851.348155}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4170148, "arrival": 1711329847.8811119}, "models": {"yolo": {"arrival": 1711329847.6427503, "serving": 1711329847.8518157}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.46851, "arrival": 1711329852.2146106}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.465281, "arrival": 1711329853.6407003}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4683635, "arrival": 1711329851.620159}, "models": {"yolo": {"arrival": 1711329850.631455, "serving": 1711329851.574152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4170885, "arrival": 1711329849.3306603}, "models": {"yolo": {"arrival": 1711329849.162482, "serving": 1711329849.3050587}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:02.604641761+00:00\"}\"\n>", "times": {"request": {"sending": 1711329842.4651031}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.458139, "arrival": 1711329850.6165588}, "models": {"yolo": {"arrival": 1711329850.279457, "serving": 1711329850.6002421}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4582477, "arrival": 1711329852.2090693}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.417264, "arrival": 1711329850.305005}, "models": {"yolo": {"arrival": 1711329849.9030762, "serving": 1711329850.2805562}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.468619, "arrival": 1711329852.2149522}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:02.598105273+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329842.4175367}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4180403, "arrival": 1711329852.2010357}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4170587, "arrival": 1711329852.1882906}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4170444, "arrival": 1711329850.2925782}, "models": {"yolo": {"arrival": 1711329849.9005039, "serving": 1711329850.2740827}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4581735, "arrival": 1711329850.4970045}, "models": {"yolo": {"arrival": 1711329849.8551962, "serving": 1711329850.4831672}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.46855, "arrival": 1711329853.6420362}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4180126, "arrival": 1711329850.6553779}, "models": {"yolo": {"arrival": 1711329850.2900467, "serving": 1711329850.6196992}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4181063, "arrival": 1711329848.21508}, "models": {"yolo": {"arrival": 1711329847.8613708, "serving": 1711329848.1635919}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4456284, "arrival": 1711329850.653597}, "models": {"yolo": {"arrival": 1711329850.2837737, "serving": 1711329850.6183245}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4652169, "arrival": 1711329851.3732815}, "models": {"yolo": {"arrival": 1711329850.627392, "serving": 1711329851.348155}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4650815, "arrival": 1711329852.209308}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4462357, "arrival": 1711329849.862881}, "models": {"yolo": {"arrival": 1711329849.5820694, "serving": 1711329849.8469791}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:02.600616017+00:00\"}\"\n>", "times": {"request": {"sending": 1711329842.4457285}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4580138, "arrival": 1711329849.8659341}, "models": {"yolo": {"arrival": 1711329849.5820694, "serving": 1711329849.8469791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4462533, "arrival": 1711329848.2375853}, "models": {"yolo": {"arrival": 1711329847.8613708, "serving": 1711329848.1635919}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4683976, "arrival": 1711329852.2143126}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:02.607143825+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329842.4685235}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4573636, "arrival": 1711329851.3645148}, "models": {"yolo": {"arrival": 1711329850.627392, "serving": 1711329851.348155}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4650245, "arrival": 1711329848.5523317}, "models": {"yolo": {"arrival": 1711329848.1754375, "serving": 1711329848.529361}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4172065, "arrival": 1711329850.290492}, "models": {"yolo": {"arrival": 1711329849.3165832, "serving": 1711329850.270371}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4180264, "arrival": 1711329850.6499834}, "models": {"yolo": {"arrival": 1711329850.2837737, "serving": 1711329850.6183245}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4175088, "arrival": 1711329850.3020337}, "models": {"yolo": {"arrival": 1711329849.9005039, "serving": 1711329850.2740827}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4180794, "arrival": 1711329850.6443098}, "models": {"yolo": {"arrival": 1711329850.2728446, "serving": 1711329850.6129186}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:02.594737589+00:00\"}\"\n>", "times": {"request": {"sending": 1711329842.4170735}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4174814, "arrival": 1711329847.8913252}, "models": {"yolo": {"arrival": 1711329847.6427503, "serving": 1711329847.8518157}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:02.600660684+00:00\"}\"\n>", "times": {"request": {"sending": 1711329842.446025}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4173672, "arrival": 1711329847.890673}, "models": {"yolo": {"arrival": 1711329847.6427503, "serving": 1711329847.8518157}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4684405, "arrival": 1711329853.641338}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.417438, "arrival": 1711329850.2917342}, "models": {"yolo": {"arrival": 1711329849.3165832, "serving": 1711329850.270371}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4458447, "arrival": 1711329850.6548746}, "models": {"yolo": {"arrival": 1711329850.2837737, "serving": 1711329850.6183245}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.468379, "arrival": 1711329851.8332002}, "models": {"yolo": {"arrival": 1711329851.3575854, "serving": 1711329851.8055372}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4176812, "arrival": 1711329850.6437774}, "models": {"yolo": {"arrival": 1711329850.2728446, "serving": 1711329850.6129186}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.457967, "arrival": 1711329850.6155179}, "models": {"yolo": {"arrival": 1711329850.279457, "serving": 1711329850.6002421}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4180923, "arrival": 1711329849.5980275}, "models": {"yolo": {"arrival": 1711329848.9100993, "serving": 1711329849.572614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4651425, "arrival": 1711329853.6400795}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:02.604619166+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329842.4648638}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4457035, "arrival": 1711329852.201437}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.464957, "arrival": 1711329850.6172924}, "models": {"yolo": {"arrival": 1711329850.279457, "serving": 1711329850.6002421}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4462187, "arrival": 1711329853.590609}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4171197, "arrival": 1711329849.586813}, "models": {"yolo": {"arrival": 1711329848.9100993, "serving": 1711329849.572614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4175227, "arrival": 1711329852.1909387}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:02.598079777+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329842.4174237}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.4172492, "arrival": 1711329847.8896894}, "models": {"yolo": {"arrival": 1711329847.6427503, "serving": 1711329847.8518157}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4576237, "arrival": 1711329852.2085621}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4457664, "arrival": 1711329850.6450524}, "models": {"yolo": {"arrival": 1711329850.2728446, "serving": 1711329850.6129186}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4173522, "arrival": 1711329849.5890048}, "models": {"yolo": {"arrival": 1711329848.9100993, "serving": 1711329849.572614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4172206, "arrival": 1711329850.2853494}, "models": {"yolo": {"arrival": 1711329849.8866286, "serving": 1711329850.2667234}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4580607, "arrival": 1711329851.6177633}, "models": {"yolo": {"arrival": 1711329850.631455, "serving": 1711329851.574152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4650483, "arrival": 1711329851.618257}, "models": {"yolo": {"arrival": 1711329850.631455, "serving": 1711329851.574152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.46859, "arrival": 1711329851.6207051}, "models": {"yolo": {"arrival": 1711329850.631455, "serving": 1711329851.574152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4179811, "arrival": 1711329849.5950353}, "models": {"yolo": {"arrival": 1711329848.9100993, "serving": 1711329849.572614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4175522, "arrival": 1711329850.2965229}, "models": {"yolo": {"arrival": 1711329849.3165832, "serving": 1711329850.270371}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:02.598133699+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 86.44 MiB free; 1.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329842.4176517}}, "outputs": []}, {"times": {"request": {"sending": 1711329842.44595, "arrival": 1711329848.2159286}, "models": {"yolo": {"arrival": 1711329847.8613708, "serving": 1711329848.1635919}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4651237, "arrival": 1711329850.6183486}, "models": {"yolo": {"arrival": 1711329850.279457, "serving": 1711329850.6002421}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329842.4581904, "arrival": 1711329848.5517695}, "models": {"yolo": {"arrival": 1711329848.1754375, "serving": 1711329848.529361}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329844.4182284, "arrival": 1711329853.2442353}, "models": {"yolo": {"arrival": 1711329853.0241468, "serving": 1711329853.231432}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4173133, "arrival": 1711329853.6856077}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.417235, "arrival": 1711329853.6631973}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.41764, "arrival": 1711329854.60492}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.435079, "arrival": 1711329854.6075637}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4479325, "arrival": 1711329853.0234652}, "models": {"yolo": {"arrival": 1711329852.6501122, "serving": 1711329853.0102217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4174585, "arrival": 1711329852.4838965}, "models": {"yolo": {"arrival": 1711329852.1725333, "serving": 1711329852.4386241}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:05.522613743+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329844.4476619}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.4173486, "arrival": 1711329852.2873795}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.418475, "arrival": 1711329854.6065078}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4349718, "arrival": 1711329853.0063028}, "models": {"yolo": {"arrival": 1711329852.4471166, "serving": 1711329852.9826183}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4474623, "arrival": 1711329853.7005131}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.435141, "arrival": 1711329853.690815}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4178214, "arrival": 1711329853.0360093}, "models": {"yolo": {"arrival": 1711329852.650996, "serving": 1711329853.0158534}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.446876, "arrival": 1711329853.2288806}, "models": {"yolo": {"arrival": 1711329852.9924107, "serving": 1711329853.2077696}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4470634, "arrival": 1711329853.2295556}, "models": {"yolo": {"arrival": 1711329852.9924107, "serving": 1711329853.2077696}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:05.519059688+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329844.4348593}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.4177964, "arrival": 1711329850.6622355}, "models": {"yolo": {"arrival": 1711329850.2718165, "serving": 1711329850.6188014}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4476907, "arrival": 1711329853.701275}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.417512, "arrival": 1711329853.664751}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4481194, "arrival": 1711329853.643504}, "models": {"yolo": {"arrival": 1711329853.2193534, "serving": 1711329853.5388184}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4480371, "arrival": 1711329853.7033632}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.447718, "arrival": 1711329851.8841536}, "models": {"yolo": {"arrival": 1711329851.58628, "serving": 1711329851.8574367}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.447648, "arrival": 1711329853.5978215}, "models": {"yolo": {"arrival": 1711329853.2193534, "serving": 1711329853.5388184}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4182534, "arrival": 1711329854.6060114}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4183557, "arrival": 1711329853.6884513}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4474902, "arrival": 1711329851.8791828}, "models": {"yolo": {"arrival": 1711329851.58628, "serving": 1711329851.8574367}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.417872, "arrival": 1711329852.991864}, "models": {"yolo": {"arrival": 1711329852.4471166, "serving": 1711329852.9826183}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4350936, "arrival": 1711329853.0069668}, "models": {"yolo": {"arrival": 1711329852.4471166, "serving": 1711329852.9826183}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4174318, "arrival": 1711329854.604548}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.443634, "arrival": 1711329852.297902}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4349563, "arrival": 1711329854.6072226}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.417378, "arrival": 1711329850.661683}, "models": {"yolo": {"arrival": 1711329850.2718165, "serving": 1711329850.6188014}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:05.518833158+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329844.418102}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.4479473, "arrival": 1711329851.8856256}, "models": {"yolo": {"arrival": 1711329851.58628, "serving": 1711329851.8574367}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4177446, "arrival": 1711329853.6862986}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:05.522594028+00:00\"}\"\n>", "times": {"request": {"sending": 1711329844.4474342}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.4347727, "arrival": 1711329852.2954006}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4480875, "arrival": 1711329853.9015589}, "models": {"yolo": {"arrival": 1711329853.544238, "serving": 1711329853.8803763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4476056, "arrival": 1711329851.88008}, "models": {"yolo": {"arrival": 1711329851.58628, "serving": 1711329851.8574367}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4477594, "arrival": 1711329853.5982385}, "models": {"yolo": {"arrival": 1711329853.2193534, "serving": 1711329853.5388184}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.417771, "arrival": 1711329852.2892988}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4473915, "arrival": 1711329853.5852993}, "models": {"yolo": {"arrival": 1711329853.240524, "serving": 1711329853.5284343}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4179502, "arrival": 1711329853.6867974}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4182787, "arrival": 1711329853.0011148}, "models": {"yolo": {"arrival": 1711329852.4471166, "serving": 1711329852.9826183}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4473767, "arrival": 1711329851.6214862}, "models": {"yolo": {"arrival": 1711329850.6299438, "serving": 1711329851.574034}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4177191, "arrival": 1711329853.6650405}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.417405, "arrival": 1711329853.0337584}, "models": {"yolo": {"arrival": 1711329852.650996, "serving": 1711329853.0158534}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4175894, "arrival": 1711329850.6619465}, "models": {"yolo": {"arrival": 1711329850.2718165, "serving": 1711329850.6188014}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.417615, "arrival": 1711329853.0352356}, "models": {"yolo": {"arrival": 1711329852.650996, "serving": 1711329853.0158534}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4438128, "arrival": 1711329853.6910543}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:05.522644399+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329844.4480042}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.4174855, "arrival": 1711329844.6497345}, "models": {"yolo": {"arrival": 1711329844.4467988, "serving": 1711329844.6276224}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4350336, "arrival": 1711329852.297454}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.447732, "arrival": 1711329853.5923038}, "models": {"yolo": {"arrival": 1711329853.240524, "serving": 1711329853.5284343}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4478028, "arrival": 1711329853.7020018}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:05.518910969+00:00\"}\"\n>", "times": {"request": {"sending": 1711329844.4183044}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.4473338, "arrival": 1711329853.6848822}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4479616, "arrival": 1711329853.900661}, "models": {"yolo": {"arrival": 1711329853.544238, "serving": 1711329853.8803763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4182038, "arrival": 1711329850.6632237}, "models": {"yolo": {"arrival": 1711329850.2718165, "serving": 1711329850.6188014}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:05.522580877+00:00\"}\"\n>", "times": {"request": {"sending": 1711329844.4473195}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.418381, "arrival": 1711329852.2949986}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4175644, "arrival": 1711329852.2880597}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:05.519238166+00:00\"}\"\n>", "times": {"request": {"sending": 1711329844.4437838}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:05.522624352+00:00\"}\"\n>", "times": {"request": {"sending": 1711329844.4477737}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.434892, "arrival": 1711329853.6889303}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4348078, "arrival": 1711329853.247918}, "models": {"yolo": {"arrival": 1711329853.0241468, "serving": 1711329853.231432}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4475362, "arrival": 1711329853.5945592}, "models": {"yolo": {"arrival": 1711329853.2193534, "serving": 1711329853.5388184}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4175386, "arrival": 1711329853.6859548}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:05.519160254+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329844.4351091}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.4176922, "arrival": 1711329844.6502624}, "models": {"yolo": {"arrival": 1711329844.4467988, "serving": 1711329844.6276224}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4474764, "arrival": 1711329852.670356}, "models": {"yolo": {"arrival": 1711329852.2777736, "serving": 1711329852.6434138}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4478736, "arrival": 1711329853.6250293}, "models": {"yolo": {"arrival": 1711329853.2193534, "serving": 1711329853.5388184}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4470494, "arrival": 1711329854.6039748}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:05.526243981+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329844.4730265}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.4474056, "arrival": 1711329854.6085696}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.418178, "arrival": 1711329852.294562}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4477046, "arrival": 1711329853.0172968}, "models": {"yolo": {"arrival": 1711329852.6501122, "serving": 1711329853.0102217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4179766, "arrival": 1711329852.2940593}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4479005, "arrival": 1711329853.68796}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4470966, "arrival": 1711329853.6846416}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4470046, "arrival": 1711329852.6681845}, "models": {"yolo": {"arrival": 1711329852.2777736, "serving": 1711329852.6434138}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.435018, "arrival": 1711329853.6895607}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.434923, "arrival": 1711329851.618826}, "models": {"yolo": {"arrival": 1711329850.6299438, "serving": 1711329851.574034}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4180775, "arrival": 1711329852.9927669}, "models": {"yolo": {"arrival": 1711329852.4471166, "serving": 1711329852.9826183}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.418027, "arrival": 1711329853.0369723}, "models": {"yolo": {"arrival": 1711329852.650996, "serving": 1711329853.0158534}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4472773, "arrival": 1711329853.5847754}, "models": {"yolo": {"arrival": 1711329853.240524, "serving": 1711329853.5284343}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:05.519328438+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329844.4469452}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.4438694, "arrival": 1711329854.6082351}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4350488, "arrival": 1711329851.6190567}, "models": {"yolo": {"arrival": 1711329850.6299438, "serving": 1711329851.574034}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4479902, "arrival": 1711329853.6427677}, "models": {"yolo": {"arrival": 1711329853.2193534, "serving": 1711329853.5388184}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:05.518725845+00:00\"}\"\n>", "times": {"request": {"sending": 1711329844.4178972}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.443798, "arrival": 1711329853.6829123}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4478164, "arrival": 1711329853.0185423}, "models": {"yolo": {"arrival": 1711329852.6501122, "serving": 1711329853.0102217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4480178, "arrival": 1711329853.6892881}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4475777, "arrival": 1711329853.7009022}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4477456, "arrival": 1711329854.6095808}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4176655, "arrival": 1711329852.4841943}, "models": {"yolo": {"arrival": 1711329852.1725333, "serving": 1711329852.4386241}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4179242, "arrival": 1711329853.6653025}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4479764, "arrival": 1711329854.6103022}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4178464, "arrival": 1711329854.6052935}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4347506, "arrival": 1711329853.6886947}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.418153, "arrival": 1711329853.6881993}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4437127, "arrival": 1711329851.619277}, "models": {"yolo": {"arrival": 1711329850.6299438, "serving": 1711329851.574034}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4349072, "arrival": 1711329852.2958043}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4476767, "arrival": 1711329853.6874137}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.447115, "arrival": 1711329853.698761}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4438417, "arrival": 1711329851.619836}, "models": {"yolo": {"arrival": 1711329850.6299438, "serving": 1711329851.574034}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:05.519109611+00:00\"}\"\n>", "times": {"request": {"sending": 1711329844.434988}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:05.51898832+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329844.434644}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.4437482, "arrival": 1711329854.6079032}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4184234, "arrival": 1711329850.663462}, "models": {"yolo": {"arrival": 1711329850.2718165, "serving": 1711329850.6188014}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4470203, "arrival": 1711329851.6209323}, "models": {"yolo": {"arrival": 1711329850.6299438, "serving": 1711329851.574034}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4473062, "arrival": 1711329853.233456}, "models": {"yolo": {"arrival": 1711329852.9924107, "serving": 1711329853.2077696}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.434841, "arrival": 1711329853.0055518}, "models": {"yolo": {"arrival": 1711329852.4471166, "serving": 1711329852.9826183}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4474196, "arrival": 1711329853.5942085}, "models": {"yolo": {"arrival": 1711329853.2193534, "serving": 1711329853.5388184}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4476333, "arrival": 1711329854.6092346}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.447449, "arrival": 1711329853.6851194}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4475203, "arrival": 1711329854.6089005}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.446965, "arrival": 1711329853.6843762}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4471307, "arrival": 1711329852.6690822}, "models": {"yolo": {"arrival": 1711329852.2777736, "serving": 1711329852.6434138}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:05.522634188+00:00\"}\"\n>", "times": {"request": {"sending": 1711329844.4478872}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.443767, "arrival": 1711329853.2279618}, "models": {"yolo": {"arrival": 1711329852.9924107, "serving": 1711329853.2077696}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4473605, "arrival": 1711329852.6697402}, "models": {"yolo": {"arrival": 1711329852.2777736, "serving": 1711329852.6434138}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4350643, "arrival": 1711329853.2512038}, "models": {"yolo": {"arrival": 1711329853.0241468, "serving": 1711329853.231432}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:05.522604194+00:00\"}\"\n>", "times": {"request": {"sending": 1711329844.447549}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.4347906, "arrival": 1711329851.6184995}, "models": {"yolo": {"arrival": 1711329850.6299438, "serving": 1711329851.574034}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.447292, "arrival": 1711329854.6211035}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4347281, "arrival": 1711329853.6820958}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4185057, "arrival": 1711329853.0027742}, "models": {"yolo": {"arrival": 1711329852.4471166, "serving": 1711329852.9826183}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4473474, "arrival": 1711329853.7001278}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4349382, "arrival": 1711329853.2501605}, "models": {"yolo": {"arrival": 1711329853.0241468, "serving": 1711329853.231432}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.418331, "arrival": 1711329853.6676009}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4472606, "arrival": 1711329851.6211598}, "models": {"yolo": {"arrival": 1711329850.6299438, "serving": 1711329851.574034}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4476202, "arrival": 1711329853.5918062}, "models": {"yolo": {"arrival": 1711329853.240524, "serving": 1711329853.5284343}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4184494, "arrival": 1711329853.245129}, "models": {"yolo": {"arrival": 1711329853.0241468, "serving": 1711329853.231432}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.434825, "arrival": 1711329854.6068666}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.447035, "arrival": 1711329853.5842206}, "models": {"yolo": {"arrival": 1711329853.240524, "serving": 1711329853.5284343}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4180014, "arrival": 1711329850.6624937}, "models": {"yolo": {"arrival": 1711329850.2718165, "serving": 1711329850.6188014}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.443827, "arrival": 1711329852.2983057}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.434876, "arrival": 1711329853.6818132}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4475048, "arrival": 1711329853.5883472}, "models": {"yolo": {"arrival": 1711329853.240524, "serving": 1711329853.5284343}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.418128, "arrival": 1711329853.6655607}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4438562, "arrival": 1711329853.5834925}, "models": {"yolo": {"arrival": 1711329853.240524, "serving": 1711329853.5284343}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.448055, "arrival": 1711329853.0243301}, "models": {"yolo": {"arrival": 1711329852.6501122, "serving": 1711329853.0102217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4437318, "arrival": 1711329853.2523549}, "models": {"yolo": {"arrival": 1711329853.0241468, "serving": 1711329853.231432}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4351265, "arrival": 1711329853.682646}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4180522, "arrival": 1711329854.6056542}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.446983, "arrival": 1711329853.6913054}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.44786, "arrival": 1711329854.6099632}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4350033, "arrival": 1711329853.6823545}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4477897, "arrival": 1711329853.6877103}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4478462, "arrival": 1711329853.899706}, "models": {"yolo": {"arrival": 1711329853.544238, "serving": 1711329853.8803763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4478323, "arrival": 1711329851.8849616}, "models": {"yolo": {"arrival": 1711329851.58628, "serving": 1711329851.8574367}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4479148, "arrival": 1711329853.7031057}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4480727, "arrival": 1711329852.2039177}, "models": {"yolo": {"arrival": 1711329851.8684251, "serving": 1711329852.1625705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.447591, "arrival": 1711329852.6709707}, "models": {"yolo": {"arrival": 1711329852.2777736, "serving": 1711329852.6434138}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329844.4481032, "arrival": 1711329854.6106617}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:05.522560452+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329844.4470782}}, "outputs": []}, {"times": {"request": {"sending": 1711329844.4475634, "arrival": 1711329853.6865509}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329843.4406888, "arrival": 1711329851.849444}, "models": {"yolo": {"arrival": 1711329851.7161686, "serving": 1711329851.811645}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:03.607294301+00:00\"}\"\n>", "times": {"request": {"sending": 1711329843.433781}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4121695, "arrival": 1711329848.95054}, "models": {"yolo": {"arrival": 1711329848.5387735, "serving": 1711329848.900985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4109375, "arrival": 1711329853.5869005}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.41122, "arrival": 1711329851.8436723}, "models": {"yolo": {"arrival": 1711329851.3575854, "serving": 1711329851.8055372}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4409223, "arrival": 1711329852.2831583}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4337387, "arrival": 1711329852.146463}, "models": {"yolo": {"arrival": 1711329851.8114374, "serving": 1711329852.1234841}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4395902, "arrival": 1711329852.2031693}, "models": {"yolo": {"arrival": 1711329851.822503, "serving": 1711329852.1584032}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4123979, "arrival": 1711329853.6618814}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4399338, "arrival": 1711329851.7291327}, "models": {"yolo": {"arrival": 1711329850.6255217, "serving": 1711329851.7078462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.412436, "arrival": 1711329850.6488256}, "models": {"yolo": {"arrival": 1711329850.493538, "serving": 1711329850.614468}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.43399, "arrival": 1711329853.6642108}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.440964, "arrival": 1711329854.6206672}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4113867, "arrival": 1711329853.6607378}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.440045, "arrival": 1711329853.6813757}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.470077, "arrival": 1711329853.0332139}, "models": {"yolo": {"arrival": 1711329852.650996, "serving": 1711329853.0158534}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4111385, "arrival": 1711329848.94819}, "models": {"yolo": {"arrival": 1711329848.5387735, "serving": 1711329848.900985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4115412, "arrival": 1711329851.844607}, "models": {"yolo": {"arrival": 1711329851.3575854, "serving": 1711329851.8055372}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4408596, "arrival": 1711329852.2248993}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4110417, "arrival": 1711329853.6603522}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:03.607118813+00:00\"}\"\n>", "times": {"request": {"sending": 1711329843.411299}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4305916, "arrival": 1711329849.317538}, "models": {"yolo": {"arrival": 1711329848.9101968, "serving": 1711329849.3003464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4335785, "arrival": 1711329853.663747}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.430542, "arrival": 1711329853.6634772}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4110937, "arrival": 1711329850.5110505}, "models": {"yolo": {"arrival": 1711329849.8551962, "serving": 1711329850.4831672}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4111793, "arrival": 1711329851.8452408}, "models": {"yolo": {"arrival": 1711329851.5843484, "serving": 1711329851.813426}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4398584, "arrival": 1711329854.494601}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4406025, "arrival": 1711329852.676697}, "models": {"yolo": {"arrival": 1711329852.1684663, "serving": 1711329852.6408873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.440029, "arrival": 1711329853.658548}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.439555, "arrival": 1711329851.7224753}, "models": {"yolo": {"arrival": 1711329850.6255217, "serving": 1711329851.7078462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4122827, "arrival": 1711329852.2160037}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4115038, "arrival": 1711329851.8458395}, "models": {"yolo": {"arrival": 1711329851.5843484, "serving": 1711329851.813426}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4339106, "arrival": 1711329854.4818022}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:03.610504121+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329843.4397678}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:03.607183254+00:00\"}\"\n>", "times": {"request": {"sending": 1711329843.4116392}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4400704, "arrival": 1711329851.7304907}, "models": {"yolo": {"arrival": 1711329850.6255217, "serving": 1711329851.7078462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4397523, "arrival": 1711329852.221699}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4395156, "arrival": 1711329853.5998626}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4117808, "arrival": 1711329850.641929}, "models": {"yolo": {"arrival": 1711329850.493538, "serving": 1711329850.614468}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.440802, "arrival": 1711329851.8499055}, "models": {"yolo": {"arrival": 1711329851.7161686, "serving": 1711329851.811645}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.412116, "arrival": 1711329850.6426961}, "models": {"yolo": {"arrival": 1711329850.493538, "serving": 1711329850.614468}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4399977, "arrival": 1711329852.2222915}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4122062, "arrival": 1711329851.8472135}, "models": {"yolo": {"arrival": 1711329851.5843484, "serving": 1711329851.813426}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4408317, "arrival": 1711329852.6774898}, "models": {"yolo": {"arrival": 1711329852.1684663, "serving": 1711329852.6408873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.43385, "arrival": 1711329851.7210543}, "models": {"yolo": {"arrival": 1711329850.6255217, "serving": 1711329851.7078462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4408877, "arrival": 1711329853.662642}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4701583, "arrival": 1711329854.621377}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4337165, "arrival": 1711329852.1947339}, "models": {"yolo": {"arrival": 1711329851.822503, "serving": 1711329852.1584032}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4114254, "arrival": 1711329850.5115998}, "models": {"yolo": {"arrival": 1711329849.8551962, "serving": 1711329850.4831672}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4340303, "arrival": 1711329849.3235369}, "models": {"yolo": {"arrival": 1711329848.9101968, "serving": 1711329849.3003464}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:03.610546343+00:00\"}\"\n>", "times": {"request": {"sending": 1711329843.4400127}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4116879, "arrival": 1711329853.5878313}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4112604, "arrival": 1711329852.2152832}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4395726, "arrival": 1711329849.3265617}, "models": {"yolo": {"arrival": 1711329848.9101968, "serving": 1711329849.3003464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4124851, "arrival": 1711329848.9511461}, "models": {"yolo": {"arrival": 1711329848.5387735, "serving": 1711329848.900985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4114652, "arrival": 1711329848.9485068}, "models": {"yolo": {"arrival": 1711329848.5387735, "serving": 1711329848.900985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4118984, "arrival": 1711329852.1401036}, "models": {"yolo": {"arrival": 1711329851.8114374, "serving": 1711329852.1234841}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4395366, "arrival": 1711329853.6644573}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:03.613027714+00:00\"}\"\n>", "times": {"request": {"sending": 1711329843.44076}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:03.607231714+00:00\"}\"\n>", "times": {"request": {"sending": 1711329843.4123206}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4339683, "arrival": 1711329853.5994325}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.439673, "arrival": 1711329853.6659193}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4113479, "arrival": 1711329853.587437}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:03.607252862+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329843.430387}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4406161, "arrival": 1711329854.6222458}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4405742, "arrival": 1711329851.8478513}, "models": {"yolo": {"arrival": 1711329851.7161686, "serving": 1711329851.811645}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:03.610482687+00:00\"}\"\n>", "times": {"request": {"sending": 1711329843.4396403}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4410224, "arrival": 1711329853.6853569}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:03.607207238+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329843.4119995}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4399498, "arrival": 1711329849.8854098}, "models": {"yolo": {"arrival": 1711329849.3089988, "serving": 1711329849.8635626}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.44095, "arrival": 1711329853.0322812}, "models": {"yolo": {"arrival": 1711329852.650996, "serving": 1711329853.0158534}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.440658, "arrival": 1711329853.6599689}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.441008, "arrival": 1711329853.6629298}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4409785, "arrival": 1711329852.2251148}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4126053, "arrival": 1711329852.2163348}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:03.610566143+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329843.4404078}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4409368, "arrival": 1711329850.279778}, "models": {"yolo": {"arrival": 1711329849.871239, "serving": 1711329850.2621024}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.411732, "arrival": 1711329853.6611269}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4125671, "arrival": 1711329852.1449013}, "models": {"yolo": {"arrival": 1711329851.8114374, "serving": 1711329852.1234841}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4405599, "arrival": 1711329853.6834178}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4120784, "arrival": 1711329853.6614943}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4407744, "arrival": 1711329853.6622584}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.44022, "arrival": 1711329854.495744}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4399822, "arrival": 1711329854.4954767}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4336932, "arrival": 1711329849.3213634}, "models": {"yolo": {"arrival": 1711329848.9101968, "serving": 1711329849.3003464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.434051, "arrival": 1711329852.2028153}, "models": {"yolo": {"arrival": 1711329851.822503, "serving": 1711329852.1584032}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.412529, "arrival": 1711329852.1921246}, "models": {"yolo": {"arrival": 1711329851.822503, "serving": 1711329852.1584032}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4399188, "arrival": 1711329853.6665478}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.43057, "arrival": 1711329850.6492631}, "models": {"yolo": {"arrival": 1711329850.493538, "serving": 1711329850.614468}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4115856, "arrival": 1711329852.2155201}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:03.610526012+00:00\"}\"\n>", "times": {"request": {"sending": 1711329843.4398885}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4340708, "arrival": 1711329854.4820635}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:03.61305217+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329843.4408736}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4713, "arrival": 1711329844.6491342}, "models": {"yolo": {"arrival": 1711329844.4467988, "serving": 1711329844.6276224}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4118595, "arrival": 1711329851.8464947}, "models": {"yolo": {"arrival": 1711329851.5843484, "serving": 1711329851.813426}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4401712, "arrival": 1711329852.6724246}, "models": {"yolo": {"arrival": 1711329852.1684663, "serving": 1711329852.6408873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4119513, "arrival": 1711329852.2157645}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.440816, "arrival": 1711329850.278414}, "models": {"yolo": {"arrival": 1711329849.871239, "serving": 1711329850.2621024}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4340909, "arrival": 1711329852.2193236}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4336655, "arrival": 1711329851.7172732}, "models": {"yolo": {"arrival": 1711329850.6255217, "serving": 1711329851.7078462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4339297, "arrival": 1711329852.218957}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.440746, "arrival": 1711329852.224669}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4118192, "arrival": 1711329848.9488235}, "models": {"yolo": {"arrival": 1711329848.5387735, "serving": 1711329848.900985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4396572, "arrival": 1711329853.6259143}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:03.610372216+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329843.4339488}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4397967, "arrival": 1711329853.6662674}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.440703, "arrival": 1711329850.2722225}, "models": {"yolo": {"arrival": 1711329849.871239, "serving": 1711329850.2621024}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4407182, "arrival": 1711329852.6770973}, "models": {"yolo": {"arrival": 1711329852.1684663, "serving": 1711329852.6408873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4337597, "arrival": 1711329852.2187245}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4408455, "arrival": 1711329854.6216345}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4406724, "arrival": 1711329853.6836739}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4396238, "arrival": 1711329852.219697}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4122443, "arrival": 1711329852.143886}, "models": {"yolo": {"arrival": 1711329851.8114374, "serving": 1711329852.1234841}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4396071, "arrival": 1711329854.4823163}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4305062, "arrival": 1711329853.5896633}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4404237, "arrival": 1711329853.6591582}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4120398, "arrival": 1711329853.588789}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.440389, "arrival": 1711329852.2225752}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.43389, "arrival": 1711329852.1950767}, "models": {"yolo": {"arrival": 1711329851.822503, "serving": 1711329852.1584032}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.430614, "arrival": 1711329852.1927285}, "models": {"yolo": {"arrival": 1711329851.822503, "serving": 1711329852.1584032}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4397206, "arrival": 1711329852.2034287}, "models": {"yolo": {"arrival": 1711329851.822503, "serving": 1711329852.1584032}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4410353, "arrival": 1711329852.286478}, "models": {"yolo": {"arrival": 1711329851.8230534, "serving": 1711329852.2631702}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4410493, "arrival": 1711329850.2809439}, "models": {"yolo": {"arrival": 1711329849.871239, "serving": 1711329850.2621024}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.44047, "arrival": 1711329849.8883972}, "models": {"yolo": {"arrival": 1711329849.3089988, "serving": 1711329849.8635626}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:03.610586969+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329843.440531}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4123597, "arrival": 1711329853.5892577}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4399657, "arrival": 1711329852.671837}, "models": {"yolo": {"arrival": 1711329852.1684663, "serving": 1711329852.6408873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4338698, "arrival": 1711329849.3227012}, "models": {"yolo": {"arrival": 1711329848.9101968, "serving": 1711329849.3003464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4404848, "arrival": 1711329852.6758869}, "models": {"yolo": {"arrival": 1711329852.1684663, "serving": 1711329852.6408873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4409943, "arrival": 1711329844.6471372}, "models": {"yolo": {"arrival": 1711329844.4467988, "serving": 1711329844.6276224}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.440546, "arrival": 1711329853.6595693}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4338298, "arrival": 1711329853.6639805}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:03.612978504+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329843.440644}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4398282, "arrival": 1711329849.8843746}, "models": {"yolo": {"arrival": 1711329849.3089988, "serving": 1711329849.8635626}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4340098, "arrival": 1711329851.7218223}, "models": {"yolo": {"arrival": 1711329850.6255217, "serving": 1711329851.7078462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4405146, "arrival": 1711329852.2228115}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:03.610457792+00:00\"}\"\n>", "times": {"request": {"sending": 1711329843.4394362}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.430656, "arrival": 1711329852.2184355}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.433805, "arrival": 1711329853.5990849}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.439737, "arrival": 1711329854.4943666}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4404414, "arrival": 1711329853.6831698}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4404557, "arrival": 1711329851.7310448}, "models": {"yolo": {"arrival": 1711329850.6255217, "serving": 1711329851.7078462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.439782, "arrival": 1711329853.6668968}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4406307, "arrival": 1711329852.2242901}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4397051, "arrival": 1711329849.8814292}, "models": {"yolo": {"arrival": 1711329849.3089988, "serving": 1711329849.8635626}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 30.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:03.607272562+00:00\"}\"\n>", "times": {"request": {"sending": 1711329843.4306788}}, "outputs": []}, {"times": {"request": {"sending": 1711329843.4398434, "arrival": 1711329852.2036772}, "models": {"yolo": {"arrival": 1711329851.822503, "serving": 1711329852.1584032}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4398735, "arrival": 1711329852.2220612}, "models": {"yolo": {"arrival": 1711329849.3595607, "serving": 1711329852.1295831}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4405003, "arrival": 1711329854.5534246}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4407313, "arrival": 1711329854.6218927}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.430635, "arrival": 1711329852.14583}, "models": {"yolo": {"arrival": 1711329851.8114374, "serving": 1711329852.1234841}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4401257, "arrival": 1711329849.887709}, "models": {"yolo": {"arrival": 1711329849.3089988, "serving": 1711329849.8635626}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.440788, "arrival": 1711329853.6839106}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4398127, "arrival": 1711329851.7286406}, "models": {"yolo": {"arrival": 1711329850.6255217, "serving": 1711329851.7078462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4409056, "arrival": 1711329853.6841452}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4399037, "arrival": 1711329853.6454103}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.430699, "arrival": 1711329853.5987341}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4405885, "arrival": 1711329849.8892858}, "models": {"yolo": {"arrival": 1711329849.3089988, "serving": 1711329849.8635626}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4396894, "arrival": 1711329851.7278545}, "models": {"yolo": {"arrival": 1711329850.6255217, "serving": 1711329851.7078462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329843.4709048, "arrival": 1711329852.4834855}, "models": {"yolo": {"arrival": 1711329852.1725333, "serving": 1711329852.4386241}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329845.410634, "arrival": 1711329853.0248878}, "models": {"yolo": {"arrival": 1711329852.6501122, "serving": 1711329853.0102217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4516304, "arrival": 1711329854.1614187}, "models": {"yolo": {"arrival": 1711329853.9037824, "serving": 1711329854.1400313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4114106, "arrival": 1711329854.611757}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4404082, "arrival": 1711329852.789256}, "models": {"yolo": {"arrival": 1711329852.1762588, "serving": 1711329852.7638428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4119248, "arrival": 1711329853.2356293}, "models": {"yolo": {"arrival": 1711329853.0170672, "serving": 1711329853.212004}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4401503, "arrival": 1711329854.2504818}, "models": {"yolo": {"arrival": 1711329853.890922, "serving": 1711329854.2243881}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4104538, "arrival": 1711329853.689813}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4522316, "arrival": 1711329853.93272}, "models": {"yolo": {"arrival": 1711329853.5470123, "serving": 1711329853.895595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4108975, "arrival": 1711329853.6900656}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4118483, "arrival": 1711329853.6982336}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4107642, "arrival": 1711329854.6110005}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.440563, "arrival": 1711329853.7023668}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.452359, "arrival": 1711329854.4928908}, "models": {"yolo": {"arrival": 1711329854.148799, "serving": 1711329854.4485638}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4513648, "arrival": 1711329854.4826663}, "models": {"yolo": {"arrival": 1711329854.2343335, "serving": 1711329854.4456546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4527898, "arrival": 1711329854.6252027}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4399705, "arrival": 1711329853.9423504}, "models": {"yolo": {"arrival": 1711329853.5549426, "serving": 1711329853.8967144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.41068, "arrival": 1711329852.2041662}, "models": {"yolo": {"arrival": 1711329851.8684251, "serving": 1711329852.1625705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4404845, "arrival": 1711329854.1684828}, "models": {"yolo": {"arrival": 1711329853.9058452, "serving": 1711329854.1604497}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4117692, "arrival": 1711329853.9359362}, "models": {"yolo": {"arrival": 1711329853.5549426, "serving": 1711329853.8967144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4105785, "arrival": 1711329853.7036276}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4120028, "arrival": 1711329854.2451334}, "models": {"yolo": {"arrival": 1711329853.890922, "serving": 1711329854.2243881}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4401314, "arrival": 1711329852.2109618}, "models": {"yolo": {"arrival": 1711329851.8684251, "serving": 1711329852.1625705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4519732, "arrival": 1711329854.4837527}, "models": {"yolo": {"arrival": 1711329854.2343335, "serving": 1711329854.4456546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4527204, "arrival": 1711329854.5019662}, "models": {"yolo": {"arrival": 1711329854.148799, "serving": 1711329854.4485638}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4521039, "arrival": 1711329853.010872}, "models": {"yolo": {"arrival": 1711329852.7720814, "serving": 1711329852.9928532}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4109378, "arrival": 1711329853.703885}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4512339, "arrival": 1711329854.1604795}, "models": {"yolo": {"arrival": 1711329853.9037824, "serving": 1711329854.1400313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.452704, "arrival": 1711329854.1556315}, "models": {"yolo": {"arrival": 1711329853.8070765, "serving": 1711329854.1188226}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4520652, "arrival": 1711329854.489718}, "models": {"yolo": {"arrival": 1711329854.148799, "serving": 1711329854.4485638}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4521372, "arrival": 1711329854.6247385}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4119642, "arrival": 1711329852.209897}, "models": {"yolo": {"arrival": 1711329851.8684251, "serving": 1711329852.1625705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4521205, "arrival": 1711329854.4839647}, "models": {"yolo": {"arrival": 1711329854.2343335, "serving": 1711329854.4456546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4517176, "arrival": 1711329854.1739118}, "models": {"yolo": {"arrival": 1711329853.9058452, "serving": 1711329854.1604497}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4113712, "arrival": 1711329853.906164}, "models": {"yolo": {"arrival": 1711329853.544238, "serving": 1711329853.8803763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.433158, "arrival": 1711329854.6127481}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.411254, "arrival": 1711329853.7041428}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4403033, "arrival": 1711329854.6230261}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4328797, "arrival": 1711329853.9083645}, "models": {"yolo": {"arrival": 1711329853.5623872, "serving": 1711329853.893048}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.454945059+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.45143}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.4401884, "arrival": 1711329853.9429138}, "models": {"yolo": {"arrival": 1711329853.5549426, "serving": 1711329853.8967144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.45153, "arrival": 1711329854.4829292}, "models": {"yolo": {"arrival": 1711329854.2343335, "serving": 1711329854.4456546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4527538, "arrival": 1711329853.3835933}, "models": {"yolo": {"arrival": 1711329853.001323, "serving": 1711329853.3625631}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.433118, "arrival": 1711329852.2106159}, "models": {"yolo": {"arrival": 1711329851.8684251, "serving": 1711329852.1625705}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.454924187+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.4405212}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.4107232, "arrival": 1711329853.9047282}, "models": {"yolo": {"arrival": 1711329853.544238, "serving": 1711329853.8803763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4516482, "arrival": 1711329853.6380503}, "models": {"yolo": {"arrival": 1711329853.2211795, "serving": 1711329853.5357587}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4330547, "arrival": 1711329853.6992934}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.458926827+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329845.4524648}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.451566, "arrival": 1711329854.1719427}, "models": {"yolo": {"arrival": 1711329853.9058452, "serving": 1711329854.1604497}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4330957, "arrival": 1711329853.5926948}, "models": {"yolo": {"arrival": 1711329853.2211795, "serving": 1711329853.5357587}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.451548, "arrival": 1711329854.623774}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.412048, "arrival": 1711329854.6225166}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.454685751+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.4114876}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.4111357, "arrival": 1711329853.6448514}, "models": {"yolo": {"arrival": 1711329853.2193534, "serving": 1711329853.5388184}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4108057, "arrival": 1711329853.644215}, "models": {"yolo": {"arrival": 1711329853.2193534, "serving": 1711329853.5388184}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4514892, "arrival": 1711329853.6672575}, "models": {"yolo": {"arrival": 1711329853.2211795, "serving": 1711329853.5357587}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4522133, "arrival": 1711329854.4901109}, "models": {"yolo": {"arrival": 1711329854.148799, "serving": 1711329854.4485638}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4110563, "arrival": 1711329853.9055216}, "models": {"yolo": {"arrival": 1711329853.544238, "serving": 1711329853.8803763}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:05.526273575+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.4108536}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.452664, "arrival": 1711329854.4814832}, "models": {"yolo": {"arrival": 1711329854.175787, "serving": 1711329854.44338}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4110172, "arrival": 1711329852.2044053}, "models": {"yolo": {"arrival": 1711329851.8684251, "serving": 1711329852.1625705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4526298, "arrival": 1711329854.846223}, "models": {"yolo": {"arrival": 1711329854.4602714, "serving": 1711329854.8251607}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4521546, "arrival": 1711329854.4804292}, "models": {"yolo": {"arrival": 1711329854.175787, "serving": 1711329854.44338}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4114492, "arrival": 1711329853.9354503}, "models": {"yolo": {"arrival": 1711329853.5549426, "serving": 1711329853.8967144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4519002, "arrival": 1711329854.1478925}, "models": {"yolo": {"arrival": 1711329853.8070765, "serving": 1711329854.1188226}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4524322, "arrival": 1711329854.6227727}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4519553, "arrival": 1711329852.7942085}, "models": {"yolo": {"arrival": 1711329852.1762588, "serving": 1711329852.7638428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.440349, "arrival": 1711329853.7016432}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4527695, "arrival": 1711329854.8473063}, "models": {"yolo": {"arrival": 1711329854.4602714, "serving": 1711329854.8251607}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4112148, "arrival": 1711329853.6903183}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4513876, "arrival": 1711329854.6235242}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.440112, "arrival": 1711329853.593069}, "models": {"yolo": {"arrival": 1711329853.2211795, "serving": 1711329853.5357587}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4116523, "arrival": 1711329852.2095642}, "models": {"yolo": {"arrival": 1711329851.8684251, "serving": 1711329852.1625705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4516037, "arrival": 1711329853.8110037}, "models": {"yolo": {"arrival": 1711329853.5579023, "serving": 1711329853.7994797}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4518092, "arrival": 1711329852.7935274}, "models": {"yolo": {"arrival": 1711329852.1762588, "serving": 1711329852.7638428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.452376, "arrival": 1711329853.9332268}, "models": {"yolo": {"arrival": 1711329853.5470123, "serving": 1711329853.895595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4514103, "arrival": 1711329854.169115}, "models": {"yolo": {"arrival": 1711329853.9058452, "serving": 1711329854.1604497}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.454851489+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.4400482}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.4331384, "arrival": 1711329854.2497933}, "models": {"yolo": {"arrival": 1711329853.890922, "serving": 1711329854.2243881}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4109774, "arrival": 1711329853.0253487}, "models": {"yolo": {"arrival": 1711329852.6501122, "serving": 1711329853.0102217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.452576, "arrival": 1711329854.4931962}, "models": {"yolo": {"arrival": 1711329854.148799, "serving": 1711329854.4485638}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4523423, "arrival": 1711329854.1541195}, "models": {"yolo": {"arrival": 1711329853.8070765, "serving": 1711329854.1188226}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4115665, "arrival": 1711329853.7043855}, "models": {"yolo": {"arrival": 1711329850.6641984, "serving": 1711329853.525598}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.411528, "arrival": 1711329853.6905718}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4525936, "arrival": 1711329853.933835}, "models": {"yolo": {"arrival": 1711329853.5470123, "serving": 1711329853.895595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4327807, "arrival": 1711329853.6990325}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4522893, "arrival": 1711329854.6249695}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4522498, "arrival": 1711329853.0114868}, "models": {"yolo": {"arrival": 1711329852.7720814, "serving": 1711329852.9928532}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.440095, "arrival": 1711329853.940814}, "models": {"yolo": {"arrival": 1711329853.5623872, "serving": 1711329853.893048}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:05.526293452+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.70 GiB already allocated; 226.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329845.4111748}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.458801698+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.4517384}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.4517732, "arrival": 1711329854.1618483}, "models": {"yolo": {"arrival": 1711329853.9037824, "serving": 1711329854.1400313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4110966, "arrival": 1711329854.611332}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4116118, "arrival": 1711329853.235001}, "models": {"yolo": {"arrival": 1711329853.0170672, "serving": 1711329853.212004}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.458900017+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329845.4521763}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.432911, "arrival": 1711329853.2362354}, "models": {"yolo": {"arrival": 1711329853.0170672, "serving": 1711329853.212004}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4113314, "arrival": 1711329852.2046325}, "models": {"yolo": {"arrival": 1711329851.8684251, "serving": 1711329852.1625705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4329803, "arrival": 1711329854.6124039}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4117296, "arrival": 1711329854.6120844}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.451469, "arrival": 1711329854.1609814}, "models": {"yolo": {"arrival": 1711329853.9037824, "serving": 1711329854.1400313}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.458939068+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.4526854}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.440428, "arrival": 1711329854.251584}, "models": {"yolo": {"arrival": 1711329853.890922, "serving": 1711329854.2243881}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4402373, "arrival": 1711329853.941371}, "models": {"yolo": {"arrival": 1711329853.5623872, "serving": 1711329853.893048}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.43296, "arrival": 1711329854.2490253}, "models": {"yolo": {"arrival": 1711329853.890922, "serving": 1711329854.2243881}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.411293, "arrival": 1711329853.2342882}, "models": {"yolo": {"arrival": 1711329853.0170672, "serving": 1711329853.212004}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4523976, "arrival": 1711329853.012208}, "models": {"yolo": {"arrival": 1711329852.7720814, "serving": 1711329852.9928532}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4526477, "arrival": 1711329854.6260862}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4519913, "arrival": 1711329854.624507}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4116907, "arrival": 1711329854.2439647}, "models": {"yolo": {"arrival": 1711329853.890922, "serving": 1711329854.2243881}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4520433, "arrival": 1711329854.1490407}, "models": {"yolo": {"arrival": 1711329853.8070765, "serving": 1711329854.1188226}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.458913486+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.4523234}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.4402726, "arrival": 1711329852.7883613}, "models": {"yolo": {"arrival": 1711329852.1762588, "serving": 1711329852.7638428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4516664, "arrival": 1711329852.791629}, "models": {"yolo": {"arrival": 1711329852.1762588, "serving": 1711329852.7638428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4329357, "arrival": 1711329852.2102995}, "models": {"yolo": {"arrival": 1711329851.8684251, "serving": 1711329852.1625705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.44032, "arrival": 1711329853.943263}, "models": {"yolo": {"arrival": 1711329853.5549426, "serving": 1711329853.8967144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4403877, "arrival": 1711329853.593825}, "models": {"yolo": {"arrival": 1711329853.2211795, "serving": 1711329853.5357587}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.454769233+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.4118078}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.440222, "arrival": 1711329853.699783}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4526126, "arrival": 1711329853.012789}, "models": {"yolo": {"arrival": 1711329852.7720814, "serving": 1711329852.9928532}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4518652, "arrival": 1711329854.1744187}, "models": {"yolo": {"arrival": 1711329853.9058452, "serving": 1711329854.1604497}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.452415, "arrival": 1711329854.8351674}, "models": {"yolo": {"arrival": 1711329854.4602714, "serving": 1711329854.8251607}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4519165, "arrival": 1711329854.1622543}, "models": {"yolo": {"arrival": 1711329853.9037824, "serving": 1711329854.1400313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4330757, "arrival": 1711329853.9378698}, "models": {"yolo": {"arrival": 1711329853.5623872, "serving": 1711329853.893048}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.451319, "arrival": 1711329853.6254206}, "models": {"yolo": {"arrival": 1711329853.2211795, "serving": 1711329853.5357587}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.45480616+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329845.4121249}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.45886063+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.4518816}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.45145, "arrival": 1711329853.7027442}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.454902339+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329845.4403338}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.454877407+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.4402056}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.440453, "arrival": 1711329854.6232724}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.454970057+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329845.4515862}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.4521966, "arrival": 1711329854.1497219}, "models": {"yolo": {"arrival": 1711329853.8070765, "serving": 1711329854.1188226}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.452737, "arrival": 1711329853.9344525}, "models": {"yolo": {"arrival": 1711329853.5470123, "serving": 1711329853.895595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4517567, "arrival": 1711329853.8119307}, "models": {"yolo": {"arrival": 1711329853.5579023, "serving": 1711329853.7994797}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.452806, "arrival": 1711329854.488957}, "models": {"yolo": {"arrival": 1711329854.175787, "serving": 1711329854.44338}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4518266, "arrival": 1711329854.4834628}, "models": {"yolo": {"arrival": 1711329854.2343335, "serving": 1711329854.4456546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4402535, "arrival": 1711329853.5934508}, "models": {"yolo": {"arrival": 1711329853.2211795, "serving": 1711329853.5357587}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4513438, "arrival": 1711329852.790228}, "models": {"yolo": {"arrival": 1711329852.1762588, "serving": 1711329852.7638428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.458953853+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329845.4805274}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.411886, "arrival": 1711329853.907899}, "models": {"yolo": {"arrival": 1711329853.5623872, "serving": 1711329853.893048}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.452084, "arrival": 1711329853.9321876}, "models": {"yolo": {"arrival": 1711329853.5470123, "serving": 1711329853.895595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4400718, "arrival": 1711329853.6995416}, "models": {"yolo": {"arrival": 1711329850.6455326, "serving": 1711329853.5235763}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.454830169+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.4330304}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.4517908, "arrival": 1711329853.6388807}, "models": {"yolo": {"arrival": 1711329853.2211795, "serving": 1711329853.5357587}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4522667, "arrival": 1711329854.8345006}, "models": {"yolo": {"arrival": 1711329854.4602714, "serving": 1711329854.8251607}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.412087, "arrival": 1711329853.9364076}, "models": {"yolo": {"arrival": 1711329853.5549426, "serving": 1711329853.8967144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4330015, "arrival": 1711329853.9368818}, "models": {"yolo": {"arrival": 1711329853.5549426, "serving": 1711329853.8967144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.451935, "arrival": 1711329853.9314508}, "models": {"yolo": {"arrival": 1711329853.5470123, "serving": 1711329853.895595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4401686, "arrival": 1711329854.6130745}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4402883, "arrival": 1711329854.251056}, "models": {"yolo": {"arrival": 1711329853.890922, "serving": 1711329854.2243881}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4524484, "arrival": 1711329854.48121}, "models": {"yolo": {"arrival": 1711329854.175787, "serving": 1711329854.44338}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4516828, "arrival": 1711329854.4832191}, "models": {"yolo": {"arrival": 1711329854.2343335, "serving": 1711329854.4456546}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4518473, "arrival": 1711329854.624266}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4525537, "arrival": 1711329854.1549542}, "models": {"yolo": {"arrival": 1711329853.8070765, "serving": 1711329854.1188226}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 170.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.458881607+00:00\"}\"\n>", "times": {"request": {"sending": 1711329845.4520257}}, "outputs": []}, {"times": {"request": {"sending": 1711329845.4523063, "arrival": 1711329854.4809554}, "models": {"yolo": {"arrival": 1711329854.175787, "serving": 1711329854.44338}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4517, "arrival": 1711329854.6240218}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4520092, "arrival": 1711329854.174849}, "models": {"yolo": {"arrival": 1711329853.9058452, "serving": 1711329854.1604497}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4403703, "arrival": 1711329853.9418695}, "models": {"yolo": {"arrival": 1711329853.5623872, "serving": 1711329853.893048}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329845.4515102, "arrival": 1711329852.7909024}, "models": {"yolo": {"arrival": 1711329852.1762588, "serving": 1711329852.7638428}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329846.4136002, "arrival": 1711329854.6254268}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4427063, "arrival": 1711329855.2012794}, "models": {"yolo": {"arrival": 1711329854.8546884, "serving": 1711329855.1492999}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4426026, "arrival": 1711329854.5381484}, "models": {"yolo": {"arrival": 1711329854.1776123, "serving": 1711329854.4486978}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4141448, "arrival": 1711329853.3894317}, "models": {"yolo": {"arrival": 1711329853.001323, "serving": 1711329853.3625631}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4291532, "arrival": 1711329854.8659906}, "models": {"yolo": {"arrival": 1711329854.4566476, "serving": 1711329854.834926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4752474, "arrival": 1711329855.5115511}, "models": {"yolo": {"arrival": 1711329855.163938, "serving": 1711329855.4903367}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.441779, "arrival": 1711329854.875134}, "models": {"yolo": {"arrival": 1711329854.4566476, "serving": 1711329854.834926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4419158, "arrival": 1711329854.8780928}, "models": {"yolo": {"arrival": 1711329854.4566476, "serving": 1711329854.834926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4427226, "arrival": 1711329854.859377}, "models": {"yolo": {"arrival": 1711329854.4582133, "serving": 1711329854.8414931}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4135003, "arrival": 1711329853.9349582}, "models": {"yolo": {"arrival": 1711329853.5470123, "serving": 1711329853.895595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4752247, "arrival": 1711329855.842012}, "models": {"yolo": {"arrival": 1711329855.506352, "serving": 1711329855.82197}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4140868, "arrival": 1711329854.181778}, "models": {"yolo": {"arrival": 1711329853.9042811, "serving": 1711329854.167513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.442174, "arrival": 1711329855.195561}, "models": {"yolo": {"arrival": 1711329854.8462873, "serving": 1711329855.1501977}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.442205, "arrival": 1711329854.8198578}, "models": {"yolo": {"arrival": 1711329854.445201, "serving": 1711329854.7999036}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.44257, "arrival": 1711329854.82367}, "models": {"yolo": {"arrival": 1711329854.445201, "serving": 1711329854.7999036}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.656009883+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.442309}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4290338, "arrival": 1711329854.8770516}, "models": {"yolo": {"arrival": 1711329854.4652522, "serving": 1711329854.8439388}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4290771, "arrival": 1711329853.3927824}, "models": {"yolo": {"arrival": 1711329853.001323, "serving": 1711329853.3625631}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.442342, "arrival": 1711329855.1938996}, "models": {"yolo": {"arrival": 1711329854.8546884, "serving": 1711329855.1492999}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4294558, "arrival": 1711329854.1861148}, "models": {"yolo": {"arrival": 1711329853.9042811, "serving": 1711329854.167513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4420533, "arrival": 1711329855.195182}, "models": {"yolo": {"arrival": 1711329854.8462873, "serving": 1711329855.1501977}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4133608, "arrival": 1711329854.156432}, "models": {"yolo": {"arrival": 1711329853.8070765, "serving": 1711329854.1188226}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4256115, "arrival": 1711329854.862798}, "models": {"yolo": {"arrival": 1711329854.4566476, "serving": 1711329854.834926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4419904, "arrival": 1711329854.4936795}, "models": {"yolo": {"arrival": 1711329854.1776123, "serving": 1711329854.4486978}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4421594, "arrival": 1711329855.1967523}, "models": {"yolo": {"arrival": 1711329854.7952158, "serving": 1711329855.1544278}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4428542, "arrival": 1711329855.544402}, "models": {"yolo": {"arrival": 1711329855.15806, "serving": 1711329855.4978297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.443225, "arrival": 1711329854.8786695}, "models": {"yolo": {"arrival": 1711329854.4582133, "serving": 1711329854.8414931}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4421163, "arrival": 1711329854.4939344}, "models": {"yolo": {"arrival": 1711329854.1776123, "serving": 1711329854.4486978}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.413535, "arrival": 1711329853.3857095}, "models": {"yolo": {"arrival": 1711329853.001323, "serving": 1711329853.3625631}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4289355, "arrival": 1711329854.6266062}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4426904, "arrival": 1711329855.1810691}, "models": {"yolo": {"arrival": 1711329854.8098953, "serving": 1711329855.1454825}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.413458, "arrival": 1711329854.502787}, "models": {"yolo": {"arrival": 1711329854.148799, "serving": 1711329854.4485638}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4424205, "arrival": 1711329855.1962826}, "models": {"yolo": {"arrival": 1711329854.8462873, "serving": 1711329855.1501977}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4290109, "arrival": 1711329854.4747274}, "models": {"yolo": {"arrival": 1711329854.128202, "serving": 1711329854.4366095}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4432695, "arrival": 1711329855.841091}, "models": {"yolo": {"arrival": 1711329855.506352, "serving": 1711329855.82197}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4292002, "arrival": 1711329854.474953}, "models": {"yolo": {"arrival": 1711329854.128202, "serving": 1711329854.4366095}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4423711, "arrival": 1711329853.541006}, "models": {"yolo": {"arrival": 1711329853.3728285, "serving": 1711329853.5170152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.441815, "arrival": 1711329854.4753854}, "models": {"yolo": {"arrival": 1711329854.128202, "serving": 1711329854.4366095}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.631136649+00:00\"}\"\n>", "times": {"request": {"sending": 1711329846.4424965}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4425867, "arrival": 1711329855.2004774}, "models": {"yolo": {"arrival": 1711329854.8546884, "serving": 1711329855.1492999}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.63142084+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4428885}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.429357, "arrival": 1711329854.8666801}, "models": {"yolo": {"arrival": 1711329854.4566476, "serving": 1711329854.834926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4430861, "arrival": 1711329854.8633707}, "models": {"yolo": {"arrival": 1711329854.4582133, "serving": 1711329854.8414931}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.428962, "arrival": 1711329854.8638477}, "models": {"yolo": {"arrival": 1711329854.4566476, "serving": 1711329854.834926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4753523, "arrival": 1711329854.881796}, "models": {"yolo": {"arrival": 1711329854.4582133, "serving": 1711329854.8414931}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4753118, "arrival": 1711329855.1925569}, "models": {"yolo": {"arrival": 1711329854.8098953, "serving": 1711329855.1454825}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4422805, "arrival": 1711329855.1971023}, "models": {"yolo": {"arrival": 1711329854.7952158, "serving": 1711329855.1544278}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4136372, "arrival": 1711329854.489425}, "models": {"yolo": {"arrival": 1711329854.175787, "serving": 1711329854.44338}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4420822, "arrival": 1711329854.8190784}, "models": {"yolo": {"arrival": 1711329854.445201, "serving": 1711329854.7999036}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.442663, "arrival": 1711329855.201535}, "models": {"yolo": {"arrival": 1711329854.8462873, "serving": 1711329855.1501977}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4752681, "arrival": 1711329855.5542827}, "models": {"yolo": {"arrival": 1711329855.1657367, "serving": 1711329855.5035791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4137442, "arrival": 1711329854.5030847}, "models": {"yolo": {"arrival": 1711329854.148799, "serving": 1711329854.4485638}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4418483, "arrival": 1711329854.4934216}, "models": {"yolo": {"arrival": 1711329854.1776123, "serving": 1711329854.4486978}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.442406, "arrival": 1711329855.1997995}, "models": {"yolo": {"arrival": 1711329854.7952158, "serving": 1711329855.1544278}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4423847, "arrival": 1711329855.5361862}, "models": {"yolo": {"arrival": 1711329855.1563447, "serving": 1711329855.497062}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4433022, "arrival": 1711329855.5440278}, "models": {"yolo": {"arrival": 1711329855.1657367, "serving": 1711329855.5035791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4135678, "arrival": 1711329854.85142}, "models": {"yolo": {"arrival": 1711329854.4602714, "serving": 1711329854.8251607}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.428905, "arrival": 1711329855.1793175}, "models": {"yolo": {"arrival": 1711329854.8359787, "serving": 1711329855.1470163}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.656097704+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4428136}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4294043, "arrival": 1711329854.475169}, "models": {"yolo": {"arrival": 1711329854.128202, "serving": 1711329854.4366095}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4753344, "arrival": 1711329855.5559459}, "models": {"yolo": {"arrival": 1711329855.15806, "serving": 1711329855.4978297}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.656118523+00:00\"}\"\n>", "times": {"request": {"sending": 1711329846.443027}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4420378, "arrival": 1711329854.808038}, "models": {"yolo": {"arrival": 1711329854.4726362, "serving": 1711329854.786402}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.41398, "arrival": 1711329854.4740174}, "models": {"yolo": {"arrival": 1711329854.128202, "serving": 1711329854.4366095}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4432075, "arrival": 1711329855.5499532}, "models": {"yolo": {"arrival": 1711329855.15806, "serving": 1711329855.4978297}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.655899325+00:00\"}\"\n>", "times": {"request": {"sending": 1711329846.4293802}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4426303, "arrival": 1711329855.5495355}, "models": {"yolo": {"arrival": 1711329855.1563447, "serving": 1711329855.497062}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4428718, "arrival": 1711329854.862151}, "models": {"yolo": {"arrival": 1711329854.4582133, "serving": 1711329854.8414931}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.631779892+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4751709}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4423249, "arrival": 1711329854.8205197}, "models": {"yolo": {"arrival": 1711329854.445201, "serving": 1711329854.7999036}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.656031995+00:00\"}\"\n>", "times": {"request": {"sending": 1711329846.4424367}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.413832, "arrival": 1711329854.85252}, "models": {"yolo": {"arrival": 1711329854.4602714, "serving": 1711329854.8251607}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.655807123+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4139187}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.655856277+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4289856}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4424827, "arrival": 1711329854.5033019}, "models": {"yolo": {"arrival": 1711329854.1776123, "serving": 1711329854.4486978}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4422357, "arrival": 1711329854.4941504}, "models": {"yolo": {"arrival": 1711329854.1776123, "serving": 1711329854.4486978}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4426467, "arrival": 1711329855.2010086}, "models": {"yolo": {"arrival": 1711329854.7952158, "serving": 1711329855.1544278}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4142058, "arrival": 1711329854.853342}, "models": {"yolo": {"arrival": 1711329854.4602714, "serving": 1711329854.8251607}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4293053, "arrival": 1711329855.1802475}, "models": {"yolo": {"arrival": 1711329854.8359787, "serving": 1711329855.1470163}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4418643, "arrival": 1711329853.5340238}, "models": {"yolo": {"arrival": 1711329853.3728285, "serving": 1711329853.5170152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.441666, "arrival": 1711329855.1806562}, "models": {"yolo": {"arrival": 1711329854.8359787, "serving": 1711329855.1470163}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4137142, "arrival": 1711329854.1570578}, "models": {"yolo": {"arrival": 1711329853.8070765, "serving": 1711329854.1188226}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.631615198+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4432476}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4294786, "arrival": 1711329853.393871}, "models": {"yolo": {"arrival": 1711329853.001323, "serving": 1711329853.3625631}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.655751301+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4136817}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.656075035+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4426768}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4427965, "arrival": 1711329855.5428023}, "models": {"yolo": {"arrival": 1711329855.1657367, "serving": 1711329855.5035791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4431555, "arrival": 1711329855.5436628}, "models": {"yolo": {"arrival": 1711329855.1657367, "serving": 1711329855.5035791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4754503, "arrival": 1711329847.5828044}, "models": {"yolo": {"arrival": 1711329847.4447951, "serving": 1711329847.5622673}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.655943968+00:00\"}\"\n>", "times": {"request": {"sending": 1711329846.4419346}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4418328, "arrival": 1711329854.8830152}, "models": {"yolo": {"arrival": 1711329854.4652522, "serving": 1711329854.8439388}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4743323, "arrival": 1711329854.8814008}, "models": {"yolo": {"arrival": 1711329854.4582133, "serving": 1711329854.8414931}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4137735, "arrival": 1711329854.1810703}, "models": {"yolo": {"arrival": 1711329853.9042811, "serving": 1711329854.167513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4742398, "arrival": 1711329855.5503228}, "models": {"yolo": {"arrival": 1711329855.15806, "serving": 1711329855.4978297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4754124, "arrival": 1711329855.5147078}, "models": {"yolo": {"arrival": 1711329855.163938, "serving": 1711329855.4903367}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4293323, "arrival": 1711329854.8024836}, "models": {"yolo": {"arrival": 1711329854.4726362, "serving": 1711329854.786402}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.655987619+00:00\"}\"\n>", "times": {"request": {"sending": 1711329846.4421904}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.631298097+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.442743}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.655921576+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4417977}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4432855, "arrival": 1711329855.5106812}, "models": {"yolo": {"arrival": 1711329855.163938, "serving": 1711329855.4903367}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.443005, "arrival": 1711329855.5432813}, "models": {"yolo": {"arrival": 1711329855.1657367, "serving": 1711329855.5035791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4292393, "arrival": 1711329854.877585}, "models": {"yolo": {"arrival": 1711329854.4652522, "serving": 1711329854.8439388}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4427793, "arrival": 1711329855.2027497}, "models": {"yolo": {"arrival": 1711329854.7952158, "serving": 1711329855.1544278}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.65605351+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4425545}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4138024, "arrival": 1711329853.386785}, "models": {"yolo": {"arrival": 1711329853.001323, "serving": 1711329853.3625631}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4425244, "arrival": 1711329855.2001693}, "models": {"yolo": {"arrival": 1711329854.7952158, "serving": 1711329855.1544278}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.441753, "arrival": 1711329854.8060648}, "models": {"yolo": {"arrival": 1711329854.4726362, "serving": 1711329854.786402}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.656160191+00:00\"}\"\n>", "times": {"request": {"sending": 1711329846.4433181}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4288208, "arrival": 1711329853.390139}, "models": {"yolo": {"arrival": 1711329853.001323, "serving": 1711329853.3625631}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.443069, "arrival": 1711329855.5488274}, "models": {"yolo": {"arrival": 1711329855.15806, "serving": 1711329855.4978297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4292834, "arrival": 1711329853.393388}, "models": {"yolo": {"arrival": 1711329853.001323, "serving": 1711329853.3625631}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4294326, "arrival": 1711329854.8826709}, "models": {"yolo": {"arrival": 1711329854.4652522, "serving": 1711329854.8439388}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.631487411+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4431024}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4433358, "arrival": 1711329855.1914275}, "models": {"yolo": {"arrival": 1711329854.8098953, "serving": 1711329855.1454825}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4422212, "arrival": 1711329855.1935341}, "models": {"yolo": {"arrival": 1711329854.8546884, "serving": 1711329855.1492999}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.655878252+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4291756}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4754305, "arrival": 1711329855.5562572}, "models": {"yolo": {"arrival": 1711329855.1657367, "serving": 1711329855.5035791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4142683, "arrival": 1711329854.6258698}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4257395, "arrival": 1711329854.4744668}, "models": {"yolo": {"arrival": 1711329854.128202, "serving": 1711329854.4366095}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4138608, "arrival": 1711329854.6256514}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4140337, "arrival": 1711329854.876035}, "models": {"yolo": {"arrival": 1711329854.4652522, "serving": 1711329854.8439388}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4257653, "arrival": 1711329854.8765693}, "models": {"yolo": {"arrival": 1711329854.4652522, "serving": 1711329854.8439388}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.656139892+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4431715}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.655966196+00:00\"}\"\n>", "times": {"request": {"sending": 1711329846.4420671}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4419715, "arrival": 1711329854.8833532}, "models": {"yolo": {"arrival": 1711329854.4652522, "serving": 1711329854.8439388}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4290552, "arrival": 1711329854.1838467}, "models": {"yolo": {"arrival": 1711329853.9042811, "serving": 1711329854.167513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4138901, "arrival": 1711329854.8585215}, "models": {"yolo": {"arrival": 1711329854.4566476, "serving": 1711329854.834926}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4431188, "arrival": 1711329855.8401654}, "models": {"yolo": {"arrival": 1711329855.506352, "serving": 1711329855.82197}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4419553, "arrival": 1711329854.815753}, "models": {"yolo": {"arrival": 1711329854.445201, "serving": 1711329854.7999036}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4422944, "arrival": 1711329855.1959283}, "models": {"yolo": {"arrival": 1711329854.8462873, "serving": 1711329855.1501977}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4424686, "arrival": 1711329855.1942503}, "models": {"yolo": {"arrival": 1711329854.8546884, "serving": 1711329855.1492999}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.442451, "arrival": 1711329854.8227112}, "models": {"yolo": {"arrival": 1711329854.445201, "serving": 1711329854.7999036}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:06.656180645+00:00\"}\"\n>", "times": {"request": {"sending": 1711329846.4752922}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4421313, "arrival": 1711329853.536182}, "models": {"yolo": {"arrival": 1711329853.3728285, "serving": 1711329853.5170152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4427629, "arrival": 1711329855.5507205}, "models": {"yolo": {"arrival": 1711329855.1563447, "serving": 1711329855.497062}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4421453, "arrival": 1711329855.5337596}, "models": {"yolo": {"arrival": 1711329855.1563447, "serving": 1711329855.497062}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4418821, "arrival": 1711329855.1920493}, "models": {"yolo": {"arrival": 1711329854.8359787, "serving": 1711329855.1470163}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4753923, "arrival": 1711329855.8438997}, "models": {"yolo": {"arrival": 1711329855.506352, "serving": 1711329855.82197}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.429262, "arrival": 1711329854.1853862}, "models": {"yolo": {"arrival": 1711329853.9042811, "serving": 1711329854.167513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4420981, "arrival": 1711329855.1930487}, "models": {"yolo": {"arrival": 1711329854.8546884, "serving": 1711329855.1492999}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.443187, "arrival": 1711329855.183546}, "models": {"yolo": {"arrival": 1711329854.8098953, "serving": 1711329855.1454825}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4428306, "arrival": 1711329855.1814659}, "models": {"yolo": {"arrival": 1711329854.8098953, "serving": 1711329855.1454825}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4291236, "arrival": 1711329854.6263294}, "models": {"yolo": {"arrival": 1711329852.168135, "serving": 1711329854.43824}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:06.655833143+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 310.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 230.44 MiB free; 1.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4257061}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4431393, "arrival": 1711329855.2033813}, "models": {"yolo": {"arrival": 1711329854.7952158, "serving": 1711329855.1544278}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.44297, "arrival": 1711329855.8388672}, "models": {"yolo": {"arrival": 1711329855.506352, "serving": 1711329855.82197}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4290988, "arrival": 1711329855.1798}, "models": {"yolo": {"arrival": 1711329854.8359787, "serving": 1711329855.1470163}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4429884, "arrival": 1711329855.2031572}, "models": {"yolo": {"arrival": 1711329854.7952158, "serving": 1711329855.1544278}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4418995, "arrival": 1711329854.8071198}, "models": {"yolo": {"arrival": 1711329854.4726362, "serving": 1711329854.786402}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4257925, "arrival": 1711329854.1822724}, "models": {"yolo": {"arrival": 1711329853.9042811, "serving": 1711329854.167513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4422512, "arrival": 1711329853.5397415}, "models": {"yolo": {"arrival": 1711329853.3728285, "serving": 1711329853.5170152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4425104, "arrival": 1711329855.5369363}, "models": {"yolo": {"arrival": 1711329855.1563447, "serving": 1711329855.497062}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4422646, "arrival": 1711329855.5344784}, "models": {"yolo": {"arrival": 1711329855.1563447, "serving": 1711329855.497062}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4420094, "arrival": 1711329853.535289}, "models": {"yolo": {"arrival": 1711329853.3728285, "serving": 1711329853.5170152}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4420233, "arrival": 1711329855.1947436}, "models": {"yolo": {"arrival": 1711329854.8359787, "serving": 1711329855.1470163}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4425404, "arrival": 1711329855.200744}, "models": {"yolo": {"arrival": 1711329854.8462873, "serving": 1711329855.1501977}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.631243389+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329846.4426157}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.63181538+00:00\"}\"\n>", "times": {"request": {"sending": 1711329846.4753733}}, "outputs": []}, {"times": {"request": {"sending": 1711329846.4423566, "arrival": 1711329854.5025103}, "models": {"yolo": {"arrival": 1711329854.1776123, "serving": 1711329854.4486978}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329846.4430425, "arrival": 1711329855.1828551}, "models": {"yolo": {"arrival": 1711329854.8098953, "serving": 1711329855.1454825}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329847.4378898, "arrival": 1711329856.8044894}, "models": {"yolo": {"arrival": 1711329856.2987957, "serving": 1711329856.7883642}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4377294, "arrival": 1711329859.4174175}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.412732, "arrival": 1711329854.8822465}, "models": {"yolo": {"arrival": 1711329854.4582133, "serving": 1711329854.8414931}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.437545, "arrival": 1711329857.0731726}, "models": {"yolo": {"arrival": 1711329856.4988797, "serving": 1711329857.0541115}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.691848246+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4362884}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:08.512252345+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.429087}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4125962, "arrival": 1711329855.5316184}, "models": {"yolo": {"arrival": 1711329855.1535387, "serving": 1711329855.4959137}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4126866, "arrival": 1711329855.5568945}, "models": {"yolo": {"arrival": 1711329855.15806, "serving": 1711329855.4978297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4698331, "arrival": 1711329857.2120743}, "models": {"yolo": {"arrival": 1711329856.940823, "serving": 1711329857.1895185}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4285543, "arrival": 1711329855.1781433}, "models": {"yolo": {"arrival": 1711329854.8520775, "serving": 1711329855.1418974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4372945, "arrival": 1711329855.8894787}, "models": {"yolo": {"arrival": 1711329855.506112, "serving": 1711329855.8303025}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:08.51794006+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4379349}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.426872, "arrival": 1711329855.177283}, "models": {"yolo": {"arrival": 1711329854.8520775, "serving": 1711329855.1418974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4374278, "arrival": 1711329855.889845}, "models": {"yolo": {"arrival": 1711329855.506112, "serving": 1711329855.8303025}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4130638, "arrival": 1711329859.3647928}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4370742, "arrival": 1711329856.309931}, "models": {"yolo": {"arrival": 1711329855.842051, "serving": 1711329856.2906737}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.413318, "arrival": 1711329859.3656237}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.437528, "arrival": 1711329856.4054303}, "models": {"yolo": {"arrival": 1711329855.8412333, "serving": 1711329856.3662138}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.631855167+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4127696}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4288862, "arrival": 1711329855.1786962}, "models": {"yolo": {"arrival": 1711329854.8520775, "serving": 1711329855.1418974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.436329, "arrival": 1711329855.894975}, "models": {"yolo": {"arrival": 1711329855.5002227, "serving": 1711329855.8322632}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.41287, "arrival": 1711329855.5565803}, "models": {"yolo": {"arrival": 1711329855.1657367, "serving": 1711329855.5035791}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.437141, "arrival": 1711329856.5163312}, "models": {"yolo": {"arrival": 1711329855.8330674, "serving": 1711329856.4898758}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4371583, "arrival": 1711329855.551765}, "models": {"yolo": {"arrival": 1711329855.1521063, "serving": 1711329855.496465}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4774575, "arrival": 1711329859.4259703}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.436241, "arrival": 1711329856.5016973}, "models": {"yolo": {"arrival": 1711329855.8330674, "serving": 1711329856.4898758}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4130003, "arrival": 1711329855.1680577}, "models": {"yolo": {"arrival": 1711329854.8520775, "serving": 1711329855.1418974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4132533, "arrival": 1711329855.1692681}, "models": {"yolo": {"arrival": 1711329854.8520775, "serving": 1711329855.1418974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4363096, "arrival": 1711329859.381734}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.699843293+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4370394}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4287224, "arrival": 1711329855.8891144}, "models": {"yolo": {"arrival": 1711329855.5165727, "serving": 1711329855.8297448}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.437863, "arrival": 1711329859.4179275}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:08.51239156+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.4375105}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.699919119+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.437312}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4285128, "arrival": 1711329855.8843272}, "models": {"yolo": {"arrival": 1711329855.5072656, "serving": 1711329855.8242538}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.469793, "arrival": 1711329859.4251475}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.700058051+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.437846}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4290466, "arrival": 1711329855.8943772}, "models": {"yolo": {"arrival": 1711329855.5165727, "serving": 1711329855.8297448}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.412804, "arrival": 1711329859.3608625}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4738925, "arrival": 1711329857.110682}, "models": {"yolo": {"arrival": 1711329856.3740377, "serving": 1711329857.086065}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4737616, "arrival": 1711329857.213331}, "models": {"yolo": {"arrival": 1711329856.940823, "serving": 1711329857.1895185}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4373457, "arrival": 1711329856.3116262}, "models": {"yolo": {"arrival": 1711329855.842051, "serving": 1711329856.2906737}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4131901, "arrival": 1711329855.5327334}, "models": {"yolo": {"arrival": 1711329855.1535387, "serving": 1711329855.4959137}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4130948, "arrival": 1711329855.5162413}, "models": {"yolo": {"arrival": 1711329855.163938, "serving": 1711329855.4903367}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4284704, "arrival": 1711329855.5514216}, "models": {"yolo": {"arrival": 1711329855.1535387, "serving": 1711329855.4959137}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4364133, "arrival": 1711329856.511509}, "models": {"yolo": {"arrival": 1711329855.8330674, "serving": 1711329856.4898758}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4697511, "arrival": 1711329855.8954704}, "models": {"yolo": {"arrival": 1711329855.506112, "serving": 1711329855.8303025}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4290056, "arrival": 1711329855.8920958}, "models": {"yolo": {"arrival": 1711329855.5002227, "serving": 1711329855.8322632}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:08.517797566+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.4376469}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4131255, "arrival": 1711329855.8879619}, "models": {"yolo": {"arrival": 1711329855.5165727, "serving": 1711329855.8297448}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.631886708+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.413032}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.473928, "arrival": 1711329857.2268093}, "models": {"yolo": {"arrival": 1711329857.0603304, "serving": 1711329857.1972833}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.436348, "arrival": 1711329855.8952222}, "models": {"yolo": {"arrival": 1711329855.5165727, "serving": 1711329855.8297448}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:08.517894418+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.43778}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4366515, "arrival": 1711329856.3830602}, "models": {"yolo": {"arrival": 1711329855.8392072, "serving": 1711329856.3639767}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4374616, "arrival": 1711329859.4091725}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4737277, "arrival": 1711329857.2279172}, "models": {"yolo": {"arrival": 1711329856.7967722, "serving": 1711329857.1995144}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:08.512290894+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.4363692}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4370222, "arrival": 1711329855.5510705}, "models": {"yolo": {"arrival": 1711329855.1521063, "serving": 1711329855.496465}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.437478, "arrival": 1711329856.8021555}, "models": {"yolo": {"arrival": 1711329856.2987957, "serving": 1711329856.7883642}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4129686, "arrival": 1711329855.882377}, "models": {"yolo": {"arrival": 1711329855.5072656, "serving": 1711329855.8242538}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4131575, "arrival": 1711329847.5841296}, "models": {"yolo": {"arrival": 1711329847.4447951, "serving": 1711329847.5622673}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:08.507503264+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.4134135}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.699956089+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.4374454}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:08.512305689+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4365244}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.437613, "arrival": 1711329856.8037846}, "models": {"yolo": {"arrival": 1711329856.2987957, "serving": 1711329856.7883642}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.436432, "arrival": 1711329855.5296228}, "models": {"yolo": {"arrival": 1711329855.1521063, "serving": 1711329855.496465}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4376633, "arrival": 1711329856.40605}, "models": {"yolo": {"arrival": 1711329855.8412333, "serving": 1711329856.3662138}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4365819, "arrival": 1711329855.5303257}, "models": {"yolo": {"arrival": 1711329855.1521063, "serving": 1711329855.496465}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.436616, "arrival": 1711329859.3884256}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4369533, "arrival": 1711329856.3848352}, "models": {"yolo": {"arrival": 1711329855.8392072, "serving": 1711329856.3639767}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4365063, "arrival": 1711329856.3822732}, "models": {"yolo": {"arrival": 1711329855.8392072, "serving": 1711329856.3639767}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:08.518047321+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4738235}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4128377, "arrival": 1711329855.515568}, "models": {"yolo": {"arrival": 1711329855.163938, "serving": 1711329855.4903367}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.699989962+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.4375787}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4368794, "arrival": 1711329855.5309863}, "models": {"yolo": {"arrival": 1711329855.1521063, "serving": 1711329855.496465}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4288054, "arrival": 1711329855.5521219}, "models": {"yolo": {"arrival": 1711329855.1535387, "serving": 1711329855.4959137}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4700608, "arrival": 1711329857.1095045}, "models": {"yolo": {"arrival": 1711329856.3740377, "serving": 1711329857.086065}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4129367, "arrival": 1711329855.5322201}, "models": {"yolo": {"arrival": 1711329855.1535387, "serving": 1711329855.4959137}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4129026, "arrival": 1711329847.5835123}, "models": {"yolo": {"arrival": 1711329847.4447951, "serving": 1711329847.5622673}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:08.512363593+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.437243}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.437395, "arrival": 1711329856.4015136}, "models": {"yolo": {"arrival": 1711329855.8412333, "serving": 1711329856.3662138}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4286346, "arrival": 1711329859.3795934}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4376965, "arrival": 1711329855.8906214}, "models": {"yolo": {"arrival": 1711329855.506112, "serving": 1711329855.8303025}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.699887448+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4371748}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4696715, "arrival": 1711329857.225485}, "models": {"yolo": {"arrival": 1711329857.0603304, "serving": 1711329857.1972833}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4377465, "arrival": 1711329856.8042536}, "models": {"yolo": {"arrival": 1711329856.2987957, "serving": 1711329856.7883642}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4371912, "arrival": 1711329859.3909132}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:08.51233575+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4369712}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4373627, "arrival": 1711329856.3983207}, "models": {"yolo": {"arrival": 1711329855.8392072, "serving": 1711329856.3639767}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4370565, "arrival": 1711329859.3901908}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4369364, "arrival": 1711329856.3089302}, "models": {"yolo": {"arrival": 1711329855.842051, "serving": 1711329856.2906737}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.437918, "arrival": 1711329856.9500794}, "models": {"yolo": {"arrival": 1711329856.3742514, "serving": 1711329856.928742}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4133494, "arrival": 1711329855.890997}, "models": {"yolo": {"arrival": 1711329855.5002227, "serving": 1711329855.8322632}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.685333864+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.413286}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4365458, "arrival": 1711329855.8859406}, "models": {"yolo": {"arrival": 1711329855.5067997, "serving": 1711329855.8304064}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4374945, "arrival": 1711329856.947072}, "models": {"yolo": {"arrival": 1711329856.3742514, "serving": 1711329856.928742}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4777372, "arrival": 1711329857.2292905}, "models": {"yolo": {"arrival": 1711329856.7967722, "serving": 1711329857.1995144}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.700088289+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.4697742}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.473695, "arrival": 1711329859.4256606}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4373288, "arrival": 1711329859.4072876}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.469814, "arrival": 1711329857.2275622}, "models": {"yolo": {"arrival": 1711329856.7967722, "serving": 1711329857.1995144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4376795, "arrival": 1711329857.0739157}, "models": {"yolo": {"arrival": 1711329856.4988797, "serving": 1711329857.0541115}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4700763, "arrival": 1711329857.2261124}, "models": {"yolo": {"arrival": 1711329857.0603304, "serving": 1711329857.1972833}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.700026665+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.437713}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4289658, "arrival": 1711329859.3812234}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4363935, "arrival": 1711329855.8855681}, "models": {"yolo": {"arrival": 1711329855.5067997, "serving": 1711329855.8304064}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4739625, "arrival": 1711329856.37788}, "models": {"yolo": {"arrival": 1711329855.8387852, "serving": 1711329856.3623118}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.437125, "arrival": 1711329855.8874836}, "models": {"yolo": {"arrival": 1711329855.5067997, "serving": 1711329855.8304064}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.437005, "arrival": 1711329856.5140023}, "models": {"yolo": {"arrival": 1711329855.8330674, "serving": 1711329856.4898758}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:08.512320647+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.436669}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.691924698+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.436599}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4372602, "arrival": 1711329856.401007}, "models": {"yolo": {"arrival": 1711329855.8412333, "serving": 1711329856.3662138}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.437763, "arrival": 1711329856.9485857}, "models": {"yolo": {"arrival": 1711329856.3742514, "serving": 1711329856.928742}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4364696, "arrival": 1711329859.3844304}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4364877, "arrival": 1711329855.8957117}, "models": {"yolo": {"arrival": 1711329855.5002227, "serving": 1711329855.8322632}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4133809, "arrival": 1711329855.888359}, "models": {"yolo": {"arrival": 1711329855.5165727, "serving": 1711329855.8297448}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.747119699+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.4773443}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:08.507568695+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.4287632}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4286811, "arrival": 1711329855.8917222}, "models": {"yolo": {"arrival": 1711329855.5002227, "serving": 1711329855.8322632}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4377959, "arrival": 1711329856.4066465}, "models": {"yolo": {"arrival": 1711329855.8412333, "serving": 1711329856.3662138}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.436707, "arrival": 1711329856.5132678}, "models": {"yolo": {"arrival": 1711329855.8330674, "serving": 1711329856.4898758}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.43763, "arrival": 1711329856.947789}, "models": {"yolo": {"arrival": 1711329856.3742514, "serving": 1711329856.928742}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4366338, "arrival": 1711329856.3049717}, "models": {"yolo": {"arrival": 1711329855.842051, "serving": 1711329856.2906737}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4370904, "arrival": 1711329856.3855476}, "models": {"yolo": {"arrival": 1711329855.8392072, "serving": 1711329856.3639767}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4369886, "arrival": 1711329855.886712}, "models": {"yolo": {"arrival": 1711329855.5067997, "serving": 1711329855.8304064}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.427974, "arrival": 1711329855.8887386}, "models": {"yolo": {"arrival": 1711329855.5165727, "serving": 1711329855.8297448}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4375618, "arrival": 1711329855.8902187}, "models": {"yolo": {"arrival": 1711329855.506112, "serving": 1711329855.8303025}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4378133, "arrival": 1711329857.074565}, "models": {"yolo": {"arrival": 1711329856.4988797, "serving": 1711329857.0541115}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.747041432+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.4736571}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.436689, "arrival": 1711329855.8863094}, "models": {"yolo": {"arrival": 1711329855.5067997, "serving": 1711329855.8304064}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.691900357+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.436451}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4132216, "arrival": 1711329855.8831894}, "models": {"yolo": {"arrival": 1711329855.5072656, "serving": 1711329855.8242538}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.69976998+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4369001}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:08.518088458+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4777956}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.436564, "arrival": 1711329856.5123737}, "models": {"yolo": {"arrival": 1711329855.8330674, "serving": 1711329856.4898758}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:08.507552992+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.4280183}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:08.517996402+00:00\"}\"\n>", "times": {"request": {"sending": 1711329847.4700446}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4372084, "arrival": 1711329856.31081}, "models": {"yolo": {"arrival": 1711329855.842051, "serving": 1711329856.2906737}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:08.512378048+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4373786}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4362657, "arrival": 1711329855.5287077}, "models": {"yolo": {"arrival": 1711329855.1521063, "serving": 1711329855.496465}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.473549, "arrival": 1711329856.3755527}, "models": {"yolo": {"arrival": 1711329855.8387852, "serving": 1711329856.3623118}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4372258, "arrival": 1711329856.3974333}, "models": {"yolo": {"arrival": 1711329855.8392072, "serving": 1711329856.3639767}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4278812, "arrival": 1711329859.379211}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4369183, "arrival": 1711329859.389365}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4372776, "arrival": 1711329856.5172973}, "models": {"yolo": {"arrival": 1711329855.8330674, "serving": 1711329856.4898758}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.691821522+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4289258}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4288454, "arrival": 1711329855.8848224}, "models": {"yolo": {"arrival": 1711329855.5072656, "serving": 1711329855.8242538}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4378302, "arrival": 1711329855.8947232}, "models": {"yolo": {"arrival": 1711329855.506112, "serving": 1711329855.8303025}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4375956, "arrival": 1711329859.4100332}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4361615, "arrival": 1711329855.8851953}, "models": {"yolo": {"arrival": 1711329855.5067997, "serving": 1711329855.8304064}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.427926, "arrival": 1711329855.8913677}, "models": {"yolo": {"arrival": 1711329855.5002227, "serving": 1711329855.8322632}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.413448, "arrival": 1711329855.5332394}, "models": {"yolo": {"arrival": 1711329855.1535387, "serving": 1711329855.4959137}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4379518, "arrival": 1711329856.407185}, "models": {"yolo": {"arrival": 1711329855.8412333, "serving": 1711329856.3662138}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.68542657+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4278066}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.477771, "arrival": 1711329857.2161047}, "models": {"yolo": {"arrival": 1711329856.940823, "serving": 1711329857.1895185}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329847.4374113, "arrival": 1711329857.0694823}, "models": {"yolo": {"arrival": 1711329856.4988797, "serving": 1711329857.0541115}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.691752662+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 1.89 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.4285944}}, "outputs": []}, {"times": {"request": {"sending": 1711329847.4267726, "arrival": 1711329855.8837638}, "models": {"yolo": {"arrival": 1711329855.5072656, "serving": 1711329855.8242538}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:08.512348633+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329847.437107}}, "outputs": []}], [{"times": {"request": {"sending": 1711329848.44504, "arrival": 1711329857.6402872}, "models": {"yolo": {"arrival": 1711329857.201861, "serving": 1711329857.6001332}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.436631, "arrival": 1711329857.8001485}, "models": {"yolo": {"arrival": 1711329857.208754, "serving": 1711329857.7706063}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4101915, "arrival": 1711329857.227208}, "models": {"yolo": {"arrival": 1711329857.0603304, "serving": 1711329857.1972833}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4107382, "arrival": 1711329857.620418}, "models": {"yolo": {"arrival": 1711329857.2009344, "serving": 1711329857.5908723}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4435961, "arrival": 1711329857.6312032}, "models": {"yolo": {"arrival": 1711329857.2009344, "serving": 1711329857.5908723}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4445772, "arrival": 1711329857.8373063}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.853194499+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.444928}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4437222, "arrival": 1711329857.8701568}, "models": {"yolo": {"arrival": 1711329857.6021776, "serving": 1711329857.843528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4438884, "arrival": 1711329857.8200028}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4365683, "arrival": 1711329857.6307092}, "models": {"yolo": {"arrival": 1711329857.2009344, "serving": 1711329857.5908723}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.411005, "arrival": 1711329857.2356794}, "models": {"yolo": {"arrival": 1711329856.7967722, "serving": 1711329857.1995144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4106605, "arrival": 1711329859.4268272}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.25915291+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4322207}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.269932004+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4439898}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4763105, "arrival": 1711329859.4417078}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444825, "arrival": 1711329859.4407604}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4105053, "arrival": 1711329857.1136186}, "models": {"yolo": {"arrival": 1711329856.3740377, "serving": 1711329857.086065}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4363594, "arrival": 1711329859.4277024}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.85326441+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4451663}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.840089122+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4443915}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:08.518144504+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4104657}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.444898, "arrival": 1711329859.3391125}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4108155, "arrival": 1711329857.1142452}, "models": {"yolo": {"arrival": 1711329856.3740377, "serving": 1711329857.086065}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.853099353+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4446912}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4105442, "arrival": 1711329857.7898905}, "models": {"yolo": {"arrival": 1711329857.208754, "serving": 1711329857.7706063}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444021, "arrival": 1711329857.8028166}, "models": {"yolo": {"arrival": 1711329857.208754, "serving": 1711329857.7706063}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.754203449+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4440475}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4364054, "arrival": 1711329857.63023}, "models": {"yolo": {"arrival": 1711329857.2009344, "serving": 1711329857.5908723}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.436469, "arrival": 1711329857.7960432}, "models": {"yolo": {"arrival": 1711329857.208754, "serving": 1711329857.7706063}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4441197, "arrival": 1711329857.8214247}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:08.518200367+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 94.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.410777}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4437077, "arrival": 1711329857.6287735}, "models": {"yolo": {"arrival": 1711329857.2082067, "serving": 1711329857.592082}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4100864, "arrival": 1711329857.1129117}, "models": {"yolo": {"arrival": 1711329856.3740377, "serving": 1711329857.086065}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.410426, "arrival": 1711329857.2175014}, "models": {"yolo": {"arrival": 1711329856.940823, "serving": 1711329857.1895185}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4440062, "arrival": 1711329857.8206725}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.753995371+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4112358}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4447954, "arrival": 1711329857.6393237}, "models": {"yolo": {"arrival": 1711329857.201861, "serving": 1711329857.6001332}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4451523, "arrival": 1711329857.848296}, "models": {"yolo": {"arrival": 1711329857.6099381, "serving": 1711329857.8385766}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.754224276+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.444162}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4447346, "arrival": 1711329858.9778223}, "models": {"yolo": {"arrival": 1711329857.855969, "serving": 1711329858.9489412}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4102418, "arrival": 1711329856.3787022}, "models": {"yolo": {"arrival": 1711329855.8387852, "serving": 1711329856.3623118}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.270062659+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4451118}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4366102, "arrival": 1711329857.2347422}, "models": {"yolo": {"arrival": 1711329857.0949361, "serving": 1711329857.2089658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4451387, "arrival": 1711329859.367198}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4450693, "arrival": 1711329859.4414291}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.445084, "arrival": 1711329859.100012}, "models": {"yolo": {"arrival": 1711329857.8462107, "serving": 1711329859.0802536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4103653, "arrival": 1711329857.229769}, "models": {"yolo": {"arrival": 1711329856.7967722, "serving": 1711329857.1995144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4763982, "arrival": 1711329859.1079588}, "models": {"yolo": {"arrival": 1711329857.8462107, "serving": 1711329859.0802536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4438574, "arrival": 1711329857.8708556}, "models": {"yolo": {"arrival": 1711329857.6021776, "serving": 1711329857.843528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4105823, "arrival": 1711329856.3990338}, "models": {"yolo": {"arrival": 1711329855.8387852, "serving": 1711329856.3623118}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4103253, "arrival": 1711329859.4264975}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4438298, "arrival": 1711329859.4327433}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4322863, "arrival": 1711329856.938169}, "models": {"yolo": {"arrival": 1711329856.371364, "serving": 1711329856.9234133}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.853167827+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4448109}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.44377, "arrival": 1711329857.8019192}, "models": {"yolo": {"arrival": 1711329857.208754, "serving": 1711329857.7706063}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.43645, "arrival": 1711329857.2323558}, "models": {"yolo": {"arrival": 1711329857.0949361, "serving": 1711329857.2089658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4321966, "arrival": 1711329857.6220737}, "models": {"yolo": {"arrival": 1711329857.2009344, "serving": 1711329857.5908723}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.74715425+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4102845}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4442906, "arrival": 1711329859.4339705}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4450974, "arrival": 1711329858.981595}, "models": {"yolo": {"arrival": 1711329857.855969, "serving": 1711329858.9489412}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.411043, "arrival": 1711329857.6213582}, "models": {"yolo": {"arrival": 1711329857.2009344, "serving": 1711329857.5908723}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.270045003+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4449952}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4450123, "arrival": 1711329857.8443937}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4111202, "arrival": 1711329857.2315552}, "models": {"yolo": {"arrival": 1711329857.0949361, "serving": 1711329857.2089658}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.269998132+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4445624}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.443755, "arrival": 1711329857.8190958}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4365475, "arrival": 1711329857.6243994}, "models": {"yolo": {"arrival": 1711329857.2082067, "serving": 1711329857.592082}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4108903, "arrival": 1711329856.3996632}, "models": {"yolo": {"arrival": 1711329855.8387852, "serving": 1711329856.3623118}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444148, "arrival": 1711329857.2210476}, "models": {"yolo": {"arrival": 1711329856.9295347, "serving": 1711329857.188419}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.269958168+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4442177}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4436455, "arrival": 1711329857.8011425}, "models": {"yolo": {"arrival": 1711329857.208754, "serving": 1711329857.7706063}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4444623, "arrival": 1711329857.8364284}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444061, "arrival": 1711329859.4333723}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.270024093+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4448664}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.44396, "arrival": 1711329857.8672955}, "models": {"yolo": {"arrival": 1711329857.601435, "serving": 1711329857.835362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4436302, "arrival": 1711329857.2360456}, "models": {"yolo": {"arrival": 1711329857.0949361, "serving": 1711329857.2089658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444204, "arrival": 1711329857.8772001}, "models": {"yolo": {"arrival": 1711329857.6021776, "serving": 1711329857.843528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4363842, "arrival": 1711329857.6234868}, "models": {"yolo": {"arrival": 1711329857.2082067, "serving": 1711329857.592082}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.410853, "arrival": 1711329857.7915127}, "models": {"yolo": {"arrival": 1711329857.208754, "serving": 1711329857.7706063}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.269972577+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.444333}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.443903, "arrival": 1711329857.802411}, "models": {"yolo": {"arrival": 1711329857.208754, "serving": 1711329857.7706063}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4764218, "arrival": 1711329858.984183}, "models": {"yolo": {"arrival": 1711329857.855969, "serving": 1711329858.9489412}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.269856107+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.436588}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4447672, "arrival": 1711329857.8399334}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4447813, "arrival": 1711329859.3383358}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4445927, "arrival": 1711329859.3362427}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.432243, "arrival": 1711329857.2319882}, "models": {"yolo": {"arrival": 1711329857.0949361, "serving": 1711329857.2089658}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.753921067+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4106207}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4438438, "arrival": 1711329857.6295671}, "models": {"yolo": {"arrival": 1711329857.2082067, "serving": 1711329857.592082}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.269889674+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4436126}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4439747, "arrival": 1711329857.8729148}, "models": {"yolo": {"arrival": 1711329857.6021776, "serving": 1711329857.843528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4437842, "arrival": 1711329857.2168815}, "models": {"yolo": {"arrival": 1711329856.9295347, "serving": 1711329857.188419}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.753972082+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4109278}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4435585, "arrival": 1711329859.4319613}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4445336, "arrival": 1711329857.8776853}, "models": {"yolo": {"arrival": 1711329857.601435, "serving": 1711329857.835362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4445198, "arrival": 1711329859.4347732}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.432074, "arrival": 1711329859.4274151}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.410699, "arrival": 1711329857.2351513}, "models": {"yolo": {"arrival": 1711329856.7967722, "serving": 1711329857.1995144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444363, "arrival": 1711329857.881496}, "models": {"yolo": {"arrival": 1711329857.781465, "serving": 1711329857.8501754}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.754117599+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.443676}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.443946, "arrival": 1711329859.4330678}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.269905069+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4437385}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4443765, "arrival": 1711329857.6344976}, "models": {"yolo": {"arrival": 1711329857.201861, "serving": 1711329857.6001332}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.840119077+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.444506}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.27007956+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4774966}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4448812, "arrival": 1711329857.8410676}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.411197, "arrival": 1711329856.4004846}, "models": {"yolo": {"arrival": 1711329855.8387852, "serving": 1711329856.3623118}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4442332, "arrival": 1711329857.8227375}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.754140613+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4438145}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.754094258+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4433508}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4440343, "arrival": 1711329857.220522}, "models": {"yolo": {"arrival": 1711329856.9295347, "serving": 1711329857.188419}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.853227471+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4450533}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.44366, "arrival": 1711329856.9434023}, "models": {"yolo": {"arrival": 1711329856.371364, "serving": 1711329856.9234133}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4439163, "arrival": 1711329857.219888}, "models": {"yolo": {"arrival": 1711329856.9295347, "serving": 1711329857.188419}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.436489, "arrival": 1711329856.9415488}, "models": {"yolo": {"arrival": 1711329856.371364, "serving": 1711329856.9234133}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4109666, "arrival": 1711329859.4271247}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444433, "arrival": 1711329857.8786445}, "models": {"yolo": {"arrival": 1711329857.6021776, "serving": 1711329857.843528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4322646, "arrival": 1711329857.795406}, "models": {"yolo": {"arrival": 1711329857.208754, "serving": 1711329857.7706063}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4441903, "arrival": 1711329857.868868}, "models": {"yolo": {"arrival": 1711329857.601435, "serving": 1711329857.835362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.411158, "arrival": 1711329857.7946527}, "models": {"yolo": {"arrival": 1711329857.208754, "serving": 1711329857.7706063}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.259097595+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4110806}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4451253, "arrival": 1711329857.84541}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.269919066+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4438727}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4449618, "arrival": 1711329859.0993884}, "models": {"yolo": {"arrival": 1711329857.8462107, "serving": 1711329859.0802536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444548, "arrival": 1711329858.976701}, "models": {"yolo": {"arrival": 1711329857.855969, "serving": 1711329858.9489412}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444319, "arrival": 1711329857.8781464}, "models": {"yolo": {"arrival": 1711329857.6021776, "serving": 1711329857.843528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444911, "arrival": 1711329857.639865}, "models": {"yolo": {"arrival": 1711329857.201861, "serving": 1711329857.6001332}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444262, "arrival": 1711329857.6339564}, "models": {"yolo": {"arrival": 1711329857.201861, "serving": 1711329857.6001332}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4365277, "arrival": 1711329859.4280176}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.270011363+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4447532}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4443486, "arrival": 1711329857.8271859}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4448388, "arrival": 1711329859.0986116}, "models": {"yolo": {"arrival": 1711329857.8462107, "serving": 1711329859.0802536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4441347, "arrival": 1711329857.8791015}, "models": {"yolo": {"arrival": 1711329857.781465, "serving": 1711329857.8501754}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.269984928+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4444478}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4446747, "arrival": 1711329857.6355083}, "models": {"yolo": {"arrival": 1711329857.201861, "serving": 1711329857.6001332}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4449782, "arrival": 1711329858.9809215}, "models": {"yolo": {"arrival": 1711329857.855969, "serving": 1711329858.9489412}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4432757, "arrival": 1711329856.9424584}, "models": {"yolo": {"arrival": 1711329856.371364, "serving": 1711329856.9234133}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444406, "arrival": 1711329859.4344738}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4321692, "arrival": 1711329857.6227725}, "models": {"yolo": {"arrival": 1711329857.2082067, "serving": 1711329857.592082}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.269943989+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.444104}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4440906, "arrival": 1711329857.8734934}, "models": {"yolo": {"arrival": 1711329857.6021776, "serving": 1711329857.843528}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.754071611+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4365084}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4444766, "arrival": 1711329857.8819463}, "models": {"yolo": {"arrival": 1711329857.781465, "serving": 1711329857.8501754}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4447052, "arrival": 1711329859.4350812}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444248, "arrival": 1711329857.880982}, "models": {"yolo": {"arrival": 1711329857.781465, "serving": 1711329857.8501754}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.840018041+00:00\"}\"\n>", "times": {"request": {"sending": 1711329848.4442766}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4449441, "arrival": 1711329859.441168}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444176, "arrival": 1711329859.4336772}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.754031606+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.436277}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4444911, "arrival": 1711329857.6351006}, "models": {"yolo": {"arrival": 1711329857.201861, "serving": 1711329857.6001332}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4435785, "arrival": 1711329857.6250556}, "models": {"yolo": {"arrival": 1711329857.2082067, "serving": 1711329857.592082}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4436908, "arrival": 1711329859.432427}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4443047, "arrival": 1711329857.869456}, "models": {"yolo": {"arrival": 1711329857.601435, "serving": 1711329857.835362}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.259169932+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 36.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.436427}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4444196, "arrival": 1711329857.876667}, "models": {"yolo": {"arrival": 1711329857.601435, "serving": 1711329857.835362}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.754180326+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329848.4439313}}, "outputs": []}, {"times": {"request": {"sending": 1711329848.4450257, "arrival": 1711329859.366428}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4440758, "arrival": 1711329857.8681588}, "models": {"yolo": {"arrival": 1711329857.601435, "serving": 1711329857.835362}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.4448516, "arrival": 1711329858.9788303}, "models": {"yolo": {"arrival": 1711329857.855969, "serving": 1711329858.9489412}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329848.444721, "arrival": 1711329859.0957892}, "models": {"yolo": {"arrival": 1711329857.8462107, "serving": 1711329859.0802536}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329849.4591396, "arrival": 1711329859.6313796}, "models": {"yolo": {"arrival": 1711329859.200263, "serving": 1711329859.6189127}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4144218, "arrival": 1711329857.8462653}, "models": {"yolo": {"arrival": 1711329857.219403, "serving": 1711329857.8004947}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.415127, "arrival": 1711329859.1094723}, "models": {"yolo": {"arrival": 1711329857.8462107, "serving": 1711329859.0802536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4412541, "arrival": 1711329859.3788218}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4146557, "arrival": 1711329857.8489685}, "models": {"yolo": {"arrival": 1711329857.6099381, "serving": 1711329857.8385766}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.44628, "arrival": 1711329859.505114}, "models": {"yolo": {"arrival": 1711329859.1959193, "serving": 1711329859.4504695}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4149284, "arrival": 1711329860.7089503}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4598193, "arrival": 1711329860.7182798}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4882987, "arrival": 1711329862.124122}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4463127, "arrival": 1711329860.7167456}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4151657, "arrival": 1711329858.9852984}, "models": {"yolo": {"arrival": 1711329857.855969, "serving": 1711329858.9489412}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.91962316+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4609761}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.414763, "arrival": 1711329859.441969}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.85329687+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.4147172}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4602675, "arrival": 1711329860.7188976}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4154851, "arrival": 1711329859.2057636}, "models": {"yolo": {"arrival": 1711329858.964451, "serving": 1711329859.182373}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4588628, "arrival": 1711329859.5053596}, "models": {"yolo": {"arrival": 1711329859.1959193, "serving": 1711329859.4504695}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.459455, "arrival": 1711329859.4877658}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.853319647+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.4150474}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4597273, "arrival": 1711329859.4880204}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4148037, "arrival": 1711329859.1087704}, "models": {"yolo": {"arrival": 1711329857.8462107, "serving": 1711329859.0802536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4599516, "arrival": 1711329859.8299932}, "models": {"yolo": {"arrival": 1711329859.4606347, "serving": 1711329859.8120525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.460315, "arrival": 1711329860.2470245}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4150872, "arrival": 1711329859.4422152}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.41621, "arrival": 1711329860.7128916}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4412374, "arrival": 1711329860.7133732}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.460946, "arrival": 1711329859.936672}, "models": {"yolo": {"arrival": 1711329859.4599843, "serving": 1711329859.9123948}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4609907, "arrival": 1711329859.4894679}, "models": {"yolo": {"arrival": 1711329859.3659596, "serving": 1711329859.4252002}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.621380783+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.4602363}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4145615, "arrival": 1711329859.369918}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4412708, "arrival": 1711329860.2226553}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4592924, "arrival": 1711329859.489931}, "models": {"yolo": {"arrival": 1711329859.3375409, "serving": 1711329859.4435666}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.415603, "arrival": 1711329859.377338}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.853461078+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4460993}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4597511, "arrival": 1711329859.6521792}, "models": {"yolo": {"arrival": 1711329859.200263, "serving": 1711329859.6189127}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.415642, "arrival": 1711329857.8552756}, "models": {"yolo": {"arrival": 1711329857.6099381, "serving": 1711329857.8385766}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4461324, "arrival": 1711329859.2148223}, "models": {"yolo": {"arrival": 1711329859.0893915, "serving": 1711329859.1887019}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.461019, "arrival": 1711329859.8377657}, "models": {"yolo": {"arrival": 1711329859.4606347, "serving": 1711329859.8120525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4148445, "arrival": 1711329858.9847782}, "models": {"yolo": {"arrival": 1711329857.855969, "serving": 1711329858.9489412}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.460479, "arrival": 1711329859.9317973}, "models": {"yolo": {"arrival": 1711329859.4599843, "serving": 1711329859.9123948}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4461818, "arrival": 1711329860.7142134}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4157567, "arrival": 1711329859.1136765}, "models": {"yolo": {"arrival": 1711329857.8462107, "serving": 1711329859.0802536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.445968, "arrival": 1711329859.4662888}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.919555332+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.460852}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.459631, "arrival": 1711329860.7179081}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.459864, "arrival": 1711329860.2463288}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4608033, "arrival": 1711329860.7260544}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4610054, "arrival": 1711329859.6580977}, "models": {"yolo": {"arrival": 1711329859.200263, "serving": 1711329859.6189127}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4591115, "arrival": 1711329859.4873564}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.854228685+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.4603374}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.62125698+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.441216}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.621288171+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4460278}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.853503368+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4463613}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.446198, "arrival": 1711329859.4065425}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4599302, "arrival": 1711329859.65294}, "models": {"yolo": {"arrival": 1711329859.200263, "serving": 1711329859.6189127}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.853574277+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4597013}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4149683, "arrival": 1711329859.3712068}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4153256, "arrival": 1711329857.8544753}, "models": {"yolo": {"arrival": 1711329857.6099381, "serving": 1711329857.8385766}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4608188, "arrival": 1711329859.9358866}, "models": {"yolo": {"arrival": 1711329859.4599843, "serving": 1711329859.9123948}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.621329115+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.4589725}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.621369245+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.459797}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4603825, "arrival": 1711329859.65361}, "models": {"yolo": {"arrival": 1711329859.200263, "serving": 1711329859.6189127}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4462147, "arrival": 1711329860.2241535}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.609241498+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.414885}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4596765, "arrival": 1711329860.2457087}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4590597, "arrival": 1711329860.2442036}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4150076, "arrival": 1711329857.8525305}, "models": {"yolo": {"arrival": 1711329857.6099381, "serving": 1711329857.8385766}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.460359, "arrival": 1711329859.4885027}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.621301896+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.446165}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.416251, "arrival": 1711329859.378408}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.613628643+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4161694}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4152853, "arrival": 1711329859.3765206}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.853372161+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4156804}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.415898, "arrival": 1711329860.710538}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.853532112+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.4590847}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4605024, "arrival": 1711329860.247638}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.62141621+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.460787}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4464116, "arrival": 1711329859.630188}, "models": {"yolo": {"arrival": 1711329859.200263, "serving": 1711329859.6189127}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4599092, "arrival": 1711329859.4882634}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4410505, "arrival": 1711329860.2200103}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.621427321+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.4609144}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.415246, "arrival": 1711329860.7095017}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.621356067+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4596057}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.609280232+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.415205}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4155648, "arrival": 1711329860.7099645}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4160547, "arrival": 1711329859.4429595}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.446264, "arrival": 1711329859.6267745}, "models": {"yolo": {"arrival": 1711329859.200263, "serving": 1711329859.6189127}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4411588, "arrival": 1711329859.465853}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.853394743+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4160166}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4595165, "arrival": 1711329859.6476657}, "models": {"yolo": {"arrival": 1711329859.200263, "serving": 1711329859.6189127}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4154043, "arrival": 1711329859.4424818}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4608347, "arrival": 1711329860.2509515}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4608672, "arrival": 1711329859.489243}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4606864, "arrival": 1711329860.250204}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4602923, "arrival": 1711329859.4907408}, "models": {"yolo": {"arrival": 1711329859.3375409, "serving": 1711329859.4435666}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.853349824+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.4153645}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.461106, "arrival": 1711329859.4897113}, "models": {"yolo": {"arrival": 1711329859.3659596, "serving": 1711329859.4252002}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4610476, "arrival": 1711329860.7268085}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.855964161+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.4607146}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4607503, "arrival": 1711329859.6567621}, "models": {"yolo": {"arrival": 1711329859.200263, "serving": 1711329859.6189127}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.853596251+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.4598868}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4604077, "arrival": 1711329859.8319547}, "models": {"yolo": {"arrival": 1711329859.4606347, "serving": 1711329859.8120525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4605951, "arrival": 1711329859.8329973}, "models": {"yolo": {"arrival": 1711329859.4606347, "serving": 1711329859.8120525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4886906, "arrival": 1711329850.6395402}, "models": {"yolo": {"arrival": 1711329850.4420755, "serving": 1711329850.6088197}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.854269675+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4605243}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4157183, "arrival": 1711329859.4427135}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.446377, "arrival": 1711329859.4670315}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.613598111+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.415524}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:09.621314212+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.446296}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4154449, "arrival": 1711329859.1114373}, "models": {"yolo": {"arrival": 1711329857.8462107, "serving": 1711329859.0802536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4610627, "arrival": 1711329859.9485013}, "models": {"yolo": {"arrival": 1711329859.4599843, "serving": 1711329859.9123948}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4159396, "arrival": 1711329859.3778758}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4160922, "arrival": 1711329859.2112951}, "models": {"yolo": {"arrival": 1711329859.0893915, "serving": 1711329859.1887019}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.853439641+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.445881}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.46093, "arrival": 1711329860.726353}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4460099, "arrival": 1711329859.5044115}, "models": {"yolo": {"arrival": 1711329859.1959193, "serving": 1711329859.4504695}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4605727, "arrival": 1711329859.654224}, "models": {"yolo": {"arrival": 1711329859.200263, "serving": 1711329859.6189127}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.460733, "arrival": 1711329859.4890127}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4463289, "arrival": 1711329859.4079504}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4878201, "arrival": 1711329859.8190575}, "models": {"yolo": {"arrival": 1711329859.6300786, "serving": 1711329859.8054404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4593444, "arrival": 1711329860.245044}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4597745, "arrival": 1711329859.5061088}, "models": {"yolo": {"arrival": 1711329859.1959193, "serving": 1711329859.4504695}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4157946, "arrival": 1711329859.209089}, "models": {"yolo": {"arrival": 1711329858.964451, "serving": 1711329859.182373}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.415979, "arrival": 1711329857.8557687}, "models": {"yolo": {"arrival": 1711329857.6099381, "serving": 1711329857.8385766}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4161308, "arrival": 1711329859.209954}, "models": {"yolo": {"arrival": 1711329858.964451, "serving": 1711329859.182373}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.613615561+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4158325}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.446149, "arrival": 1711329859.504852}, "models": {"yolo": {"arrival": 1711329859.1959193, "serving": 1711329859.4504695}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.853417453+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4411361}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.446247, "arrival": 1711329859.4668012}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4590333, "arrival": 1711329859.4085672}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4606414, "arrival": 1711329860.7257335}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.446048, "arrival": 1711329860.7138102}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4591684, "arrival": 1711329859.505592}, "models": {"yolo": {"arrival": 1711329859.1959193, "serving": 1711329859.4504695}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4609609, "arrival": 1711329860.254177}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4605489, "arrival": 1711329859.4887483}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4610336, "arrival": 1711329850.6322699}, "models": {"yolo": {"arrival": 1711329850.4420755, "serving": 1711329850.6088197}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.919664757+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4610915}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.460899, "arrival": 1711329859.8371313}, "models": {"yolo": {"arrival": 1711329859.4606347, "serving": 1711329859.8120525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4463463, "arrival": 1711329860.2388856}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.621403842+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.460618}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.441198, "arrival": 1711329859.210658}, "models": {"yolo": {"arrival": 1711329858.964451, "serving": 1711329859.182373}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4595592, "arrival": 1711329859.5058765}, "models": {"yolo": {"arrival": 1711329859.1959193, "serving": 1711329859.4504695}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4607658, "arrival": 1711329859.836405}, "models": {"yolo": {"arrival": 1711329859.4606347, "serving": 1711329859.8120525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4460826, "arrival": 1711329860.2234864}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.85348251+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.446231}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4606643, "arrival": 1711329859.934731}, "models": {"yolo": {"arrival": 1711329859.4599843, "serving": 1711329859.9123948}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 23.64 GiB total capacity; 1.90 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.853553361+00:00\"}\"\n>", "times": {"request": {"sending": 1711329849.4593954}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.621343482+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4592078}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:09.621392628+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 48.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329849.4604335}}, "outputs": []}, {"times": {"request": {"sending": 1711329849.4598424, "arrival": 1711329859.4903564}, "models": {"yolo": {"arrival": 1711329859.3375409, "serving": 1711329859.4435666}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4610767, "arrival": 1711329860.2546892}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.460456, "arrival": 1711329860.7252693}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4461155, "arrival": 1711329859.4665604}, "models": {"yolo": {"arrival": 1711329855.8680532, "serving": 1711329859.3258786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4608824, "arrival": 1711329859.6576872}, "models": {"yolo": {"arrival": 1711329859.200263, "serving": 1711329859.6189127}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4596543, "arrival": 1711329859.4901502}, "models": {"yolo": {"arrival": 1711329859.3375409, "serving": 1711329859.4435666}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4459898, "arrival": 1711329859.2142227}, "models": {"yolo": {"arrival": 1711329859.0893915, "serving": 1711329859.1887019}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4411798, "arrival": 1711329859.2135248}, "models": {"yolo": {"arrival": 1711329859.0893915, "serving": 1711329859.1887019}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4590075, "arrival": 1711329860.717234}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4592538, "arrival": 1711329860.7175858}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329849.4460642, "arrival": 1711329859.4011679}, "models": {"yolo": {"arrival": 1711329857.8662708, "serving": 1711329859.3220959}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329850.4491718, "arrival": 1711329860.7066495}, "models": {"yolo": {"arrival": 1711329859.9268234, "serving": 1711329860.6732578}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.44897, "arrival": 1711329860.1639056}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.978681315+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4495819}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4381983, "arrival": 1711329860.7018266}, "models": {"yolo": {"arrival": 1711329859.9268234, "serving": 1711329860.6732578}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.44941, "arrival": 1711329860.739239}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4485965, "arrival": 1711329862.1473842}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4118197, "arrival": 1711329859.9525394}, "models": {"yolo": {"arrival": 1711329859.4599843, "serving": 1711329859.9123948}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4381218, "arrival": 1711329860.160507}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4120917, "arrival": 1711329860.728317}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4124413, "arrival": 1711329859.9538143}, "models": {"yolo": {"arrival": 1711329859.4599843, "serving": 1711329859.9123948}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4116566, "arrival": 1711329859.8221688}, "models": {"yolo": {"arrival": 1711329859.6300786, "serving": 1711329859.8054404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4118583, "arrival": 1711329860.2557082}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.91974253+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.412207}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4494753, "arrival": 1711329860.1680346}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4113688, "arrival": 1711329860.7271}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.919769272+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4125187}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.98574082+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.449945}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4121299, "arrival": 1711329859.953185}, "models": {"yolo": {"arrival": 1711329859.4599843, "serving": 1711329859.9123948}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4489872, "arrival": 1711329860.8494012}, "models": {"yolo": {"arrival": 1711329859.94932, "serving": 1711329860.8164885}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4122462, "arrival": 1711329860.1398013}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4495518, "arrival": 1711329860.8214517}, "models": {"yolo": {"arrival": 1711329860.683583, "serving": 1711329860.8056471}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4491124, "arrival": 1711329860.8498168}, "models": {"yolo": {"arrival": 1711329859.94932, "serving": 1711329860.8164885}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.919697126+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.4115758}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4486992, "arrival": 1711329860.1633518}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4499147, "arrival": 1711329863.3024588}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4115324, "arrival": 1711329860.2551196}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4114785, "arrival": 1711329859.9516692}, "models": {"yolo": {"arrival": 1711329859.4599843, "serving": 1711329859.9123948}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4116971, "arrival": 1711329862.1257272}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4499297, "arrival": 1711329860.2729495}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4126332, "arrival": 1711329862.1442432}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4496276, "arrival": 1711329862.1560252}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4490972, "arrival": 1711329860.1643882}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.978423909+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4488187}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4496722, "arrival": 1711329863.3015244}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4498992, "arrival": 1711329860.7435715}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4377944, "arrival": 1711329860.2607977}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4498699, "arrival": 1711329862.1566777}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.448578, "arrival": 1711329859.9644706}, "models": {"yolo": {"arrival": 1711329859.814226, "serving": 1711329859.9363518}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.448632, "arrival": 1711329860.7347379}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4493148, "arrival": 1711329860.2655334}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4324486, "arrival": 1711329860.1551285}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4117792, "arrival": 1711329860.7273974}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4119747, "arrival": 1711329859.8241956}, "models": {"yolo": {"arrival": 1711329859.6300786, "serving": 1711329859.8054404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4376194, "arrival": 1711329860.7308347}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4495976, "arrival": 1711329860.1685963}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4499598, "arrival": 1711329860.579999}, "models": {"yolo": {"arrival": 1711329860.137551, "serving": 1711329860.5681195}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.571496436+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.438169}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4116166, "arrival": 1711329860.1380806}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4491267, "arrival": 1711329862.1530185}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.571632144+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.4491422}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4124033, "arrival": 1711329860.7286406}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4496562, "arrival": 1711329860.7397592}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.437579, "arrival": 1711329862.1452777}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.571596014+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.4490187}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4495368, "arrival": 1711329860.739514}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.574835842+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.4496422}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4323292, "arrival": 1711329860.2595375}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.924230556+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.448683}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.919719977+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4118981}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4492207, "arrival": 1711329860.165685}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.571464851+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4378998}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.985691197+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4498243}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.571529474+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4486144}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4117365, "arrival": 1711329850.640478}, "models": {"yolo": {"arrival": 1711329850.4420755, "serving": 1711329850.6088197}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4492986, "arrival": 1711329860.8184545}, "models": {"yolo": {"arrival": 1711329860.683583, "serving": 1711329860.8056471}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4492517, "arrival": 1711329862.1533327}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4493601, "arrival": 1711329860.851655}, "models": {"yolo": {"arrival": 1711329859.94932, "serving": 1711329860.8164885}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4377124, "arrival": 1711329859.9617627}, "models": {"yolo": {"arrival": 1711329859.814226, "serving": 1711329859.9363518}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4119363, "arrival": 1711329860.139083}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4492836, "arrival": 1711329860.737783}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.919866714+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4376686}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4492357, "arrival": 1711329860.8511798}, "models": {"yolo": {"arrival": 1711329859.94932, "serving": 1711329860.8164885}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.432537, "arrival": 1711329860.7305393}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4497347, "arrival": 1711329860.8546345}, "models": {"yolo": {"arrival": 1711329859.94932, "serving": 1711329860.8164885}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4376853, "arrival": 1711329860.1580987}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.571560763+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4487512}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4491568, "arrival": 1711329860.7374277}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4487164, "arrival": 1711329859.9648857}, "models": {"yolo": {"arrival": 1711329859.814226, "serving": 1711329859.9363518}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4488, "arrival": 1711329860.2631605}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4497938, "arrival": 1711329863.3020039}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4801285, "arrival": 1711329860.855455}, "models": {"yolo": {"arrival": 1711329859.94932, "serving": 1711329860.8164885}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4375036, "arrival": 1711329859.9613593}, "models": {"yolo": {"arrival": 1711329859.814226, "serving": 1711329859.9363518}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.574851574+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4497643}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.571217258+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.412363}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4487343, "arrival": 1711329862.1478636}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4376364, "arrival": 1711329859.9575648}, "models": {"yolo": {"arrival": 1711329859.4599843, "serving": 1711329859.9123948}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4378836, "arrival": 1711329862.1462953}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4326165, "arrival": 1711329860.1573653}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4490674, "arrival": 1711329860.264927}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4121678, "arrival": 1711329860.2561138}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.571396331+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.4375992}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.978599176+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.449331}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.448767, "arrival": 1711329860.7350328}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4486651, "arrival": 1711329860.262679}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.412013, "arrival": 1711329862.1270494}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4494448, "arrival": 1711329860.2658322}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.449809, "arrival": 1711329860.2726438}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4377632, "arrival": 1711329860.7311277}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4120517, "arrival": 1711329850.6412063}, "models": {"yolo": {"arrival": 1711329850.4420755, "serving": 1711329850.6088197}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.412479, "arrival": 1711329860.259039}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4123244, "arrival": 1711329862.1434941}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4496868, "arrival": 1711329860.2722116}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.571707039+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.449394}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4381545, "arrival": 1711329862.1468158}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4490519, "arrival": 1711329860.7058113}, "models": {"yolo": {"arrival": 1711329859.9268234, "serving": 1711329860.6732578}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4376519, "arrival": 1711329860.2603807}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4495053, "arrival": 1711329862.1539502}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.449749, "arrival": 1711329862.1563742}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4378417, "arrival": 1711329860.1587496}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4496124, "arrival": 1711329860.8541908}, "models": {"yolo": {"arrival": 1711329859.94932, "serving": 1711329860.8164885}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.571305703+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4126718}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4380744, "arrival": 1711329860.2611835}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.919909554+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.4381056}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.919815628+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4324217}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4325767, "arrival": 1711329860.2599618}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4324722, "arrival": 1711329859.9609382}, "models": {"yolo": {"arrival": 1711329859.814226, "serving": 1711329859.9363518}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.978515938+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.4490826}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4497197, "arrival": 1711329860.169145}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4382129, "arrival": 1711329860.261564}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4122853, "arrival": 1711329859.8251095}, "models": {"yolo": {"arrival": 1711329859.6300786, "serving": 1711329859.8054404}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4125562, "arrival": 1711329860.1542184}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.571431985+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4377444}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.448649, "arrival": 1711329860.7025137}, "models": {"yolo": {"arrival": 1711329859.9268234, "serving": 1711329860.6732578}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.571350363+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.4325142}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4487836, "arrival": 1711329860.7050004}, "models": {"yolo": {"arrival": 1711329859.9268234, "serving": 1711329860.6732578}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4377787, "arrival": 1711329860.697475}, "models": {"yolo": {"arrival": 1711329859.9268234, "serving": 1711329860.6732578}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4490333, "arrival": 1711329860.735322}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4497788, "arrival": 1711329860.7432535}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4494298, "arrival": 1711329860.8205893}, "models": {"yolo": {"arrival": 1711329860.683583, "serving": 1711329860.8056471}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.57479528+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.4495194}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4381392, "arrival": 1711329859.9640586}, "models": {"yolo": {"arrival": 1711329859.814226, "serving": 1711329859.9363518}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4127104, "arrival": 1711329860.7301257}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.978637136+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.44946}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4802136, "arrival": 1711329862.1570046}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4490027, "arrival": 1711329862.1526566}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4495664, "arrival": 1711329860.2660997}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.438184, "arrival": 1711329860.7344224}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4125943, "arrival": 1711329859.960316}, "models": {"yolo": {"arrival": 1711329859.814226, "serving": 1711329859.9363518}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4377282, "arrival": 1711329862.1458137}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4380586, "arrival": 1711329860.7009711}, "models": {"yolo": {"arrival": 1711329859.9268234, "serving": 1711329860.6732578}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.57166861+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.449267}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4498384, "arrival": 1711329860.5774572}, "models": {"yolo": {"arrival": 1711329860.137551, "serving": 1711329860.5681195}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4324937, "arrival": 1711329862.1447852}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4491873, "arrival": 1711329860.2652543}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4127486, "arrival": 1711329859.9559076}, "models": {"yolo": {"arrival": 1711329859.4599843, "serving": 1711329859.9123948}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4485545, "arrival": 1711329860.161219}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.574879098+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.4802413}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.978558467+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4492054}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.924183626+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.448472}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.985617263+00:00\"}\"\n>", "times": {"request": {"sending": 1711329850.4497032}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4494908, "arrival": 1711329860.853718}, "models": {"yolo": {"arrival": 1711329859.94932, "serving": 1711329860.8164885}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.91988746+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4378262}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4325576, "arrival": 1711329859.9566553}, "models": {"yolo": {"arrival": 1711329859.4599843, "serving": 1711329859.9123948}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4493454, "arrival": 1711329860.16625}, "models": {"yolo": {"arrival": 1711329859.4393957, "serving": 1711329860.123884}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.574865064+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 100.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4498844}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4498558, "arrival": 1711329860.8550506}, "models": {"yolo": {"arrival": 1711329859.94932, "serving": 1711329860.8164885}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4493783, "arrival": 1711329862.153651}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329850.4378688, "arrival": 1711329859.963564}, "models": {"yolo": {"arrival": 1711329859.814226, "serving": 1711329859.9363518}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.919845033+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329850.4325964}}, "outputs": []}, {"times": {"request": {"sending": 1711329850.4379144, "arrival": 1711329860.7340267}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329851.4153428, "arrival": 1711329860.582149}, "models": {"yolo": {"arrival": 1711329860.137551, "serving": 1711329860.5681195}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.73641668+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.55232}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5522263, "arrival": 1711329863.3172462}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.414739, "arrival": 1711329860.7448235}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5715432, "arrival": 1711329861.9643652}, "models": {"yolo": {"arrival": 1711329861.2681212, "serving": 1711329861.9370787}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5522768, "arrival": 1711329860.8246858}, "models": {"yolo": {"arrival": 1711329860.5740578, "serving": 1711329860.8119712}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5749595, "arrival": 1711329863.3298235}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.4152725, "arrival": 1711329860.273682}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.985788322+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.4149904}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5523615, "arrival": 1711329860.2742424}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.985834269+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.4153183}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.4151123, "arrival": 1711329862.1582816}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.4150727, "arrival": 1711329860.85712}, "models": {"yolo": {"arrival": 1711329859.94932, "serving": 1711329860.8164885}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5524423, "arrival": 1711329860.7484012}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.574737, "arrival": 1711329863.6737406}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5737848, "arrival": 1711329860.2772584}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5522912, "arrival": 1711329861.2762744}, "models": {"yolo": {"arrival": 1711329860.8299344, "serving": 1711329861.260529}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.414945, "arrival": 1711329860.2733698}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5526304, "arrival": 1711329860.8370898}, "models": {"yolo": {"arrival": 1711329860.5740578, "serving": 1711329860.8119712}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.414866, "arrival": 1711329863.3034108}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5746486, "arrival": 1711329862.1681948}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5523481, "arrival": 1711329863.3176522}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5523062, "arrival": 1711329862.1591206}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.4152324, "arrival": 1711329863.3167489}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5744689, "arrival": 1711329860.6270354}, "models": {"yolo": {"arrival": 1711329860.234736, "serving": 1711329860.6083987}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5524018, "arrival": 1711329861.2790754}, "models": {"yolo": {"arrival": 1711329860.8299344, "serving": 1711329861.260529}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.4150324, "arrival": 1711329860.5808837}, "models": {"yolo": {"arrival": 1711329860.137551, "serving": 1711329860.5681195}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.989017348+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.573913}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.04550148+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.575213}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.4151926, "arrival": 1711329860.7451081}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5521588, "arrival": 1711329860.745337}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.4154255, "arrival": 1711329862.1587179}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5523348, "arrival": 1711329860.7481077}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.736471205+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.552428}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5525239, "arrival": 1711329862.160395}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.574709, "arrival": 1711329860.8408477}, "models": {"yolo": {"arrival": 1711329860.6194963, "serving": 1711329860.8151946}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.574589, "arrival": 1711329860.627687}, "models": {"yolo": {"arrival": 1711329860.234736, "serving": 1711329860.6083987}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.731571936+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.4151514}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.985924233+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.5523748}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.731634392+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.4154782}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5745738, "arrival": 1711329863.3257275}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5743535, "arrival": 1711329863.672619}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.736497657+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.5525372}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.985881604+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.5522614}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5739524, "arrival": 1711329861.9666495}, "models": {"yolo": {"arrival": 1711329861.2681212, "serving": 1711329861.9370787}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5523887, "arrival": 1711329860.8255224}, "models": {"yolo": {"arrival": 1711329860.5740578, "serving": 1711329860.8119712}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5714598, "arrival": 1711329863.318836}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5754604, "arrival": 1711329863.3320825}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5755112, "arrival": 1711329863.6776612}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5525887, "arrival": 1711329863.3184707}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5522463, "arrival": 1711329860.273975}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5741792, "arrival": 1711329863.3210745}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.4153829, "arrival": 1711329860.857613}, "models": {"yolo": {"arrival": 1711329859.94932, "serving": 1711329860.8164885}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5752678, "arrival": 1711329862.1714494}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.73656531+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.5737314}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5740418, "arrival": 1711329863.3206544}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5526037, "arrival": 1711329860.275417}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5742579, "arrival": 1711329862.1667213}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.736521463+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.5713441}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5524967, "arrival": 1711329860.826848}, "models": {"yolo": {"arrival": 1711329860.5740578, "serving": 1711329860.8119712}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5746322, "arrival": 1711329862.148338}, "models": {"yolo": {"arrival": 1711329861.9467561, "serving": 1711329862.119013}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5746927, "arrival": 1711329863.3264377}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.574559, "arrival": 1711329860.843556}, "models": {"yolo": {"arrival": 1711329860.714211, "serving": 1711329860.8136806}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5526438, "arrival": 1711329861.280546}, "models": {"yolo": {"arrival": 1711329860.8299344, "serving": 1711329861.260529}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.740416139+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.5747848}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.574289, "arrival": 1711329860.7528114}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5755484, "arrival": 1711329862.172776}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:12.498126948+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.5755663}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5750477, "arrival": 1711329862.171223}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5752487, "arrival": 1711329862.1542454}, "models": {"yolo": {"arrival": 1711329861.9467561, "serving": 1711329862.119013}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5525105, "arrival": 1711329861.2798843}, "models": {"yolo": {"arrival": 1711329860.8299344, "serving": 1711329861.260529}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5745275, "arrival": 1711329862.1673274}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5741255, "arrival": 1711329862.1631372}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5753217, "arrival": 1711329863.3311675}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.575529, "arrival": 1711329862.484057}, "models": {"yolo": {"arrival": 1711329862.129663, "serving": 1711329862.4699667}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5754433, "arrival": 1711329861.9631243}, "models": {"yolo": {"arrival": 1711329860.8274128, "serving": 1711329861.9358768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5743685, "arrival": 1711329861.9778097}, "models": {"yolo": {"arrival": 1711329861.2681212, "serving": 1711329861.9370787}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.05284457+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.5754929}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5742223, "arrival": 1711329863.6660247}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5748892, "arrival": 1711329862.1502566}, "models": {"yolo": {"arrival": 1711329861.9467561, "serving": 1711329862.119013}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5746772, "arrival": 1711329860.845822}, "models": {"yolo": {"arrival": 1711329860.714211, "serving": 1711329860.8136806}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.552415, "arrival": 1711329862.1600728}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5715237, "arrival": 1711329860.8378496}, "models": {"yolo": {"arrival": 1711329860.5740578, "serving": 1711329860.8119712}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5524554, "arrival": 1711329863.3180282}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5736704, "arrival": 1711329863.6642544}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5750282, "arrival": 1711329862.1506987}, "models": {"yolo": {"arrival": 1711329861.9467561, "serving": 1711329862.119013}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.740372777+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.5745423}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5524688, "arrival": 1711329860.275094}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5748718, "arrival": 1711329863.6750467}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.575389, "arrival": 1711329862.4836116}, "models": {"yolo": {"arrival": 1711329862.129663, "serving": 1711329862.4699667}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5743241, "arrival": 1711329860.6253023}, "models": {"yolo": {"arrival": 1711329860.234736, "serving": 1711329860.6083987}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5748377, "arrival": 1711329860.8416288}, "models": {"yolo": {"arrival": 1711329860.6194963, "serving": 1711329860.8151946}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.573933, "arrival": 1711329863.664855}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5737686, "arrival": 1711329863.3195438}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.574977, "arrival": 1711329860.8422692}, "models": {"yolo": {"arrival": 1711329860.6194963, "serving": 1711329860.8151946}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5750823, "arrival": 1711329861.958217}, "models": {"yolo": {"arrival": 1711329860.8274128, "serving": 1711329861.9358768}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.740326834+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.574273}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.575337, "arrival": 1711329860.8489249}, "models": {"yolo": {"arrival": 1711329860.6194963, "serving": 1711329860.8151946}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.05278914+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.575356}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5744517, "arrival": 1711329863.323367}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.986051858+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.571506}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5736363, "arrival": 1711329860.276989}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5753016, "arrival": 1711329861.9622838}, "models": {"yolo": {"arrival": 1711329860.8274128, "serving": 1711329861.9358768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5737495, "arrival": 1711329860.751452}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5736156, "arrival": 1711329863.3191912}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.57501, "arrival": 1711329863.6753705}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5526571, "arrival": 1711329862.1616583}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.985966819+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.552483}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.498039769+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.575284}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.740394101+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.5746627}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.989178039+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.5743387}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5714366, "arrival": 1711329860.7488942}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.740349336+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.5744097}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.57443, "arrival": 1711329860.7533307}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5754766, "arrival": 1711329861.7628758}, "models": {"yolo": {"arrival": 1711329860.8271089, "serving": 1711329861.746263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.575374, "arrival": 1711329863.6773474}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5714788, "arrival": 1711329860.2766628}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5751953, "arrival": 1711329860.8429317}, "models": {"yolo": {"arrival": 1711329860.6194963, "serving": 1711329860.8151946}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.988947488+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.573655}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5749412, "arrival": 1711329860.8472104}, "models": {"yolo": {"arrival": 1711329860.714211, "serving": 1711329860.8136806}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.992463415+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.5746036}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.574024, "arrival": 1711329860.751738}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5748193, "arrival": 1711329863.3271375}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5741642, "arrival": 1711329860.7525408}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5747705, "arrival": 1711329862.1684532}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5740592, "arrival": 1711329860.2775118}, "models": {"yolo": {"arrival": 1711329857.8976293, "serving": 1711329860.1958106}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.73654286+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.5735178}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.989490437+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.5744836}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:11.740245464+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.573999}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5748026, "arrival": 1711329860.8465922}, "models": {"yolo": {"arrival": 1711329860.714211, "serving": 1711329860.8136806}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.045465414+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.5749922}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5743096, "arrival": 1711329863.3225806}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.57398, "arrival": 1711329862.162888}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5751743, "arrival": 1711329863.3305392}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.045380099+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.5748546}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.98600962+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.552617}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:12.498080459+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.575426}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5746179, "arrival": 1711329863.6733823}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.989127596+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.5742078}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 54.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:11.740299506+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.5741467}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5736864, "arrival": 1711329861.9650068}, "models": {"yolo": {"arrival": 1711329861.2681212, "serving": 1711329861.9370787}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5741935, "arrival": 1711329860.6240854}, "models": {"yolo": {"arrival": 1711329860.234736, "serving": 1711329860.6083987}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.997400044+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.574723}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5754087, "arrival": 1711329862.1717005}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5715663, "arrival": 1711329862.1619072}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5525556, "arrival": 1711329860.7486503}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.573592, "arrival": 1711329860.751169}, "models": {"yolo": {"arrival": 1711329857.8507562, "serving": 1711329860.6826599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.574091, "arrival": 1711329863.6654458}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5752304, "arrival": 1711329863.6756406}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5744975, "arrival": 1711329863.6730015}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:12.497876255+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.5749261}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5741045, "arrival": 1711329861.9673457}, "models": {"yolo": {"arrival": 1711329861.2681212, "serving": 1711329861.9370787}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.497996812+00:00\"}\"\n>", "times": {"request": {"sending": 1711329851.5750644}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.574511, "arrival": 1711329861.9782605}, "models": {"yolo": {"arrival": 1711329861.2681212, "serving": 1711329861.9370787}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5743954, "arrival": 1711329862.1670625}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.573712, "arrival": 1711329862.1621258}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.5749087, "arrival": 1711329862.170931}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.989077131+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 390.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329851.5740767}}, "outputs": []}, {"times": {"request": {"sending": 1711329851.5747542, "arrival": 1711329862.148793}, "models": {"yolo": {"arrival": 1711329861.9467561, "serving": 1711329862.119013}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329851.574237, "arrival": 1711329861.9769337}, "models": {"yolo": {"arrival": 1711329861.2681212, "serving": 1711329861.9370787}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329852.4159439, "arrival": 1711329862.494234}, "models": {"yolo": {"arrival": 1711329862.129663, "serving": 1711329862.4699667}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4758077, "arrival": 1711329863.3519309}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4428172, "arrival": 1711329862.6862068}, "models": {"yolo": {"arrival": 1711329862.3079097, "serving": 1711329862.653976}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.356751698+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4426706}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4156995, "arrival": 1711329863.6779292}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4356642, "arrival": 1711329861.9838731}, "models": {"yolo": {"arrival": 1711329860.8274128, "serving": 1711329861.9358768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4159687, "arrival": 1711329862.1740227}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4427202, "arrival": 1711329862.5049162}, "models": {"yolo": {"arrival": 1711329862.110524, "serving": 1711329862.48508}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.052919153+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4160957}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4154227, "arrival": 1711329861.9637494}, "models": {"yolo": {"arrival": 1711329860.8274128, "serving": 1711329861.9358768}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.357010194+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4768972}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4160223, "arrival": 1711329861.9760108}, "models": {"yolo": {"arrival": 1711329860.8274128, "serving": 1711329861.9358768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4157255, "arrival": 1711329862.4843216}, "models": {"yolo": {"arrival": 1711329862.129663, "serving": 1711329862.4699667}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.356988773+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4437149}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4161463, "arrival": 1711329862.4947581}, "models": {"yolo": {"arrival": 1711329862.129663, "serving": 1711329862.4699667}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4156396, "arrival": 1711329861.7657683}, "models": {"yolo": {"arrival": 1711329860.8271089, "serving": 1711329861.746263}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.976006512+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4435987}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.976028638+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4437594}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4355328, "arrival": 1711329861.9834566}, "models": {"yolo": {"arrival": 1711329860.8274128, "serving": 1711329861.9358768}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.052872464+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4156725}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.432794, "arrival": 1711329863.6800625}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4434311, "arrival": 1711329863.6940613}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4354599, "arrival": 1711329863.6803348}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4426868, "arrival": 1711329862.6854916}, "models": {"yolo": {"arrival": 1711329862.3079097, "serving": 1711329862.653976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4158134, "arrival": 1711329861.9753945}, "models": {"yolo": {"arrival": 1711329860.8274128, "serving": 1711329861.9358768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4156039, "arrival": 1711329863.3360527}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4426215, "arrival": 1711329862.2923625}, "models": {"yolo": {"arrival": 1711329861.9564488, "serving": 1711329862.2768824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4356127, "arrival": 1711329862.6894917}, "models": {"yolo": {"arrival": 1711329862.4789062, "serving": 1711329862.6732218}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:12.970281912+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4355135}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4424775, "arrival": 1711329862.693421}, "models": {"yolo": {"arrival": 1711329862.4789062, "serving": 1711329862.6732218}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.118029289+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.442638}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.96690995+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.416393}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.970423123+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4420526}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.498228296+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4159963}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4328158, "arrival": 1711329862.49541}, "models": {"yolo": {"arrival": 1711329862.129663, "serving": 1711329862.4699667}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.130535846+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4768338}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4436314, "arrival": 1711329863.349935}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.130358124+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4434156}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4433997, "arrival": 1711329862.2999234}, "models": {"yolo": {"arrival": 1711329861.9564488, "serving": 1711329862.2768824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.435335, "arrival": 1711329861.9829621}, "models": {"yolo": {"arrival": 1711329860.8274128, "serving": 1711329861.9358768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4357274, "arrival": 1711329863.6825047}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.052896672+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4158914}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4421616, "arrival": 1711329862.691645}, "models": {"yolo": {"arrival": 1711329862.4789062, "serving": 1711329862.6732218}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.356897657+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4430711}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4436982, "arrival": 1711329863.6946306}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4157546, "arrival": 1711329862.17303}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.118188321+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4429028}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4158664, "arrival": 1711329861.7665465}, "models": {"yolo": {"arrival": 1711329860.8271089, "serving": 1711329861.746263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4420729, "arrival": 1711329862.130562}, "models": {"yolo": {"arrival": 1711329861.9458888, "serving": 1711329862.1005814}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4420316, "arrival": 1711329862.676539}, "models": {"yolo": {"arrival": 1711329862.3079097, "serving": 1711329862.653976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4159157, "arrival": 1711329863.6781979}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.416121, "arrival": 1711329863.6784785}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4428523, "arrival": 1711329862.5053122}, "models": {"yolo": {"arrival": 1711329862.110524, "serving": 1711329862.48508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4160712, "arrival": 1711329861.7672062}, "models": {"yolo": {"arrival": 1711329860.8271089, "serving": 1711329861.746263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.442736, "arrival": 1711329863.346175}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4436147, "arrival": 1711329862.5234432}, "models": {"yolo": {"arrival": 1711329862.110524, "serving": 1711329862.48508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4158385, "arrival": 1711329863.336747}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.498173518+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4157825}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:12.97588794+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4428332}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4163694, "arrival": 1711329862.1745234}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4428704, "arrival": 1711329863.3465698}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4421778, "arrival": 1711329862.678131}, "models": {"yolo": {"arrival": 1711329862.3079097, "serving": 1711329862.653976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4429202, "arrival": 1711329863.6854482}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.356874208+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4429379}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.130198274+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4430377}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4422796, "arrival": 1711329863.6830766}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.443183, "arrival": 1711329863.6937}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4161708, "arrival": 1711329862.174266}, "models": {"yolo": {"arrival": 1711329859.8695743, "serving": 1711329862.093879}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4163206, "arrival": 1711329863.679752}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4421391, "arrival": 1711329863.6827888}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4354005, "arrival": 1711329863.3417954}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.052941881+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4162962}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4420907, "arrival": 1711329863.343092}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.44302, "arrival": 1711329862.2965138}, "models": {"yolo": {"arrival": 1711329861.9564488, "serving": 1711329862.2768824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.416047, "arrival": 1711329863.3376381}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4428864, "arrival": 1711329862.2958193}, "models": {"yolo": {"arrival": 1711329861.9564488, "serving": 1711329862.2768824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.442988, "arrival": 1711329862.505679}, "models": {"yolo": {"arrival": 1711329862.110524, "serving": 1711329862.48508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4426033, "arrival": 1711329863.34577}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4354224, "arrival": 1711329861.7727606}, "models": {"yolo": {"arrival": 1711329860.8271089, "serving": 1711329861.746263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4427853, "arrival": 1711329863.685246}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.054906724+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4421241}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.054956556+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4424162}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4327161, "arrival": 1711329863.3389752}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.442447, "arrival": 1711329863.6848018}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4355977, "arrival": 1711329863.6821456}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:12.975962553+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.443235}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.356967066+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.443566}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4422135, "arrival": 1711329862.131139}, "models": {"yolo": {"arrival": 1711329861.9458888, "serving": 1711329862.1005814}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4434917, "arrival": 1711329862.5213768}, "models": {"yolo": {"arrival": 1711329862.110524, "serving": 1711329862.48508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4422467, "arrival": 1711329861.9807763}, "models": {"yolo": {"arrival": 1711329861.7615194, "serving": 1711329861.945611}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:12.970505277+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4423277}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.053009034+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4355814}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:12.966805902+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.416196}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.975833369+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4427023}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.356831027+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4428017}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4425092, "arrival": 1711329862.681628}, "models": {"yolo": {"arrival": 1711329862.3079097, "serving": 1711329862.653976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.443775, "arrival": 1711329862.523845}, "models": {"yolo": {"arrival": 1711329862.110524, "serving": 1711329862.48508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4162228, "arrival": 1711329861.9764817}, "models": {"yolo": {"arrival": 1711329860.8274128, "serving": 1711329861.9358768}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.052964147+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4327695}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4356964, "arrival": 1711329861.978691}, "models": {"yolo": {"arrival": 1711329861.7615194, "serving": 1711329861.945611}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4327445, "arrival": 1711329861.7720742}, "models": {"yolo": {"arrival": 1711329860.8271089, "serving": 1711329861.746263}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.13029687+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4431677}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4419541, "arrival": 1711329862.6903868}, "models": {"yolo": {"arrival": 1711329862.4789062, "serving": 1711329862.6732218}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.435477, "arrival": 1711329862.496206}, "models": {"yolo": {"arrival": 1711329862.129663, "serving": 1711329862.4699667}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4328375, "arrival": 1711329862.3060896}, "models": {"yolo": {"arrival": 1711329862.1333592, "serving": 1711329862.2945983}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4354925, "arrival": 1711329862.3068373}, "models": {"yolo": {"arrival": 1711329862.1333592, "serving": 1711329862.2945983}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.118132861+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4427702}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4435215, "arrival": 1711329862.3005748}, "models": {"yolo": {"arrival": 1711329861.9564488, "serving": 1711329862.2768824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4422305, "arrival": 1711329863.343488}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4425714, "arrival": 1711329862.5043323}, "models": {"yolo": {"arrival": 1711329862.110524, "serving": 1711329862.48508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.432621, "arrival": 1711329861.9773777}, "models": {"yolo": {"arrival": 1711329860.8274128, "serving": 1711329861.9358768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4430032, "arrival": 1711329863.3470037}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.054851705+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4357123}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4423811, "arrival": 1711329861.981406}, "models": {"yolo": {"arrival": 1711329861.7615194, "serving": 1711329861.945611}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.356919441+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4431992}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.443736, "arrival": 1711329862.9981537}, "models": {"yolo": {"arrival": 1711329862.665791, "serving": 1711329862.972925}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4432168, "arrival": 1711329862.687572}, "models": {"yolo": {"arrival": 1711329862.3079097, "serving": 1711329862.653976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4421084, "arrival": 1711329861.980267}, "models": {"yolo": {"arrival": 1711329861.7615194, "serving": 1711329861.945611}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4436564, "arrival": 1711329862.3012154}, "models": {"yolo": {"arrival": 1711329861.9564488, "serving": 1711329862.2768824}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.052987453+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4354415}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4162471, "arrival": 1711329863.3383055}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4768796, "arrival": 1711329863.6948943}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4430556, "arrival": 1711329863.6856444}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4431198, "arrival": 1711329862.5200925}, "models": {"yolo": {"arrival": 1711329862.110524, "serving": 1711329862.48508}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.97034864+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4356463}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.970544211+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4425397}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:12.966956492+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4328587}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.970464505+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4421935}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4162717, "arrival": 1711329861.7696514}, "models": {"yolo": {"arrival": 1711329860.8271089, "serving": 1711329861.746263}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.130490789+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4436822}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4423466, "arrival": 1711329862.1316805}, "models": {"yolo": {"arrival": 1711329861.9458888, "serving": 1711329862.1005814}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4356294, "arrival": 1711329862.6756382}, "models": {"yolo": {"arrival": 1711329862.3079097, "serving": 1711329862.653976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4423642, "arrival": 1711329863.3453205}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4163446, "arrival": 1711329862.4950778}, "models": {"yolo": {"arrival": 1711329862.129663, "serving": 1711329862.4699667}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.443462, "arrival": 1711329862.9945927}, "models": {"yolo": {"arrival": 1711329862.665791, "serving": 1711329862.972925}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.443551, "arrival": 1711329863.6943412}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.442754, "arrival": 1711329862.294948}, "models": {"yolo": {"arrival": 1711329861.9564488, "serving": 1711329862.2768824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4356806, "arrival": 1711329863.34269}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4798784, "arrival": 1711329862.9994688}, "models": {"yolo": {"arrival": 1711329862.665791, "serving": 1711329862.972925}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.130445062+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4435368}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.443151, "arrival": 1711329862.2991395}, "models": {"yolo": {"arrival": 1711329861.9564488, "serving": 1711329862.2768824}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.443583, "arrival": 1711329862.9972305}, "models": {"yolo": {"arrival": 1711329862.665791, "serving": 1711329862.972925}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4433818, "arrival": 1711329863.3491085}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:12.975985021+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4434767}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:12.975916777+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4429724}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4430861, "arrival": 1711329862.6871212}, "models": {"yolo": {"arrival": 1711329862.3079097, "serving": 1711329862.653976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4423125, "arrival": 1711329862.6804075}, "models": {"yolo": {"arrival": 1711329862.3079097, "serving": 1711329862.653976}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.356943731+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4434466}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.05493274+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.442263}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4355652, "arrival": 1711329861.7733736}, "models": {"yolo": {"arrival": 1711329860.8271089, "serving": 1711329861.746263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4433637, "arrival": 1711329862.5208545}, "models": {"yolo": {"arrival": 1711329862.110524, "serving": 1711329862.48508}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:12.97594069+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329852.4431043}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.442654, "arrival": 1711329863.685042}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.435548, "arrival": 1711329863.3422513}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.443136, "arrival": 1711329863.3486445}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4422963, "arrival": 1711329862.6929467}, "models": {"yolo": {"arrival": 1711329862.4789062, "serving": 1711329862.6732218}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 6.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:12.976052584+00:00\"}\"\n>", "times": {"request": {"sending": 1711329852.4799213}}, "outputs": []}, {"times": {"request": {"sending": 1711329852.4758897, "arrival": 1711329862.5320764}, "models": {"yolo": {"arrival": 1711329862.290511, "serving": 1711329862.4980836}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.4435065, "arrival": 1711329863.3495023}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329852.442956, "arrival": 1711329862.6866627}, "models": {"yolo": {"arrival": 1711329862.3079097, "serving": 1711329862.653976}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329853.4350512, "arrival": 1711329863.657357}, "models": {"yolo": {"arrival": 1711329863.2976823, "serving": 1711329863.6132636}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.130712891+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4107177}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.508107217+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4711978}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4355743, "arrival": 1711329862.7566288}, "models": {"yolo": {"arrival": 1711329862.5110972, "serving": 1711329862.7322373}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4100661, "arrival": 1711329862.733883}, "models": {"yolo": {"arrival": 1711329862.495515, "serving": 1711329862.713658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4364917, "arrival": 1711329863.7075906}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.644495773+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4105806}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4354832, "arrival": 1711329863.7028892}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.357262044+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4351168}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4232578, "arrival": 1711329863.2981431}, "models": {"yolo": {"arrival": 1711329862.9831367, "serving": 1711329863.2720144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4363103, "arrival": 1711329863.2794797}, "models": {"yolo": {"arrival": 1711329862.743676, "serving": 1711329863.2548442}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.435305, "arrival": 1711329864.0074835}, "models": {"yolo": {"arrival": 1711329863.6261952, "serving": 1711329863.970072}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4362097, "arrival": 1711329863.7071273}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.436025, "arrival": 1711329864.0141582}, "models": {"yolo": {"arrival": 1711329863.6261952, "serving": 1711329863.970072}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4098117, "arrival": 1711329863.352258}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4098556, "arrival": 1711329862.5341907}, "models": {"yolo": {"arrival": 1711329862.290511, "serving": 1711329862.4980836}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4359796, "arrival": 1711329863.669882}, "models": {"yolo": {"arrival": 1711329863.2824738, "serving": 1711329863.624847}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.131043442+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.435341}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4103854, "arrival": 1711329862.5353205}, "models": {"yolo": {"arrival": 1711329862.290511, "serving": 1711329862.4980836}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4258568, "arrival": 1711329862.752075}, "models": {"yolo": {"arrival": 1711329862.5110972, "serving": 1711329862.7322373}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4103525, "arrival": 1711329863.352799}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4233153, "arrival": 1711329862.7383616}, "models": {"yolo": {"arrival": 1711329862.495515, "serving": 1711329862.713658}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.357169224+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4256186}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4256012, "arrival": 1711329863.7009504}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4097211, "arrival": 1711329862.5241752}, "models": {"yolo": {"arrival": 1711329862.110524, "serving": 1711329862.48508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4779034, "arrival": 1711329863.7079928}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4106145, "arrival": 1711329862.7366412}, "models": {"yolo": {"arrival": 1711329862.495515, "serving": 1711329862.713658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4258227, "arrival": 1711329863.1543422}, "models": {"yolo": {"arrival": 1711329862.7230208, "serving": 1711329863.1340199}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.644729208+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4354055}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4745538, "arrival": 1711329863.9885364}, "models": {"yolo": {"arrival": 1711329863.6351204, "serving": 1711329863.9639893}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4711504, "arrival": 1711329864.3456976}, "models": {"yolo": {"arrival": 1711329863.9817245, "serving": 1711329864.3335707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.644778994+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4356523}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4101305, "arrival": 1711329862.534834}, "models": {"yolo": {"arrival": 1711329862.290511, "serving": 1711329862.4980836}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.130797257+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.42558}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4349916, "arrival": 1711329863.3058786}, "models": {"yolo": {"arrival": 1711329862.9831367, "serving": 1711329863.2720144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4258401, "arrival": 1711329863.6564457}, "models": {"yolo": {"arrival": 1711329863.2976823, "serving": 1711329863.6132636}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.47467, "arrival": 1711329863.2839117}, "models": {"yolo": {"arrival": 1711329862.743676, "serving": 1711329863.2548442}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.35703109+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4099612}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4104812, "arrival": 1711329863.6973422}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.130628602+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4101622}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4709318, "arrival": 1711329863.988137}, "models": {"yolo": {"arrival": 1711329863.6351204, "serving": 1711329863.9639893}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.357496484+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4780116}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.43529, "arrival": 1711329863.160274}, "models": {"yolo": {"arrival": 1711329862.7230208, "serving": 1711329863.1340199}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.410547, "arrival": 1711329863.2961845}, "models": {"yolo": {"arrival": 1711329862.9831367, "serving": 1711329863.2720144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.425475, "arrival": 1711329863.2991586}, "models": {"yolo": {"arrival": 1711329862.9831367, "serving": 1711329863.2720144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.436151, "arrival": 1711329863.294878}, "models": {"yolo": {"arrival": 1711329863.1434095, "serving": 1711329863.2686384}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4351656, "arrival": 1711329863.1576536}, "models": {"yolo": {"arrival": 1711329862.7230208, "serving": 1711329863.1340199}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4361799, "arrival": 1711329863.2785392}, "models": {"yolo": {"arrival": 1711329862.743676, "serving": 1711329863.2548442}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.436436, "arrival": 1711329864.3435116}, "models": {"yolo": {"arrival": 1711329863.9817245, "serving": 1711329864.3335707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.357392464+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4360921}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.508033872+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4361951}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.645573972+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4361227}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.644561937+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4254932}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.507941775+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4357216}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4711733, "arrival": 1711329863.283097}, "models": {"yolo": {"arrival": 1711329862.743676, "serving": 1711329863.2548442}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4355447, "arrival": 1711329863.1616356}, "models": {"yolo": {"arrival": 1711329862.7230208, "serving": 1711329863.1340199}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4255612, "arrival": 1711329862.540915}, "models": {"yolo": {"arrival": 1711329862.290511, "serving": 1711329862.4980836}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.130579654+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.409892}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.357329506+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4354978}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.130849902+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4257228}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4746122, "arrival": 1711329863.803609}, "models": {"yolo": {"arrival": 1711329863.280803, "serving": 1711329863.785378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4102561, "arrival": 1711329863.001548}, "models": {"yolo": {"arrival": 1711329862.665791, "serving": 1711329862.972925}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4100983, "arrival": 1711329863.3525302}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4364533, "arrival": 1711329863.2809184}, "models": {"yolo": {"arrival": 1711329862.743676, "serving": 1711329863.2548442}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4356973, "arrival": 1711329863.2657518}, "models": {"yolo": {"arrival": 1711329862.743676, "serving": 1711329863.2548442}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4231224, "arrival": 1711329863.6976306}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.644749919+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4355297}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.644527478+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4232855}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4258914, "arrival": 1711329863.7015471}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4353573, "arrival": 1711329863.7025414}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4352608, "arrival": 1711329863.6587667}, "models": {"yolo": {"arrival": 1711329863.2824738, "serving": 1711329863.624847}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.357125428+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.423222}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4350708, "arrival": 1711329862.752743}, "models": {"yolo": {"arrival": 1711329862.5110972, "serving": 1711329862.7322373}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.35706683+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.410225}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.43624, "arrival": 1711329863.9871936}, "models": {"yolo": {"arrival": 1711329863.6351204, "serving": 1711329863.9639893}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.409927, "arrival": 1711329863.6965947}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4099996, "arrival": 1711329863.0009239}, "models": {"yolo": {"arrival": 1711329862.665791, "serving": 1711329862.972925}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4351015, "arrival": 1711329863.7018375}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.425436, "arrival": 1711329863.6979177}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4350328, "arrival": 1711329863.156743}, "models": {"yolo": {"arrival": 1711329862.7230208, "serving": 1711329863.1340199}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:13.644402418+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4100323}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4351814, "arrival": 1711329863.6580827}, "models": {"yolo": {"arrival": 1711329863.2976823, "serving": 1711329863.6132636}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4355595, "arrival": 1711329864.0099044}, "models": {"yolo": {"arrival": 1711329863.6261952, "serving": 1711329863.970072}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4360406, "arrival": 1711329863.2666965}, "models": {"yolo": {"arrival": 1711329862.743676, "serving": 1711329863.2548442}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.645654922+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4710214}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.357147362+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4254558}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.644465178+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4102879}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4101937, "arrival": 1711329863.6970434}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.357204944+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4257598}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.35730399+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4353728}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.130670778+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4104476}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.5080857+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4364743}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.425541, "arrival": 1711329863.361522}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4256885, "arrival": 1711329863.6553323}, "models": {"yolo": {"arrival": 1711329863.2976823, "serving": 1711329863.6132636}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.471047, "arrival": 1711329863.8027906}, "models": {"yolo": {"arrival": 1711329863.280803, "serving": 1711329863.785378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4355152, "arrival": 1711329863.6600375}, "models": {"yolo": {"arrival": 1711329863.2824738, "serving": 1711329863.624847}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4106522, "arrival": 1711329863.353056}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4103198, "arrival": 1711329862.7358952}, "models": {"yolo": {"arrival": 1711329862.495515, "serving": 1711329862.713658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4354217, "arrival": 1711329863.1609874}, "models": {"yolo": {"arrival": 1711329862.7230208, "serving": 1711329863.1340199}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4356825, "arrival": 1711329864.0105138}, "models": {"yolo": {"arrival": 1711329863.6261952, "serving": 1711329863.970072}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.508062836+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.436331}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.357240801+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4349122}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4746413, "arrival": 1711329864.3465078}, "models": {"yolo": {"arrival": 1711329863.9817245, "serving": 1711329864.3335707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.35741216+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4362252}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.508128924+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4746985}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4361656, "arrival": 1711329864.0154903}, "models": {"yolo": {"arrival": 1711329863.6261952, "serving": 1711329863.970072}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4252787, "arrival": 1711329863.359133}, "models": {"yolo": {"arrival": 1711329860.859817, "serving": 1711329863.2667599}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4257417, "arrival": 1711329863.7012746}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4361067, "arrival": 1711329863.6722326}, "models": {"yolo": {"arrival": 1711329863.2824738, "serving": 1711329863.624847}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.357090926+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.410514}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.130995202+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4352126}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4362957, "arrival": 1711329864.0160499}, "models": {"yolo": {"arrival": 1711329863.6261952, "serving": 1711329863.970072}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4353235, "arrival": 1711329862.7540698}, "models": {"yolo": {"arrival": 1711329862.5110972, "serving": 1711329862.7322373}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4256356, "arrival": 1711329863.299822}, "models": {"yolo": {"arrival": 1711329862.9831367, "serving": 1711329863.2720144}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4255202, "arrival": 1711329862.7391207}, "models": {"yolo": {"arrival": 1711329862.495515, "serving": 1711329862.713658}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.471218, "arrival": 1711329863.7077918}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4353895, "arrival": 1711329863.6593993}, "models": {"yolo": {"arrival": 1711329863.2824738, "serving": 1711329863.624847}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.645625031+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4364002}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.644678722+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4351475}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.357432863+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4363651}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4351327, "arrival": 1711329863.3063264}, "models": {"yolo": {"arrival": 1711329862.9831367, "serving": 1711329863.2720144}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.645539284+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4359949}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4356372, "arrival": 1711329863.663697}, "models": {"yolo": {"arrival": 1711329863.2824738, "serving": 1711329863.624847}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.357453179+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4365096}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.130755917+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4254007}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4256723, "arrival": 1711329862.7405975}, "models": {"yolo": {"arrival": 1711329862.495515, "serving": 1711329862.713658}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.644653744+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.435015}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.35728298+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4352443}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.644587829+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4256535}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4354367, "arrival": 1711329864.0079825}, "models": {"yolo": {"arrival": 1711329863.6261952, "serving": 1711329863.970072}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4253628, "arrival": 1711329862.540302}, "models": {"yolo": {"arrival": 1711329862.290511, "serving": 1711329862.4980836}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.143608593+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4355898}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.143541643+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.435468}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.82 GiB already allocated; 66.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.82 GiB already allocated; 66.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.508195557+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.474585}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.508006253+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.436061}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.645599771+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4362535}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4360778, "arrival": 1711329863.7068455}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.82 GiB already allocated; 66.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.508218511+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.82 GiB already allocated; 66.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4780815}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4356046, "arrival": 1711329863.7031608}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.478052, "arrival": 1711329864.0053377}, "models": {"yolo": {"arrival": 1711329863.6351204, "serving": 1711329863.9639893}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4360101, "arrival": 1711329863.29411}, "models": {"yolo": {"arrival": 1711329863.1434095, "serving": 1711329863.2686384}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4362667, "arrival": 1711329863.2955174}, "models": {"yolo": {"arrival": 1711329863.1434095, "serving": 1711329863.2686384}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4257052, "arrival": 1711329862.7512317}, "models": {"yolo": {"arrival": 1711329862.5110972, "serving": 1711329862.7322373}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4351964, "arrival": 1711329862.7534492}, "models": {"yolo": {"arrival": 1711329862.5110972, "serving": 1711329862.7322373}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.357370171+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4359634}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4363482, "arrival": 1711329863.7073514}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.357350635+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.435622}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4352279, "arrival": 1711329863.70217}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4363816, "arrival": 1711329863.987714}, "models": {"yolo": {"arrival": 1711329863.6351204, "serving": 1711329863.9639893}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4106848, "arrival": 1711329862.5397618}, "models": {"yolo": {"arrival": 1711329862.290511, "serving": 1711329862.4980836}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4356685, "arrival": 1711329863.2932122}, "models": {"yolo": {"arrival": 1711329863.1434095, "serving": 1711329863.2686384}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.130951128+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.435086}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4257846, "arrival": 1711329863.3053434}, "models": {"yolo": {"arrival": 1711329862.9831367, "serving": 1711329863.2720144}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.64463145+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4258037}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.1308921+00:00\"}\"\n>", "times": {"request": {"sending": 1711329853.4258745}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:13.644699169+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.87 GiB already allocated; 10.44 MiB free; 2.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4352748}}, "outputs": []}, {"times": {"request": {"sending": 1711329853.4359398, "arrival": 1711329863.703423}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.435451, "arrival": 1711329862.7559304}, "models": {"yolo": {"arrival": 1711329862.5110972, "serving": 1711329862.7322373}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329853.4364185, "arrival": 1711329863.8000882}, "models": {"yolo": {"arrival": 1711329863.280803, "serving": 1711329863.785378}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.357473901+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329853.4744492}}, "outputs": []}], [{"times": {"request": {"sending": 1711329854.4402378, "arrival": 1711329864.8310728}, "models": {"yolo": {"arrival": 1711329864.3439305, "serving": 1711329864.7740738}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.375228395+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4417398}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.357516521+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4176314}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.687207868+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4343076}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.417751, "arrival": 1711329864.363931}, "models": {"yolo": {"arrival": 1711329863.9817245, "serving": 1711329864.3335707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.442275, "arrival": 1711329864.390471}, "models": {"yolo": {"arrival": 1711329863.9748967, "serving": 1711329864.3317685}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4184184, "arrival": 1711329864.3850935}, "models": {"yolo": {"arrival": 1711329863.97375, "serving": 1711329864.3380246}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4425113, "arrival": 1711329865.2633271}, "models": {"yolo": {"arrival": 1711329864.7845066, "serving": 1711329865.2362313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4341862, "arrival": 1711329863.8186312}, "models": {"yolo": {"arrival": 1711329863.280803, "serving": 1711329863.785378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4402921, "arrival": 1711329863.9782846}, "models": {"yolo": {"arrival": 1711329863.648024, "serving": 1711329863.9485712}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.94 GiB already allocated; 60.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.94 GiB already allocated; 60.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.49322977+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4699419}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.381594415+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4421918}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.696454842+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4408877}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4682238, "arrival": 1711329864.8083084}, "models": {"yolo": {"arrival": 1711329864.343997, "serving": 1711329864.772312}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4173796, "arrival": 1711329863.804267}, "models": {"yolo": {"arrival": 1711329863.280803, "serving": 1711329863.785378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.417984, "arrival": 1711329864.3648849}, "models": {"yolo": {"arrival": 1711329863.9817245, "serving": 1711329864.3335707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4175332, "arrival": 1711329863.2845626}, "models": {"yolo": {"arrival": 1711329862.743676, "serving": 1711329863.2548442}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.696500446+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4416628}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4177222, "arrival": 1711329863.8104577}, "models": {"yolo": {"arrival": 1711329863.280803, "serving": 1711329863.785378}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.388031467+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4682844}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4299383, "arrival": 1711329863.6372426}, "models": {"yolo": {"arrival": 1711329863.2671945, "serving": 1711329863.6084867}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.46987, "arrival": 1711329864.8087635}, "models": {"yolo": {"arrival": 1711329864.343997, "serving": 1711329864.772312}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.69667806+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.440752}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4342215, "arrival": 1711329864.0003242}, "models": {"yolo": {"arrival": 1711329863.6209822, "serving": 1711329863.9625223}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.6966555+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4406178}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4421234, "arrival": 1711329865.6842892}, "models": {"yolo": {"arrival": 1711329865.34726, "serving": 1711329865.663119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.696766397+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4420795}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4185116, "arrival": 1711329864.8151855}, "models": {"yolo": {"arrival": 1711329864.3439305, "serving": 1711329864.7740738}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.358184284+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4403098}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.508151872+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.417568}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.440431, "arrival": 1711329864.390996}, "models": {"yolo": {"arrival": 1711329863.9588573, "serving": 1711329864.342113}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4399915, "arrival": 1711329863.8195314}, "models": {"yolo": {"arrival": 1711329863.280803, "serving": 1711329863.785378}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.696381796+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4404147}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.434048, "arrival": 1711329864.8155677}, "models": {"yolo": {"arrival": 1711329864.3439305, "serving": 1711329864.7740738}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.357537861+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4178672}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4176612, "arrival": 1711329864.0069737}, "models": {"yolo": {"arrival": 1711329863.6351204, "serving": 1711329863.9639893}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.696408304+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4405484}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.364297627+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.440717}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4174922, "arrival": 1711329864.3630264}, "models": {"yolo": {"arrival": 1711329863.9817245, "serving": 1711329864.3335707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.417838, "arrival": 1711329863.7091377}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.358034399+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4342747}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4419017, "arrival": 1711329864.3690064}, "models": {"yolo": {"arrival": 1711329863.9748967, "serving": 1711329864.3317685}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4417734, "arrival": 1711329864.8356657}, "models": {"yolo": {"arrival": 1711329864.3541226, "serving": 1711329864.7742221}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.440925, "arrival": 1711329864.3922274}, "models": {"yolo": {"arrival": 1711329863.9588573, "serving": 1711329864.342113}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.442105, "arrival": 1711329864.4000404}, "models": {"yolo": {"arrival": 1711329863.9635699, "serving": 1711329864.3471444}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.686972305+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4180408}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.434204, "arrival": 1711329864.8159347}, "models": {"yolo": {"arrival": 1711329864.3439305, "serving": 1711329864.7740738}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4408116, "arrival": 1711329865.359061}, "models": {"yolo": {"arrival": 1711329864.7865653, "serving": 1711329865.3373244}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.696568008+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4422915}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4699626, "arrival": 1711329864.8176858}, "models": {"yolo": {"arrival": 1711329864.35435, "serving": 1711329864.7729552}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4301279, "arrival": 1711329864.3856983}, "models": {"yolo": {"arrival": 1711329863.97375, "serving": 1711329864.3380246}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4176004, "arrival": 1711329863.7081947}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.39267599+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4699805}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.687139258+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4184499}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.372202491+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4412491}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.82 GiB already allocated; 66.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.50824218+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.82 GiB already allocated; 66.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.417691}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4418745, "arrival": 1711329865.3613398}, "models": {"yolo": {"arrival": 1711329864.7865653, "serving": 1711329865.3373244}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4424121, "arrival": 1711329865.691172}, "models": {"yolo": {"arrival": 1711329865.34726, "serving": 1711329865.663119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.440531, "arrival": 1711329864.0133357}, "models": {"yolo": {"arrival": 1711329863.6209822, "serving": 1711329863.9625223}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.687184921+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4341645}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.387082237+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4424908}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.357597965+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4300997}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.687114179+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.440123}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.382666283+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4423225}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.687066724+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.434091}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.88 GiB already allocated; 66.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.508172682+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4178095}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4682667, "arrival": 1711329864.8173404}, "models": {"yolo": {"arrival": 1711329864.35435, "serving": 1711329864.7729552}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.358125239+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.440162}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.357619951+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4341278}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4407334, "arrival": 1711329864.819946}, "models": {"yolo": {"arrival": 1711329864.3541226, "serving": 1711329864.7742221}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4422095, "arrival": 1711329865.2533164}, "models": {"yolo": {"arrival": 1711329864.7845066, "serving": 1711329865.2362313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4179559, "arrival": 1711329863.8112779}, "models": {"yolo": {"arrival": 1711329863.280803, "serving": 1711329863.785378}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.687091055+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4342387}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4417017, "arrival": 1711329864.400427}, "models": {"yolo": {"arrival": 1711329863.9588573, "serving": 1711329864.342113}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.441626, "arrival": 1711329864.368281}, "models": {"yolo": {"arrival": 1711329863.9748967, "serving": 1711329864.3317685}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.69631894+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4402738}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.696523042+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4419322}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.696744228+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.44181}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.440327, "arrival": 1711329864.3898249}, "models": {"yolo": {"arrival": 1711329863.97375, "serving": 1711329864.3380246}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.696477823+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4411871}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4401433, "arrival": 1711329863.9776177}, "models": {"yolo": {"arrival": 1711329863.648024, "serving": 1711329863.9485712}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.357558518+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4183664}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4683418, "arrival": 1711329864.4021459}, "models": {"yolo": {"arrival": 1711329863.9635699, "serving": 1711329864.3471444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4178967, "arrival": 1711329864.3843296}, "models": {"yolo": {"arrival": 1711329863.97375, "serving": 1711329864.3380246}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4180121, "arrival": 1711329863.6365306}, "models": {"yolo": {"arrival": 1711329863.2671945, "serving": 1711329863.6084867}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.697146651+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4424448}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4421742, "arrival": 1711329864.4010057}, "models": {"yolo": {"arrival": 1711329863.9588573, "serving": 1711329864.342113}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.696722176+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4413023}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.441566, "arrival": 1711329864.3948057}, "models": {"yolo": {"arrival": 1711329863.9635699, "serving": 1711329864.3471444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.441844, "arrival": 1711329864.3953578}, "models": {"yolo": {"arrival": 1711329863.9635699, "serving": 1711329864.3471444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4422436, "arrival": 1711329864.4012578}, "models": {"yolo": {"arrival": 1711329863.9635699, "serving": 1711329864.3471444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.440498, "arrival": 1711329863.9835634}, "models": {"yolo": {"arrival": 1711329863.7954066, "serving": 1711329863.9537818}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4403994, "arrival": 1711329864.006434}, "models": {"yolo": {"arrival": 1711329863.6209822, "serving": 1711329863.9625223}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.442043, "arrival": 1711329865.251183}, "models": {"yolo": {"arrival": 1711329864.7845066, "serving": 1711329865.2362313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4177797, "arrival": 1711329863.6356065}, "models": {"yolo": {"arrival": 1711329863.2671945, "serving": 1711329863.6084867}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4401805, "arrival": 1711329864.3890119}, "models": {"yolo": {"arrival": 1711329863.97375, "serving": 1711329864.3380246}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4341094, "arrival": 1711329863.971263}, "models": {"yolo": {"arrival": 1711329863.648024, "serving": 1711329863.9485712}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.440601, "arrival": 1711329864.8185675}, "models": {"yolo": {"arrival": 1711329864.3541226, "serving": 1711329864.7742221}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.687162361+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4301531}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4403827, "arrival": 1711329864.8320117}, "models": {"yolo": {"arrival": 1711329864.3439305, "serving": 1711329864.7740738}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4412763, "arrival": 1711329864.8352015}, "models": {"yolo": {"arrival": 1711329864.3541226, "serving": 1711329864.7742221}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.378542979+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4420035}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4404635, "arrival": 1711329864.8180366}, "models": {"yolo": {"arrival": 1711329864.3541226, "serving": 1711329864.7742221}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.696612167+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4403453}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.468358, "arrival": 1711329865.6925092}, "models": {"yolo": {"arrival": 1711329865.34726, "serving": 1711329865.663119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.82 GiB already allocated; 66.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.82 GiB already allocated; 66.44 MiB free; 1.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.508264211+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4179256}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4410732, "arrival": 1711329864.3934405}, "models": {"yolo": {"arrival": 1711329863.9635699, "serving": 1711329864.3471444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4415994, "arrival": 1711329865.3607838}, "models": {"yolo": {"arrival": 1711329864.7865653, "serving": 1711329865.3373244}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4407012, "arrival": 1711329864.3918533}, "models": {"yolo": {"arrival": 1711329863.9588573, "serving": 1711329864.342113}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4423392, "arrival": 1711329865.2542968}, "models": {"yolo": {"arrival": 1711329864.7845066, "serving": 1711329865.2362313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4425445, "arrival": 1711329864.401869}, "models": {"yolo": {"arrival": 1711329863.9635699, "serving": 1711329864.3471444}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.94 GiB already allocated; 60.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.94 GiB already allocated; 60.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.493154857+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4682474}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4403653, "arrival": 1711329863.9827669}, "models": {"yolo": {"arrival": 1711329863.7954066, "serving": 1711329863.9537818}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.696787913+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4422266}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.696811399+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4423556}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4423068, "arrival": 1711329864.816641}, "models": {"yolo": {"arrival": 1711329864.35435, "serving": 1711329864.7729552}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4400778, "arrival": 1711329864.816296}, "models": {"yolo": {"arrival": 1711329864.3439305, "serving": 1711329864.7740738}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4422596, "arrival": 1711329865.6902068}, "models": {"yolo": {"arrival": 1711329865.34726, "serving": 1711329865.663119}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.696589331+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4401987}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.696432806+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4406843}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.44197, "arrival": 1711329864.4007053}, "models": {"yolo": {"arrival": 1711329863.9588573, "serving": 1711329864.342113}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4340718, "arrival": 1711329863.639042}, "models": {"yolo": {"arrival": 1711329863.2671945, "serving": 1711329863.6084867}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4401019, "arrival": 1711329864.0047307}, "models": {"yolo": {"arrival": 1711329863.6209822, "serving": 1711329863.9625223}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.358237559+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4404473}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4407806, "arrival": 1711329864.3929374}, "models": {"yolo": {"arrival": 1711329863.9635699, "serving": 1711329864.3471444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4683046, "arrival": 1711329865.2645218}, "models": {"yolo": {"arrival": 1711329864.7845066, "serving": 1711329865.2362313}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4405668, "arrival": 1711329864.3915033}, "models": {"yolo": {"arrival": 1711329863.9588573, "serving": 1711329864.342113}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4180698, "arrival": 1711329863.7093709}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.696699881+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.441035}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4411528, "arrival": 1711329864.3661785}, "models": {"yolo": {"arrival": 1711329863.9748967, "serving": 1711329864.3317685}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4408488, "arrival": 1711329864.3655386}, "models": {"yolo": {"arrival": 1711329863.9748967, "serving": 1711329864.3317685}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.440256, "arrival": 1711329864.0058875}, "models": {"yolo": {"arrival": 1711329863.6209822, "serving": 1711329863.9625223}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4184813, "arrival": 1711329863.8138673}, "models": {"yolo": {"arrival": 1711329863.280803, "serving": 1711329863.785378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4699986, "arrival": 1711329865.6930406}, "models": {"yolo": {"arrival": 1711329865.2433157, "serving": 1711329865.6646135}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4341462, "arrival": 1711329864.3862994}, "models": {"yolo": {"arrival": 1711329863.97375, "serving": 1711329864.3380246}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 60.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 60.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.493304017+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4425278}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4342573, "arrival": 1711329863.9764302}, "models": {"yolo": {"arrival": 1711329863.648024, "serving": 1711329863.9485712}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4406524, "arrival": 1711329865.3583794}, "models": {"yolo": {"arrival": 1711329864.7865653, "serving": 1711329865.3373244}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4402194, "arrival": 1711329863.9789598}, "models": {"yolo": {"arrival": 1711329863.7954066, "serving": 1711329863.9537818}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4424286, "arrival": 1711329864.8078194}, "models": {"yolo": {"arrival": 1711329864.343997, "serving": 1711329864.772312}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4406679, "arrival": 1711329864.0149353}, "models": {"yolo": {"arrival": 1711329863.6209822, "serving": 1711329863.9625223}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 60.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 60.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.493347748+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4700155}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4412212, "arrival": 1711329864.3994575}, "models": {"yolo": {"arrival": 1711329863.9588573, "serving": 1711329864.342113}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4406357, "arrival": 1711329863.9842074}, "models": {"yolo": {"arrival": 1711329863.7954066, "serving": 1711329863.9537818}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.696632927+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 62.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4404805}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4342911, "arrival": 1711329864.3882387}, "models": {"yolo": {"arrival": 1711329863.97375, "serving": 1711329864.3380246}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4405143, "arrival": 1711329865.3575482}, "models": {"yolo": {"arrival": 1711329864.7865653, "serving": 1711329865.3373244}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.368280186+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4409614}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:14.696545671+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4421575}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4409976, "arrival": 1711329864.8203864}, "models": {"yolo": {"arrival": 1711329864.3541226, "serving": 1711329864.7742221}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4339635, "arrival": 1711329863.8151681}, "models": {"yolo": {"arrival": 1711329863.280803, "serving": 1711329863.785378}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.361860137+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4405851}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4300723, "arrival": 1711329863.709575}, "models": {"yolo": {"arrival": 1711329860.8599162, "serving": 1711329863.6153908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 60.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 60.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.493324512+00:00\"}\"\n>", "times": {"request": {"sending": 1711329854.4683223}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4681346, "arrival": 1711329865.6918497}, "models": {"yolo": {"arrival": 1711329865.34726, "serving": 1711329865.663119}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.441114, "arrival": 1711329865.3594892}, "models": {"yolo": {"arrival": 1711329864.7865653, "serving": 1711329865.3373244}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4423735, "arrival": 1711329864.401514}, "models": {"yolo": {"arrival": 1711329863.9635699, "serving": 1711329864.3471444}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329854.4421408, "arrival": 1711329864.3871017}, "models": {"yolo": {"arrival": 1711329863.9748967, "serving": 1711329864.3317685}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:14.687039935+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 23.64 GiB total capacity; 1.92 GiB already allocated; 62.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329854.4300401}}, "outputs": []}, {"times": {"request": {"sending": 1711329854.4424624, "arrival": 1711329864.8169878}, "models": {"yolo": {"arrival": 1711329864.35435, "serving": 1711329864.7729552}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329855.4368005, "arrival": 1711329865.3543181}, "models": {"yolo": {"arrival": 1711329864.783161, "serving": 1711329865.3304245}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.836056678+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.415782}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.83300429+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.451923}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4417477, "arrival": 1711329865.7124012}, "models": {"yolo": {"arrival": 1711329865.3405364, "serving": 1711329865.669684}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.436644, "arrival": 1711329865.3532763}, "models": {"yolo": {"arrival": 1711329864.783161, "serving": 1711329865.3304245}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4520116, "arrival": 1711329864.1814299}, "models": {"yolo": {"arrival": 1711329863.6376753, "serving": 1711329864.1467304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4175782, "arrival": 1711329865.3511698}, "models": {"yolo": {"arrival": 1711329864.783161, "serving": 1711329865.3304245}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4146264, "arrival": 1711329864.7960765}, "models": {"yolo": {"arrival": 1711329864.3547668, "serving": 1711329864.7644634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4513614, "arrival": 1711329865.720241}, "models": {"yolo": {"arrival": 1711329865.2483404, "serving": 1711329865.6721525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4514287, "arrival": 1711329867.4953687}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4516003, "arrival": 1711329866.0073261}, "models": {"yolo": {"arrival": 1711329865.6808193, "serving": 1711329865.9860103}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.824344641+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4418232}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.451692, "arrival": 1711329867.4960382}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4417326, "arrival": 1711329866.3433676}, "models": {"yolo": {"arrival": 1711329866.0595043, "serving": 1711329866.3306031}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.406208089+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4154384}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4516327, "arrival": 1711329865.999681}, "models": {"yolo": {"arrival": 1711329865.680595, "serving": 1711329865.9768093}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4522352, "arrival": 1711329864.336454}, "models": {"yolo": {"arrival": 1711329864.1598735, "serving": 1711329864.3221502}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4151413, "arrival": 1711329864.7965205}, "models": {"yolo": {"arrival": 1711329864.3547668, "serving": 1711329864.7644634}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.864407642+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.441763}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.864451805+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4420104}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4419942, "arrival": 1711329865.7134297}, "models": {"yolo": {"arrival": 1711329865.3405364, "serving": 1711329865.669684}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.436535, "arrival": 1711329863.6705828}, "models": {"yolo": {"arrival": 1711329863.4427145, "serving": 1711329863.6239238}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4513214, "arrival": 1711329865.7189188}, "models": {"yolo": {"arrival": 1711329865.3405364, "serving": 1711329865.669684}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4519668, "arrival": 1711329866.0121574}, "models": {"yolo": {"arrival": 1711329865.6808193, "serving": 1711329865.9860103}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4518933, "arrival": 1711329864.180778}, "models": {"yolo": {"arrival": 1711329863.6376753, "serving": 1711329864.1467304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4514627, "arrival": 1711329865.7192535}, "models": {"yolo": {"arrival": 1711329865.3405364, "serving": 1711329865.669684}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.40377891+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.414967}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4151988, "arrival": 1711329866.0711806}, "models": {"yolo": {"arrival": 1711329865.6735883, "serving": 1711329866.0518286}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4516475, "arrival": 1711329864.1768427}, "models": {"yolo": {"arrival": 1711329863.6376753, "serving": 1711329864.1467304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4147294, "arrival": 1711329866.0701401}, "models": {"yolo": {"arrival": 1711329865.6735883, "serving": 1711329866.0518286}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.416071, "arrival": 1711329864.7984018}, "models": {"yolo": {"arrival": 1711329864.3547668, "serving": 1711329864.7644634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.45157, "arrival": 1711329867.4956048}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.864429155+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4418862}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4366846, "arrival": 1711329865.7141008}, "models": {"yolo": {"arrival": 1711329865.2483404, "serving": 1711329865.6721525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4516616, "arrival": 1711329866.8417916}, "models": {"yolo": {"arrival": 1711329866.313805, "serving": 1711329866.8216314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4147925, "arrival": 1711329864.8146224}, "models": {"yolo": {"arrival": 1711329864.343997, "serving": 1711329864.772312}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.8644976+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4514894}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4154954, "arrival": 1711329865.6953676}, "models": {"yolo": {"arrival": 1711329865.2433157, "serving": 1711329865.6646135}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.419478368+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4163597}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4420407, "arrival": 1711329864.1668785}, "models": {"yolo": {"arrival": 1711329863.6376753, "serving": 1711329864.1467304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4414735, "arrival": 1711329865.714442}, "models": {"yolo": {"arrival": 1711329865.2483404, "serving": 1711329865.6721525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4419322, "arrival": 1711329866.314954}, "models": {"yolo": {"arrival": 1711329865.972664, "serving": 1711329866.3027985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.415025, "arrival": 1711329865.6935608}, "models": {"yolo": {"arrival": 1711329865.2433157, "serving": 1711329865.6646135}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4149091, "arrival": 1711329864.8313935}, "models": {"yolo": {"arrival": 1711329864.35435, "serving": 1711329864.7729552}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4419024, "arrival": 1711329865.7195816}, "models": {"yolo": {"arrival": 1711329865.2483404, "serving": 1711329865.6721525}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.832939448+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4518013}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4521585, "arrival": 1711329867.5012686}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.836314682+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4167078}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.94 GiB already allocated; 60.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.94 GiB already allocated; 60.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.493256626+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.414852}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4366033, "arrival": 1711329867.490792}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4521751, "arrival": 1711329868.53316}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.816128881+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.416939}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.43818787+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4172869}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4156108, "arrival": 1711329864.7978852}, "models": {"yolo": {"arrival": 1711329864.3547668, "serving": 1711329864.7644634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4517214, "arrival": 1711329866.0105946}, "models": {"yolo": {"arrival": 1711329865.6808193, "serving": 1711329865.9860103}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.416129, "arrival": 1711329866.0737927}, "models": {"yolo": {"arrival": 1711329865.6735883, "serving": 1711329866.0518286}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4416695, "arrival": 1711329866.3113356}, "models": {"yolo": {"arrival": 1711329865.972664, "serving": 1711329866.3027985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4158394, "arrival": 1711329865.2651722}, "models": {"yolo": {"arrival": 1711329864.7817109, "serving": 1711329865.2361145}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4414968, "arrival": 1711329863.6714408}, "models": {"yolo": {"arrival": 1711329863.4427145, "serving": 1711329863.6239238}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4522212, "arrival": 1711329868.3281758}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4157252, "arrival": 1711329864.831706}, "models": {"yolo": {"arrival": 1711329864.343997, "serving": 1711329864.772312}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 60.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 60.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.493371754+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4150827}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.436624, "arrival": 1711329866.3378558}, "models": {"yolo": {"arrival": 1711329866.0595043, "serving": 1711329866.3306031}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.864540701+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4517372}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4519079, "arrival": 1711329866.8463542}, "models": {"yolo": {"arrival": 1711329866.313805, "serving": 1711329866.8216314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4514465, "arrival": 1711329868.5301092}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4519377, "arrival": 1711329867.496484}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 60.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 1.81 GiB already allocated; 60.44 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.49339328+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4155521}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4518783, "arrival": 1711329866.0041695}, "models": {"yolo": {"arrival": 1711329865.680595, "serving": 1711329865.9768093}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4166505, "arrival": 1711329865.349625}, "models": {"yolo": {"arrival": 1711329864.783161, "serving": 1711329865.3304245}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.864379228+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4416165}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4512417, "arrival": 1711329868.5296965}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4643676, "arrival": 1711329866.8477373}, "models": {"yolo": {"arrival": 1711329866.313805, "serving": 1711329866.8216314}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.824648781+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4516766}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.441855, "arrival": 1711329866.3440104}, "models": {"yolo": {"arrival": 1711329866.0595043, "serving": 1711329866.3306031}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4173446, "arrival": 1711329865.9807389}, "models": {"yolo": {"arrival": 1711329865.675542, "serving": 1711329865.9655552}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.836287261+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4162445}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4152625, "arrival": 1711329864.830465}, "models": {"yolo": {"arrival": 1711329864.343997, "serving": 1711329864.772312}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.417055, "arrival": 1711329866.0773923}, "models": {"yolo": {"arrival": 1711329865.6735883, "serving": 1711329866.0518286}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.816024572+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4160128}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4174619, "arrival": 1711329867.4904382}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4416344, "arrival": 1711329865.7181466}, "models": {"yolo": {"arrival": 1711329865.2483404, "serving": 1711329865.6721525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4167657, "arrival": 1711329865.2677515}, "models": {"yolo": {"arrival": 1711329864.7817109, "serving": 1711329865.2361145}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.436723, "arrival": 1711329865.9843647}, "models": {"yolo": {"arrival": 1711329865.675542, "serving": 1711329865.9655552}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4367037, "arrival": 1711329863.6710389}, "models": {"yolo": {"arrival": 1711329863.4427145, "serving": 1711329863.6239238}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4159555, "arrival": 1711329865.6962037}, "models": {"yolo": {"arrival": 1711329865.2433157, "serving": 1711329865.6646135}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4517853, "arrival": 1711329866.8436446}, "models": {"yolo": {"arrival": 1711329866.313805, "serving": 1711329866.8216314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4169974, "arrival": 1711329867.4900272}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.824271296+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4415367}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.416187, "arrival": 1711329865.3459506}, "models": {"yolo": {"arrival": 1711329864.783161, "serving": 1711329865.3304245}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.833031987+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4521394}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4518478, "arrival": 1711329866.011568}, "models": {"yolo": {"arrival": 1711329865.6808193, "serving": 1711329865.9860103}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4515219, "arrival": 1711329864.175989}, "models": {"yolo": {"arrival": 1711329863.6376753, "serving": 1711329864.1467304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4165926, "arrival": 1711329866.0767663}, "models": {"yolo": {"arrival": 1711329865.6735883, "serving": 1711329866.0518286}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4420257, "arrival": 1711329865.7199166}, "models": {"yolo": {"arrival": 1711329865.2483404, "serving": 1711329865.6721525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.451953, "arrival": 1711329868.5319066}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.415381, "arrival": 1711329864.8323097}, "models": {"yolo": {"arrival": 1711329864.35435, "serving": 1711329864.7729552}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4163024, "arrival": 1711329865.265835}, "models": {"yolo": {"arrival": 1711329864.7817109, "serving": 1711329865.2361145}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4518166, "arrival": 1711329867.4962604}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.816093393+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.416475}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4515853, "arrival": 1711329868.5305383}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.451832, "arrival": 1711329868.5314767}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4415574, "arrival": 1711329867.4922264}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.416534, "arrival": 1711329864.8071225}, "models": {"yolo": {"arrival": 1711329864.3547668, "serving": 1711329864.7644634}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4367614, "arrival": 1711329867.4910357}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4419174, "arrival": 1711329864.1660812}, "models": {"yolo": {"arrival": 1711329863.6376753, "serving": 1711329864.1467304}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.864352312+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4413927}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4521906, "arrival": 1711329866.0140924}, "models": {"yolo": {"arrival": 1711329865.6808193, "serving": 1711329865.9860103}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.94 GiB already allocated; 60.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 1.94 GiB already allocated; 60.44 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.493283025+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.415323}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4417934, "arrival": 1711329864.164852}, "models": {"yolo": {"arrival": 1711329863.6376753, "serving": 1711329864.1467304}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.417024097+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4158978}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4417782, "arrival": 1711329865.7185664}, "models": {"yolo": {"arrival": 1711329865.2483404, "serving": 1711329865.6721525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.416881, "arrival": 1711329865.7137692}, "models": {"yolo": {"arrival": 1711329865.2433157, "serving": 1711329865.6646135}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4520295, "arrival": 1711329866.84712}, "models": {"yolo": {"arrival": 1711329866.313805, "serving": 1711329866.8216314}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.833057736+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4647532}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4519966, "arrival": 1711329866.0049596}, "models": {"yolo": {"arrival": 1711329865.680595, "serving": 1711329865.9768093}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4517696, "arrival": 1711329864.1800387}, "models": {"yolo": {"arrival": 1711329863.6376753, "serving": 1711329864.1467304}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.836366867+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.417171}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.824198385+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.43658}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.442088, "arrival": 1711329867.4931824}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4415977, "arrival": 1711329865.7118635}, "models": {"yolo": {"arrival": 1711329865.3405364, "serving": 1711329865.669684}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.824307056+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.441699}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.864265201+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4364183}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.415668, "arrival": 1711329866.0718453}, "models": {"yolo": {"arrival": 1711329865.6735883, "serving": 1711329866.0518286}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.824114865+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4174035}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.432389496+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 18.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4168231}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4164171, "arrival": 1711329865.7127469}, "models": {"yolo": {"arrival": 1711329865.2433157, "serving": 1711329865.6646135}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.864475282+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4513423}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4517064, "arrival": 1711329868.5309303}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4513795, "arrival": 1711329864.1683404}, "models": {"yolo": {"arrival": 1711329863.6376753, "serving": 1711329864.1467304}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4172285, "arrival": 1711329865.2684715}, "models": {"yolo": {"arrival": 1711329864.7817109, "serving": 1711329865.2361145}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.824467532+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.442071}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.417113, "arrival": 1711329865.3505116}, "models": {"yolo": {"arrival": 1711329864.783161, "serving": 1711329865.3304245}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.864586397+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4519818}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4418697, "arrival": 1711329865.7130892}, "models": {"yolo": {"arrival": 1711329865.3405364, "serving": 1711329865.669684}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4513953, "arrival": 1711329866.319088}, "models": {"yolo": {"arrival": 1711329865.972664, "serving": 1711329866.3027985}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.864325515+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4366646}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.824411123+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4419472}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.824541138+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4514112}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.824599406+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4515522}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4515054, "arrival": 1711329865.997925}, "models": {"yolo": {"arrival": 1711329865.680595, "serving": 1711329865.9768093}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.864518843+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4516165}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.4419785, "arrival": 1711329868.5290782}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4420552, "arrival": 1711329866.3165114}, "models": {"yolo": {"arrival": 1711329865.972664, "serving": 1711329866.3027985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4416513, "arrival": 1711329863.6718402}, "models": {"yolo": {"arrival": 1711329863.4427145, "serving": 1711329863.6239238}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4415777, "arrival": 1711329866.3423247}, "models": {"yolo": {"arrival": 1711329866.0595043, "serving": 1711329866.3306031}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.441717, "arrival": 1711329867.4924579}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.864614203+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.4522057}}, "outputs": []}, {"times": {"request": {"sending": 1711329855.441963, "arrival": 1711329867.492949}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.44184, "arrival": 1711329867.4926922}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.436558, "arrival": 1711329865.9834569}, "models": {"yolo": {"arrival": 1711329865.675542, "serving": 1711329865.9655552}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.441808, "arrival": 1711329866.3140924}, "models": {"yolo": {"arrival": 1711329865.972664, "serving": 1711329866.3027985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4175196, "arrival": 1711329866.07795}, "models": {"yolo": {"arrival": 1711329865.6735883, "serving": 1711329866.0518286}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4415174, "arrival": 1711329865.9850886}, "models": {"yolo": {"arrival": 1711329865.675542, "serving": 1711329865.9655552}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.451753, "arrival": 1711329866.0028937}, "models": {"yolo": {"arrival": 1711329865.680595, "serving": 1711329865.9768093}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4367797, "arrival": 1711329866.3387916}, "models": {"yolo": {"arrival": 1711329866.0595043, "serving": 1711329866.3306031}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.436507, "arrival": 1711329865.269623}, "models": {"yolo": {"arrival": 1711329864.7817109, "serving": 1711329865.2361145}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329855.4515378, "arrival": 1711329866.31983}, "models": {"yolo": {"arrival": 1711329865.972664, "serving": 1711329866.3027985}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:15.86456197+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.86 GiB already allocated; 54.44 MiB free; 2.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329855.451863}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 4.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:15.824237798+00:00\"}\"\n>", "times": {"request": {"sending": 1711329855.4367414}}, "outputs": []}], [{"times": {"request": {"sending": 1711329856.4597225, "arrival": 1711329857.659421}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.415651, "arrival": 1711329867.5016081}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429415, "arrival": 1711329867.4934108}, "models": {"yolo": {"arrival": 1711329867.1608443, "serving": 1711329867.4258437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429128, "arrival": 1711329867.5416453}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4158897, "arrival": 1711329867.5026202}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4291945, "arrival": 1711329857.6493013}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4293988, "arrival": 1711329864.8331797}, "models": {"yolo": {"arrival": 1711329864.3376873, "serving": 1711329864.7897558}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4333127, "arrival": 1711329868.5605621}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4158666, "arrival": 1711329857.640696}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4159572, "arrival": 1711329857.6436746}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.461467, "arrival": 1711329867.836745}, "models": {"yolo": {"arrival": 1711329867.439102, "serving": 1711329867.7974005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4157481, "arrival": 1711329866.0148375}, "models": {"yolo": {"arrival": 1711329865.6808193, "serving": 1711329865.9860103}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4330852, "arrival": 1711329867.553601}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4256957, "arrival": 1711329864.832598}, "models": {"yolo": {"arrival": 1711329864.3376873, "serving": 1711329864.7897558}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4292142, "arrival": 1711329868.3440487}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4301612, "arrival": 1711329868.356232}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4294777, "arrival": 1711329866.361024}, "models": {"yolo": {"arrival": 1711329865.9977942, "serving": 1711329866.3399577}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.433352, "arrival": 1711329868.3622336}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4662857, "arrival": 1711329867.837647}, "models": {"yolo": {"arrival": 1711329867.439102, "serving": 1711329867.7974005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.461515, "arrival": 1711329868.5636802}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4237497, "arrival": 1711329868.33183}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.461373, "arrival": 1711329868.5623636}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4160006, "arrival": 1711329864.3406537}, "models": {"yolo": {"arrival": 1711329864.1598735, "serving": 1711329864.3221502}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4596856, "arrival": 1711329865.5666249}, "models": {"yolo": {"arrival": 1711329864.8038635, "serving": 1711329865.5239131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4330037, "arrival": 1711329857.6578503}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4160903, "arrival": 1711329868.5397184}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.459704, "arrival": 1711329867.836322}, "models": {"yolo": {"arrival": 1711329867.439102, "serving": 1711329867.7974005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4157176, "arrival": 1711329868.533778}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4295535, "arrival": 1711329867.4936445}, "models": {"yolo": {"arrival": 1711329867.1608443, "serving": 1711329867.4258437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4290743, "arrival": 1711329857.6417162}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4334185, "arrival": 1711329868.5619452}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.46145, "arrival": 1711329865.5674322}, "models": {"yolo": {"arrival": 1711329864.8038635, "serving": 1711329865.5239131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.430146, "arrival": 1711329857.6576316}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4333389, "arrival": 1711329857.6617696}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4297621, "arrival": 1711329866.9557645}, "models": {"yolo": {"arrival": 1711329866.3541548, "serving": 1711329866.9386873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4330993, "arrival": 1711329868.5575624}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4295068, "arrival": 1711329868.3473094}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4293694, "arrival": 1711329868.3465452}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4331517, "arrival": 1711329865.5616064}, "models": {"yolo": {"arrival": 1711329864.8038635, "serving": 1711329865.5239131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4330425, "arrival": 1711329865.5602152}, "models": {"yolo": {"arrival": 1711329864.8038635, "serving": 1711329865.5239131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4297104, "arrival": 1711329857.6460536}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4334056, "arrival": 1711329867.5554247}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.551710347+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329856.4663012}}, "outputs": []}, {"times": {"request": {"sending": 1711329856.433286, "arrival": 1711329857.6580637}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4292774, "arrival": 1711329857.6445901}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4614277, "arrival": 1711329868.3708053}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4331384, "arrival": 1711329868.3604436}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4157736, "arrival": 1711329857.6420434}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4157975, "arrival": 1711329868.329066}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4299803, "arrival": 1711329867.559498}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4159787, "arrival": 1711329868.331088}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4300554, "arrival": 1711329864.839198}, "models": {"yolo": {"arrival": 1711329864.3376873, "serving": 1711329864.7897558}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.415821, "arrival": 1711329864.339474}, "models": {"yolo": {"arrival": 1711329864.1598735, "serving": 1711329864.3221502}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429688, "arrival": 1711329867.4938772}, "models": {"yolo": {"arrival": 1711329867.1608443, "serving": 1711329867.4258437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4302194, "arrival": 1711329867.5597358}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4298728, "arrival": 1711329868.547521}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4238498, "arrival": 1711329867.1762867}, "models": {"yolo": {"arrival": 1711329866.8306148, "serving": 1711329867.1484783}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4298248, "arrival": 1711329867.4942062}, "models": {"yolo": {"arrival": 1711329867.1608443, "serving": 1711329867.4258437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4595768, "arrival": 1711329857.6619892}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4296544, "arrival": 1711329868.3514962}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.461412, "arrival": 1711329857.6621976}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4300861, "arrival": 1711329857.6508586}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4615788, "arrival": 1711329865.568078}, "models": {"yolo": {"arrival": 1711329864.8038635, "serving": 1711329865.5239131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4301753, "arrival": 1711329864.8395066}, "models": {"yolo": {"arrival": 1711329864.3376873, "serving": 1711329864.7897558}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429729, "arrival": 1711329867.544176}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4159353, "arrival": 1711329866.3520627}, "models": {"yolo": {"arrival": 1711329865.9977942, "serving": 1711329866.3399577}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.416112, "arrival": 1711329866.3532135}, "models": {"yolo": {"arrival": 1711329865.9977942, "serving": 1711329866.3399577}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429352, "arrival": 1711329857.6497314}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4329667, "arrival": 1711329866.9624553}, "models": {"yolo": {"arrival": 1711329866.3541548, "serving": 1711329866.9386873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4332328, "arrival": 1711329857.6614969}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.43019, "arrival": 1711329867.4969378}, "models": {"yolo": {"arrival": 1711329867.1608443, "serving": 1711329867.4258437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4331923, "arrival": 1711329867.5545359}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4298868, "arrival": 1711329866.9565024}, "models": {"yolo": {"arrival": 1711329866.3541548, "serving": 1711329866.9386873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429586, "arrival": 1711329867.543671}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.106139403+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329856.4615462}}, "outputs": []}, {"times": {"request": {"sending": 1711329856.433393, "arrival": 1711329857.659147}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4331257, "arrival": 1711329857.660173}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4296167, "arrival": 1711329866.9537237}, "models": {"yolo": {"arrival": 1711329866.3541548, "serving": 1711329866.9386873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4292336, "arrival": 1711329864.832887}, "models": {"yolo": {"arrival": 1711329864.3376873, "serving": 1711329864.7897558}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4256356, "arrival": 1711329866.354895}, "models": {"yolo": {"arrival": 1711329865.9977942, "serving": 1711329866.3399577}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4596615, "arrival": 1711329868.3631303}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4332993, "arrival": 1711329867.555004}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4299483, "arrival": 1711329867.4958234}, "models": {"yolo": {"arrival": 1711329867.1608443, "serving": 1711329867.4258437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4614835, "arrival": 1711329857.659668}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4297934, "arrival": 1711329868.3523152}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.433027, "arrival": 1711329868.3598597}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.551619924+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329856.4616134}}, "outputs": []}, {"times": {"request": {"sending": 1711329856.4292955, "arrival": 1711329867.5424213}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4158442, "arrival": 1711329867.1718812}, "models": {"yolo": {"arrival": 1711329866.8306148, "serving": 1711329867.1484783}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4616468, "arrival": 1711329868.564111}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4615304, "arrival": 1711329867.19201}, "models": {"yolo": {"arrival": 1711329866.9518878, "serving": 1711329867.1698027}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4291732, "arrival": 1711329866.3573794}, "models": {"yolo": {"arrival": 1711329865.9977942, "serving": 1711329866.3399577}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.433057, "arrival": 1711329867.4971533}, "models": {"yolo": {"arrival": 1711329867.1608443, "serving": 1711329867.4258437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4301157, "arrival": 1711329868.5566084}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4332201, "arrival": 1711329867.1904356}, "models": {"yolo": {"arrival": 1711329866.9518878, "serving": 1711329867.1698027}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.461594, "arrival": 1711329867.8372808}, "models": {"yolo": {"arrival": 1711329867.439102, "serving": 1711329867.7974005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4301007, "arrival": 1711329867.554135}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.425714, "arrival": 1711329867.1770947}, "models": {"yolo": {"arrival": 1711329866.8306148, "serving": 1711329867.1484783}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4238713, "arrival": 1711329857.6414254}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4255571, "arrival": 1711329867.5410337}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4295702, "arrival": 1711329857.6457126}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4299645, "arrival": 1711329857.6506393}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.466269, "arrival": 1711329865.7035277}, "models": {"yolo": {"arrival": 1711329865.5380232, "serving": 1711329865.6657271}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4300256, "arrival": 1711329857.6574128}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4613068, "arrival": 1711329867.555777}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.106274845+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329856.4662325}}, "outputs": []}, {"times": {"request": {"sending": 1711329856.4299333, "arrival": 1711329864.8388946}, "models": {"yolo": {"arrival": 1711329864.3376873, "serving": 1711329864.7897558}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4331784, "arrival": 1711329857.6569264}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429538, "arrival": 1711329864.8345582}, "models": {"yolo": {"arrival": 1711329864.3376873, "serving": 1711329864.7897558}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4334311, "arrival": 1711329867.1912277}, "models": {"yolo": {"arrival": 1711329866.9518878, "serving": 1711329867.1698027}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4300702, "arrival": 1711329867.4966965}, "models": {"yolo": {"arrival": 1711329867.1608443, "serving": 1711329867.4258437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4616313, "arrival": 1711329867.5565095}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4160223, "arrival": 1711329867.172797}, "models": {"yolo": {"arrival": 1711329866.8306148, "serving": 1711329867.1484783}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4159126, "arrival": 1711329868.538158}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4301314, "arrival": 1711329866.9617226}, "models": {"yolo": {"arrival": 1711329866.3541548, "serving": 1711329866.9386873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.430205, "arrival": 1711329857.6516175}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4333253, "arrival": 1711329867.190843}, "models": {"yolo": {"arrival": 1711329866.9518878, "serving": 1711329867.1698027}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4256136, "arrival": 1711329868.540131}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4292536, "arrival": 1711329867.1777387}, "models": {"yolo": {"arrival": 1711329866.8306148, "serving": 1711329867.1484783}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429672, "arrival": 1711329864.838021}, "models": {"yolo": {"arrival": 1711329864.3376873, "serving": 1711329864.7897558}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429448, "arrival": 1711329867.5431597}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4300096, "arrival": 1711329866.9586747}, "models": {"yolo": {"arrival": 1711329866.3541548, "serving": 1711329866.9386873}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4333785, "arrival": 1711329867.8332996}, "models": {"yolo": {"arrival": 1711329867.439102, "serving": 1711329867.7974005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4332724, "arrival": 1711329867.8329275}, "models": {"yolo": {"arrival": 1711329867.439102, "serving": 1711329867.7974005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4294324, "arrival": 1711329857.6448581}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.433206, "arrival": 1711329868.558189}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4332461, "arrival": 1711329868.360961}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4331126, "arrival": 1711329867.1899652}, "models": {"yolo": {"arrival": 1711329866.9518878, "serving": 1711329867.1698027}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4296012, "arrival": 1711329868.5457585}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4300404, "arrival": 1711329868.3555145}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.433365, "arrival": 1711329865.5631988}, "models": {"yolo": {"arrival": 1711329864.8038635, "serving": 1711329865.5239131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4329197, "arrival": 1711329868.5571458}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4299948, "arrival": 1711329868.5477445}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4293149, "arrival": 1711329868.5417466}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.466252, "arrival": 1711329868.3719378}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4160445, "arrival": 1711329857.6411338}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4294627, "arrival": 1711329868.5453928}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4298089, "arrival": 1711329864.838571}, "models": {"yolo": {"arrival": 1711329864.3376873, "serving": 1711329864.7897558}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4331648, "arrival": 1711329867.4973726}, "models": {"yolo": {"arrival": 1711329867.1608443, "serving": 1711329867.4258437}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4160678, "arrival": 1711329867.5400743}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4613926, "arrival": 1711329867.1916254}, "models": {"yolo": {"arrival": 1711329866.9518878, "serving": 1711329867.1698027}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429333, "arrival": 1711329866.3600886}, "models": {"yolo": {"arrival": 1711329865.9977942, "serving": 1711329866.3399577}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4296358, "arrival": 1711329857.6503232}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4294918, "arrival": 1711329857.6500323}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.425656, "arrival": 1711329857.6443024}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4661744, "arrival": 1711329867.4912791}, "models": {"yolo": {"arrival": 1711329867.1828103, "serving": 1711329867.4203496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4297454, "arrival": 1711329868.5472004}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4256759, "arrival": 1711329868.3417108}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429841, "arrival": 1711329857.648932}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.423826, "arrival": 1711329864.3414032}, "models": {"yolo": {"arrival": 1711329864.1598735, "serving": 1711329864.3221502}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.416133, "arrival": 1711329857.6440246}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.433071, "arrival": 1711329857.6518662}, "models": {"yolo": {"arrival": 1711329856.557121, "serving": 1711329857.609689}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4297776, "arrival": 1711329857.6565814}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4299173, "arrival": 1711329868.35483}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4298568, "arrival": 1711329867.5592573}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4615617, "arrival": 1711329868.3715131}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.4614995, "arrival": 1711329867.5561247}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429902, "arrival": 1711329857.6571865}, "models": {"yolo": {"arrival": 1711329856.5496392, "serving": 1711329857.6109765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.433259, "arrival": 1711329865.562338}, "models": {"yolo": {"arrival": 1711329864.8038635, "serving": 1711329865.5239131}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329856.429151, "arrival": 1711329868.5413847}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329857.4094546, "arrival": 1711329867.556867}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.441588, "arrival": 1711329868.6240828}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:18.552025145+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.440608}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.495362, "arrival": 1711329867.9332898}, "models": {"yolo": {"arrival": 1711329867.6810167, "serving": 1711329867.9163632}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4095905, "arrival": 1711329868.5694664}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.409841, "arrival": 1711329867.837998}, "models": {"yolo": {"arrival": 1711329867.439102, "serving": 1711329867.7974005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4413528, "arrival": 1711329868.383237}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4406252, "arrival": 1711329867.5606947}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.106735092+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4399047}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.441286, "arrival": 1711329867.5616364}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4414454, "arrival": 1711329868.6235313}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.106302627+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4097176}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.409922, "arrival": 1711329867.557204}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.439934, "arrival": 1711329868.3772502}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4101164, "arrival": 1711329865.7106588}, "models": {"yolo": {"arrival": 1711329865.5380232, "serving": 1711329865.6657271}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4404294, "arrival": 1711329866.1925476}, "models": {"yolo": {"arrival": 1711329865.6815917, "serving": 1711329866.1701648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4411697, "arrival": 1711329868.6166751}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4099607, "arrival": 1711329868.5699632}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4417048, "arrival": 1711329867.9309385}, "models": {"yolo": {"arrival": 1711329867.6810167, "serving": 1711329867.9163632}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.409669, "arrival": 1711329867.491517}, "models": {"yolo": {"arrival": 1711329867.1828103, "serving": 1711329867.4203496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4415698, "arrival": 1711329867.6859236}, "models": {"yolo": {"arrival": 1711329867.442705, "serving": 1711329867.6726937}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.552113355+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4412699}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.115214666+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4417477}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4411871, "arrival": 1711329867.832559}, "models": {"yolo": {"arrival": 1711329867.432963, "serving": 1711329867.7878985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.441461, "arrival": 1711329868.0502381}, "models": {"yolo": {"arrival": 1711329867.796021, "serving": 1711329868.0351112}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4416327, "arrival": 1711329868.383851}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4097595, "arrival": 1711329868.3723364}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4409273, "arrival": 1711329868.615728}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.441324, "arrival": 1711329868.04743}, "models": {"yolo": {"arrival": 1711329867.796021, "serving": 1711329868.0351112}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4243748, "arrival": 1711329865.7113307}, "models": {"yolo": {"arrival": 1711329865.5380232, "serving": 1711329865.6657271}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4311926, "arrival": 1711329868.6112404}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4399917, "arrival": 1711329868.081124}, "models": {"yolo": {"arrival": 1711329867.81155, "serving": 1711329868.0458963}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4394727, "arrival": 1711329866.1821117}, "models": {"yolo": {"arrival": 1711329865.6815917, "serving": 1711329866.1701648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4416692, "arrival": 1711329869.1048625}, "models": {"yolo": {"arrival": 1711329868.7898872, "serving": 1711329869.0927074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.439797, "arrival": 1711329867.5590062}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4405515, "arrival": 1711329868.3781857}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4954445, "arrival": 1711329868.3850431}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4399624, "arrival": 1711329866.1886091}, "models": {"yolo": {"arrival": 1711329865.6815917, "serving": 1711329866.1701648}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.551873789+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4309864}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4405167, "arrival": 1711329867.820615}, "models": {"yolo": {"arrival": 1711329867.432963, "serving": 1711329867.7878985}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:18.551895936+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4311526}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.551981737+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4402761}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.430933, "arrival": 1711329866.1786604}, "models": {"yolo": {"arrival": 1711329865.6815917, "serving": 1711329866.1701648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4310892, "arrival": 1711329868.3746495}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.111713887+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4408197}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.106493658+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4310694}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4310281, "arrival": 1711329868.6178467}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.424518, "arrival": 1711329867.4944737}, "models": {"yolo": {"arrival": 1711329867.1828103, "serving": 1711329867.4203496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4402094, "arrival": 1711329866.189351}, "models": {"yolo": {"arrival": 1711329865.6815917, "serving": 1711329866.1701648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4953868, "arrival": 1711329868.62497}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4954042, "arrival": 1711329868.0531309}, "models": {"yolo": {"arrival": 1711329867.796021, "serving": 1711329868.0351112}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4098017, "arrival": 1711329865.7044508}, "models": {"yolo": {"arrival": 1711329865.5380232, "serving": 1711329865.6657271}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:18.555239788+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4415553}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.439285, "arrival": 1711329867.4949324}, "models": {"yolo": {"arrival": 1711329867.1828103, "serving": 1711329867.4203496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4311092, "arrival": 1711329866.1811886}, "models": {"yolo": {"arrival": 1711329865.6815917, "serving": 1711329866.1701648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4406648, "arrival": 1711329867.8279705}, "models": {"yolo": {"arrival": 1711329867.432963, "serving": 1711329867.7878985}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.111691114+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4406822}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.115155346+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4416187}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.551759014+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4098804}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4404101, "arrival": 1711329868.3778837}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.440352, "arrival": 1711329868.6099286}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.431049, "arrival": 1711329867.494707}, "models": {"yolo": {"arrival": 1711329867.1828103, "serving": 1711329867.4203496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.441426, "arrival": 1711329867.6850383}, "models": {"yolo": {"arrival": 1711329867.442705, "serving": 1711329867.6726937}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.551961102+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.440025}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.440839, "arrival": 1711329868.3821805}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.35451798+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4412346}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.55206965+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4408925}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.552092159+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4411347}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.111779902+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4413395}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.441389, "arrival": 1711329868.8046687}, "models": {"yolo": {"arrival": 1711329868.05909, "serving": 1711329868.7766857}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4398346, "arrival": 1711329868.6128795}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:18.551848491+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.424433}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4407873, "arrival": 1711329868.6150885}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4404984, "arrival": 1711329868.6182597}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4101543, "arrival": 1711329867.8383443}, "models": {"yolo": {"arrival": 1711329867.439102, "serving": 1711329867.7974005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4412158, "arrival": 1711329868.3828676}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:18.55529359+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.441687}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.111670449+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4405334}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.106386929+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4245465}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.44179, "arrival": 1711329869.1075864}, "models": {"yolo": {"arrival": 1711329868.7898872, "serving": 1711329869.0927074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4311295, "arrival": 1711329868.0790963}, "models": {"yolo": {"arrival": 1711329867.81155, "serving": 1711329868.0458963}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4401743, "arrival": 1711329868.3775702}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.551810332+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.410194}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:18.552136143+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4414089}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4311728, "arrival": 1711329867.5585136}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4395788, "arrival": 1711329867.5587604}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.106603821+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4396544}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4406455, "arrival": 1711329868.6144543}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.111802301+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.441484}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4099996, "arrival": 1711329867.491754}, "models": {"yolo": {"arrival": 1711329867.1828103, "serving": 1711329867.4203496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4408028, "arrival": 1711329867.831396}, "models": {"yolo": {"arrival": 1711329867.432963, "serving": 1711329867.7878985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4404793, "arrival": 1711329867.5604587}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.410078, "arrival": 1711329868.3727198}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.111735117+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4410584}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.106327318+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.410039}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4102328, "arrival": 1711329867.557596}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4402404, "arrival": 1711329868.7921503}, "models": {"yolo": {"arrival": 1711329868.05909, "serving": 1711329868.7766857}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.361249229+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4417758}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.555318709+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4952738}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4396784, "arrival": 1711329868.3768659}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4396238, "arrival": 1711329867.4951513}, "models": {"yolo": {"arrival": 1711329867.1828103, "serving": 1711329867.4203496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4405875, "arrival": 1711329868.795207}, "models": {"yolo": {"arrival": 1711329868.05909, "serving": 1711329868.7766857}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.552003453+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4404645}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4408727, "arrival": 1711329868.8002825}, "models": {"yolo": {"arrival": 1711329868.05909, "serving": 1711329868.7766857}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4417331, "arrival": 1711329868.0518112}, "models": {"yolo": {"arrival": 1711329867.796021, "serving": 1711329868.0351112}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:18.551938519+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4397645}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.111607059+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4401438}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4400558, "arrival": 1711329867.5599813}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.356451068+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.441521}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4409099, "arrival": 1711329867.5611677}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4954772, "arrival": 1711329869.1084483}, "models": {"yolo": {"arrival": 1711329868.7898872, "serving": 1711329869.0927074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.440769, "arrival": 1711329867.5609338}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.361307909+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4954607}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4395072, "arrival": 1711329868.0798557}, "models": {"yolo": {"arrival": 1711329867.81155, "serving": 1711329868.0458963}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.354445289+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4410944}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4102707, "arrival": 1711329868.590971}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4407356, "arrival": 1711329868.7977262}, "models": {"yolo": {"arrival": 1711329868.05909, "serving": 1711329868.7766857}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4397082, "arrival": 1711329866.1867547}, "models": {"yolo": {"arrival": 1711329865.6815917, "serving": 1711329866.1701648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.440377, "arrival": 1711329867.8199675}, "models": {"yolo": {"arrival": 1711329867.432963, "serving": 1711329867.7878985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4404457, "arrival": 1711329868.7943525}, "models": {"yolo": {"arrival": 1711329868.05909, "serving": 1711329868.7766857}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.441307, "arrival": 1711329868.6228595}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4409418, "arrival": 1711329867.8320172}, "models": {"yolo": {"arrival": 1711329867.432963, "serving": 1711329867.7878985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4394329, "arrival": 1711329868.3750713}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4417617, "arrival": 1711329868.384153}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4243398, "arrival": 1711329868.3731}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.106551655+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4393928}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.354554502+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4413722}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.439732, "arrival": 1711329868.0805163}, "models": {"yolo": {"arrival": 1711329867.81155, "serving": 1711329868.0458963}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.111757131+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4412017}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.106357245+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.424298}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.356517023+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.44165}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4398723, "arrival": 1711329867.818624}, "models": {"yolo": {"arrival": 1711329867.432963, "serving": 1711329867.7878985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4400902, "arrival": 1711329868.61379}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4244046, "arrival": 1711329868.063074}, "models": {"yolo": {"arrival": 1711329867.81155, "serving": 1711329868.0458963}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4244905, "arrival": 1711329868.6173458}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4309633, "arrival": 1711329868.0783098}, "models": {"yolo": {"arrival": 1711329867.81155, "serving": 1711329868.0458963}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4417195, "arrival": 1711329868.624544}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:18.551917657+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4395418}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.115241286+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4954274}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.440718, "arrival": 1711329866.1941154}, "models": {"yolo": {"arrival": 1711329865.6815917, "serving": 1711329866.1701648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.424194, "arrival": 1711329867.491988}, "models": {"yolo": {"arrival": 1711329867.1828103, "serving": 1711329867.4203496}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4401152, "arrival": 1711329867.8193107}, "models": {"yolo": {"arrival": 1711329867.432963, "serving": 1711329867.7878985}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4406996, "arrival": 1711329868.3817625}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4412508, "arrival": 1711329868.8037388}, "models": {"yolo": {"arrival": 1711329868.05909, "serving": 1711329868.7766857}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.44115, "arrival": 1711329867.5614045}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4416044, "arrival": 1711329868.0511386}, "models": {"yolo": {"arrival": 1711329867.796021, "serving": 1711329868.0351112}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4408557, "arrival": 1711329866.1948192}, "models": {"yolo": {"arrival": 1711329865.6815917, "serving": 1711329866.1701648}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4396017, "arrival": 1711329868.6121802}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4244633, "arrival": 1711329867.557939}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.558535315+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.4988246}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4415014, "arrival": 1711329868.383544}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.430844, "arrival": 1711329868.373455}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4403121, "arrival": 1711329867.5602224}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4415367, "arrival": 1711329868.8053212}, "models": {"yolo": {"arrival": 1711329868.05909, "serving": 1711329868.7766857}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4310074, "arrival": 1711329867.5582552}, "models": {"yolo": {"arrival": 1711329864.8175778, "serving": 1711329867.4073653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.441114, "arrival": 1711329868.801011}, "models": {"yolo": {"arrival": 1711329868.05909, "serving": 1711329868.7766857}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.111647231+00:00\"}\"\n>", "times": {"request": {"sending": 1711329857.4403946}}, "outputs": []}, {"times": {"request": {"sending": 1711329857.4410746, "arrival": 1711329868.3825395}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329857.4405696, "arrival": 1711329866.193298}, "models": {"yolo": {"arrival": 1711329865.6815917, "serving": 1711329866.1701648}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.552046395+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329857.440753}}, "outputs": []}], [{"times": {"request": {"sending": 1711329858.4153008, "arrival": 1711329867.9340203}, "models": {"yolo": {"arrival": 1711329867.6810167, "serving": 1711329867.9163632}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.506112371+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4608607}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.504916243+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4154947}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.538657647+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4173086}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4167001, "arrival": 1711329868.8970563}, "models": {"yolo": {"arrival": 1711329868.5193188, "serving": 1711329868.8833263}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.361370152+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4159338}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4163942, "arrival": 1711329868.6266046}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4157395, "arrival": 1711329867.934666}, "models": {"yolo": {"arrival": 1711329867.6810167, "serving": 1711329867.9163632}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.361507026+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4173584}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.52861186+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4161627}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4165854, "arrival": 1711329869.609762}, "models": {"yolo": {"arrival": 1711329869.1072714, "serving": 1711329869.58855}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4173827, "arrival": 1711329869.6111398}, "models": {"yolo": {"arrival": 1711329869.1072714, "serving": 1711329869.58855}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.503303513+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.416623}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4154398, "arrival": 1711329868.6253612}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4402945, "arrival": 1711329868.3915713}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4880323, "arrival": 1711329870.1975772}, "models": {"yolo": {"arrival": 1711329869.8439093, "serving": 1711329870.1849847}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4470448, "arrival": 1711329869.1314821}, "models": {"yolo": {"arrival": 1711329868.8919752, "serving": 1711329869.1106749}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4600234, "arrival": 1711329869.8520288}, "models": {"yolo": {"arrival": 1711329869.6005409, "serving": 1711329869.8335168}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.367302996+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4471068}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4162776, "arrival": 1711329869.1144693}, "models": {"yolo": {"arrival": 1711329868.7898872, "serving": 1711329869.0927074}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.505757469+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.447301}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.503350345+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4174075}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.538919226+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4602597}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.447091, "arrival": 1711329868.538589}, "models": {"yolo": {"arrival": 1711329868.3386788, "serving": 1711329868.4989965}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.507036906+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4401698}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:18.561297937+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 23.64 GiB total capacity; 2.38 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4156985}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4471226, "arrival": 1711329869.6191344}, "models": {"yolo": {"arrival": 1711329869.1072714, "serving": 1711329869.58855}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4168143, "arrival": 1711329868.3898735}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4155803, "arrival": 1711329868.385389}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4163558, "arrival": 1711329867.9383123}, "models": {"yolo": {"arrival": 1711329867.6810167, "serving": 1711329867.9163632}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4603627, "arrival": 1711329869.5955064}, "models": {"yolo": {"arrival": 1711329869.1198645, "serving": 1711329869.5728674}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.528517589+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4155397}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.503326404+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4169366}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.515088318+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.460946}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.528659637+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4167762}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4156594, "arrival": 1711329869.1098433}, "models": {"yolo": {"arrival": 1711329868.7898872, "serving": 1711329869.0927074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.415971, "arrival": 1711329869.1132286}, "models": {"yolo": {"arrival": 1711329868.7898872, "serving": 1711329869.0927074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4166622, "arrival": 1711329867.9405458}, "models": {"yolo": {"arrival": 1711329867.6810167, "serving": 1711329867.9163632}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.506171994+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.460991}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.528636369+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4164712}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.503273619+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4163153}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.538800697+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4468124}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.528584627+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4158561}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4157782, "arrival": 1711329868.6257508}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.50511398+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4161248}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.361444727+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4165478}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.539011255+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4608774}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.505172743+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4164324}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.503215574+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.41601}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4165087, "arrival": 1711329868.3896222}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4162023, "arrival": 1711329868.3893712}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.364705706+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.440112}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4174368, "arrival": 1711329868.0846243}, "models": {"yolo": {"arrival": 1711329867.9290814, "serving": 1711329868.05614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.361406855+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4162395}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.507134406+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4468846}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.361480236+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4168518}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4608035, "arrival": 1711329869.858491}, "models": {"yolo": {"arrival": 1711329869.6005409, "serving": 1711329869.8335168}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.361334562+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.41562}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.538848894+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4470754}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.505051023+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4158158}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4606586, "arrival": 1711329869.0201042}, "models": {"yolo": {"arrival": 1711329868.508836, "serving": 1711329869.0081499}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4169762, "arrival": 1711329867.9415343}, "models": {"yolo": {"arrival": 1711329867.6810167, "serving": 1711329867.9163632}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.505584558+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4467926}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.417462, "arrival": 1711329868.9004843}, "models": {"yolo": {"arrival": 1711329868.5193188, "serving": 1711329868.8833263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4158957, "arrival": 1711329868.3890367}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.505312796+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4172819}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.515065077+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4608183}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4608958, "arrival": 1711329869.0330482}, "models": {"yolo": {"arrival": 1711329868.508836, "serving": 1711329869.0081499}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4173336, "arrival": 1711329868.3901103}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4160879, "arrival": 1711329868.6261556}, "models": {"yolo": {"arrival": 1711329866.3784912, "serving": 1711329868.4862812}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4607315, "arrival": 1711329869.5979948}, "models": {"yolo": {"arrival": 1711329869.1198645, "serving": 1711329869.5728674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.417584, "arrival": 1711329869.6151116}, "models": {"yolo": {"arrival": 1711329869.1072714, "serving": 1711329869.58855}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4160502, "arrival": 1711329867.9375958}, "models": {"yolo": {"arrival": 1711329867.6810167, "serving": 1711329867.9163632}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4176333, "arrival": 1711329868.085035}, "models": {"yolo": {"arrival": 1711329867.9290814, "serving": 1711329868.05614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4606123, "arrival": 1711329869.597238}, "models": {"yolo": {"arrival": 1711329869.1198645, "serving": 1711329869.5728674}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.505393204+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.417487}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.505232201+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4167373}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.416898, "arrival": 1711329869.6105256}, "models": {"yolo": {"arrival": 1711329869.1072714, "serving": 1711329869.58855}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4172542, "arrival": 1711329868.8997438}, "models": {"yolo": {"arrival": 1711329868.5193188, "serving": 1711329868.8833263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4608457, "arrival": 1711329869.604347}, "models": {"yolo": {"arrival": 1711329869.1198645, "serving": 1711329869.5728674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4175353, "arrival": 1711329868.3903444}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.372090963+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4602962}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.375622807+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4606726}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4400713, "arrival": 1711329868.390588}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4607146, "arrival": 1711329868.9340987}, "models": {"yolo": {"arrival": 1711329868.070488, "serving": 1711329868.9081762}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.507106718+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.44037}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.460963, "arrival": 1711329868.9366388}, "models": {"yolo": {"arrival": 1711329868.070488, "serving": 1711329868.9081762}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.505993188+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.460628}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.460144, "arrival": 1711329868.5393183}, "models": {"yolo": {"arrival": 1711329868.3386788, "serving": 1711329868.4989965}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4470272, "arrival": 1711329868.0877337}, "models": {"yolo": {"arrival": 1711329867.9290814, "serving": 1711329868.05614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4472847, "arrival": 1711329869.1328719}, "models": {"yolo": {"arrival": 1711329868.8919752, "serving": 1711329869.1106749}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.514978619+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.460463}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.50547231+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4176807}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4468288, "arrival": 1711329868.391863}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4402218, "arrival": 1711329869.1241345}, "models": {"yolo": {"arrival": 1711329868.8919752, "serving": 1711329869.1106749}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4176571, "arrival": 1711329868.9011142}, "models": {"yolo": {"arrival": 1711329868.5193188, "serving": 1711329868.8833263}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4604452, "arrival": 1711329869.857137}, "models": {"yolo": {"arrival": 1711329869.6005409, "serving": 1711329869.8335168}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4467702, "arrival": 1711329869.126227}, "models": {"yolo": {"arrival": 1711329868.8919752, "serving": 1711329869.1106749}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.505813189+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4601052}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.36155547+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4175587}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4604115, "arrival": 1711329869.0191326}, "models": {"yolo": {"arrival": 1711329868.508836, "serving": 1711329869.0081499}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.538779262+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4402719}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4403467, "arrival": 1711329869.616367}, "models": {"yolo": {"arrival": 1711329869.1072714, "serving": 1711329869.58855}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.50337669+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4176087}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.505936544+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4603782}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.515037242+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4607005}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4609773, "arrival": 1711329869.6051395}, "models": {"yolo": {"arrival": 1711329869.1198645, "serving": 1711329869.5728674}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.506050927+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4607463}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.460313, "arrival": 1711329869.854987}, "models": {"yolo": {"arrival": 1711329869.6005409, "serving": 1711329869.8335168}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4401464, "arrival": 1711329869.6157944}, "models": {"yolo": {"arrival": 1711329869.1072714, "serving": 1711329869.58855}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4607751, "arrival": 1711329869.0320973}, "models": {"yolo": {"arrival": 1711329868.508836, "serving": 1711329869.0081499}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.365934332+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.44698}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4602768, "arrival": 1711329869.0178158}, "models": {"yolo": {"arrival": 1711329868.508836, "serving": 1711329869.0081499}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.538896081+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4601257}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.538965214+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4606433}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.380465187+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4607894}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.446917, "arrival": 1711329869.1305315}, "models": {"yolo": {"arrival": 1711329868.8919752, "serving": 1711329869.1106749}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4609294, "arrival": 1711329870.1967592}, "models": {"yolo": {"arrival": 1711329869.8439093, "serving": 1711329870.1849847}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.538724985+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.417511}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.541417479+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4610052}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4468677, "arrival": 1711329869.6176257}, "models": {"yolo": {"arrival": 1711329869.1072714, "serving": 1711329869.58855}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4601738, "arrival": 1711329869.8541605}, "models": {"yolo": {"arrival": 1711329869.6005409, "serving": 1711329869.8335168}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4600863, "arrival": 1711329869.5924685}, "models": {"yolo": {"arrival": 1711329869.1198645, "serving": 1711329869.5728674}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.515110609+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4886947}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.51084583+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4471385}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.371448302+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4601593}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.37137621+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.460004}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4472687, "arrival": 1711329868.088209}, "models": {"yolo": {"arrival": 1711329867.9290814, "serving": 1711329868.05614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.510922279+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.460331}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.510787152+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.447011}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.50564018+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4469323}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.460591, "arrival": 1711329868.927344}, "models": {"yolo": {"arrival": 1711329868.070488, "serving": 1711329868.9081762}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.364750071+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4403214}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.510897545+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4601936}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.460686, "arrival": 1711329869.8578284}, "models": {"yolo": {"arrival": 1711329869.6005409, "serving": 1711329869.8335168}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.446694, "arrival": 1711329868.0858562}, "models": {"yolo": {"arrival": 1711329867.9290814, "serving": 1711329868.05614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4610198, "arrival": 1711329869.0357053}, "models": {"yolo": {"arrival": 1711329868.508836, "serving": 1711329869.0081499}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.505700004+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4470606}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.538872349+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4598966}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.538987977+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4607613}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4469635, "arrival": 1711329868.3920987}, "models": {"yolo": {"arrival": 1711329866.0286176, "serving": 1711329868.3049006}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.460212, "arrival": 1711329868.9259138}, "models": {"yolo": {"arrival": 1711329868.070488, "serving": 1711329868.9081762}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.51087312+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4600422}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4608316, "arrival": 1711329868.9349713}, "models": {"yolo": {"arrival": 1711329868.070488, "serving": 1711329868.9081762}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4603472, "arrival": 1711329868.9266953}, "models": {"yolo": {"arrival": 1711329868.070488, "serving": 1711329868.9081762}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.364773403+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4468472}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.373749034+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4604287}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.538823502+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4469476}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.382205595+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.4609132}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.387889554+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4610353}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4602292, "arrival": 1711329869.5947022}, "models": {"yolo": {"arrival": 1711329869.1198645, "serving": 1711329869.5728674}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.538756444+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.439956}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.460059, "arrival": 1711329868.923595}, "models": {"yolo": {"arrival": 1711329868.070488, "serving": 1711329868.9081762}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.538943137+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4603958}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4469008, "arrival": 1711329868.087286}, "models": {"yolo": {"arrival": 1711329867.9290814, "serving": 1711329868.05614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.505527667+00:00\"}\"\n>", "times": {"request": {"sending": 1711329858.4402468}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4401932, "arrival": 1711329868.085431}, "models": {"yolo": {"arrival": 1711329867.9290814, "serving": 1711329868.05614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.50587026+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329858.460244}}, "outputs": []}, {"times": {"request": {"sending": 1711329858.4469957, "arrival": 1711329869.6181788}, "models": {"yolo": {"arrival": 1711329869.1072714, "serving": 1711329869.58855}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329858.4599812, "arrival": 1711329868.5389168}, "models": {"yolo": {"arrival": 1711329868.3386788, "serving": 1711329868.4989965}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329859.4394588, "arrival": 1711329869.835383}, "models": {"yolo": {"arrival": 1711329869.5843687, "serving": 1711329869.8157547}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4653225, "arrival": 1711329869.689885}, "models": {"yolo": {"arrival": 1711329869.0401802, "serving": 1711329869.6689813}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.398331593+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4391494}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.426463741+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4399247}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.441444138+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.464993}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4387352, "arrival": 1711329869.03675}, "models": {"yolo": {"arrival": 1711329868.508836, "serving": 1711329869.0081499}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.388138953+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4649513}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.506632197+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4393523}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4868004, "arrival": 1711329870.6542563}, "models": {"yolo": {"arrival": 1711329870.3394654, "serving": 1711329870.4492536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4654455, "arrival": 1711329869.69063}, "models": {"yolo": {"arrival": 1711329869.0401802, "serving": 1711329869.6689813}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4386787, "arrival": 1711329869.6060092}, "models": {"yolo": {"arrival": 1711329869.1198645, "serving": 1711329869.5728674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4654157, "arrival": 1711329870.9183125}, "models": {"yolo": {"arrival": 1711329870.479672, "serving": 1711329870.8728027}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.41037985+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4395213}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.438822, "arrival": 1711329869.827554}, "models": {"yolo": {"arrival": 1711329869.5843687, "serving": 1711329869.8157547}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4389536, "arrival": 1711329869.8298035}, "models": {"yolo": {"arrival": 1711329869.5843687, "serving": 1711329869.8157547}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.506230258+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4387}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.486655, "arrival": 1711329869.6936827}, "models": {"yolo": {"arrival": 1711329869.0401802, "serving": 1711329869.6689813}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4388065, "arrival": 1711329868.9408662}, "models": {"yolo": {"arrival": 1711329868.070488, "serving": 1711329868.9081762}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.438604, "arrival": 1711329868.9377174}, "models": {"yolo": {"arrival": 1711329868.070488, "serving": 1711329868.9081762}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.525166014+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4866855}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4867444, "arrival": 1711329870.9264226}, "models": {"yolo": {"arrival": 1711329870.479672, "serving": 1711329870.8728027}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4650457, "arrival": 1711329869.6863122}, "models": {"yolo": {"arrival": 1711329869.0401802, "serving": 1711329869.6689813}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.439544, "arrival": 1711329870.6584063}, "models": {"yolo": {"arrival": 1711329870.1956108, "serving": 1711329870.468765}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.089369412+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.5042763}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.081787804+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4397094}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.506569659+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4392276}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.088268691+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4654307}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.464864, "arrival": 1711329870.6593246}, "models": {"yolo": {"arrival": 1711329870.1956108, "serving": 1711329870.468765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.486445, "arrival": 1711329869.69301}, "models": {"yolo": {"arrival": 1711329869.0401802, "serving": 1711329869.6689813}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4650612, "arrival": 1711329870.362062}, "models": {"yolo": {"arrival": 1711329869.8265502, "serving": 1711329870.3309264}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.390740863+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4387527}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4393213, "arrival": 1711329869.0399547}, "models": {"yolo": {"arrival": 1711329868.9201746, "serving": 1711329869.02682}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4391332, "arrival": 1711329869.3810108}, "models": {"yolo": {"arrival": 1711329869.0181525, "serving": 1711329869.370726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4391649, "arrival": 1711329870.2042067}, "models": {"yolo": {"arrival": 1711329869.8439093, "serving": 1711329870.1849847}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4401925, "arrival": 1711329869.682518}, "models": {"yolo": {"arrival": 1711329869.0401802, "serving": 1711329869.6689813}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.388045738+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4397717}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.46501, "arrival": 1711329870.915887}, "models": {"yolo": {"arrival": 1711329870.479672, "serving": 1711329870.8728027}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.506322354+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4388382}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:19.541477545+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4387186}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4651928, "arrival": 1711329870.3626242}, "models": {"yolo": {"arrival": 1711329869.8265502, "serving": 1711329870.3309264}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.537520467+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4868171}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.439787, "arrival": 1711329869.7688606}, "models": {"yolo": {"arrival": 1711329869.3807964, "serving": 1711329869.7475889}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4395053, "arrival": 1711329869.7628295}, "models": {"yolo": {"arrival": 1711329869.3807964, "serving": 1711329869.7475889}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.473016258+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4868712}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4651098, "arrival": 1711329870.1489184}, "models": {"yolo": {"arrival": 1711329869.7566824, "serving": 1711329870.1343973}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4868884, "arrival": 1711329871.1824036}, "models": {"yolo": {"arrival": 1711329870.8829782, "serving": 1711329871.1624537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.486526, "arrival": 1711329870.6537237}, "models": {"yolo": {"arrival": 1711329870.3394654, "serving": 1711329870.4492536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4396935, "arrival": 1711329870.6586401}, "models": {"yolo": {"arrival": 1711329870.1956108, "serving": 1711329870.468765}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.388227225+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4653678}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.48678, "arrival": 1711329869.6942568}, "models": {"yolo": {"arrival": 1711329869.0401802, "serving": 1711329869.6689813}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.374133535+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4389863}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4652455, "arrival": 1711329870.1518679}, "models": {"yolo": {"arrival": 1711329869.7566824, "serving": 1711329870.1343973}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.419773506+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.439678}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4866216, "arrival": 1711329870.9260292}, "models": {"yolo": {"arrival": 1711329870.479672, "serving": 1711329870.8728027}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4389033, "arrival": 1711329870.201297}, "models": {"yolo": {"arrival": 1711329869.8439093, "serving": 1711329870.1849847}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.394883729+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4388864}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.452059728+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4654002}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.081670107+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.43905}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4387698, "arrival": 1711329870.1983624}, "models": {"yolo": {"arrival": 1711329869.8439093, "serving": 1711329870.1849847}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.388107954+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4402354}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4398644, "arrival": 1711329870.3586316}, "models": {"yolo": {"arrival": 1711329869.8265502, "serving": 1711329870.3309264}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4868536, "arrival": 1711329870.654778}, "models": {"yolo": {"arrival": 1711329870.1425407, "serving": 1711329870.4518704}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.46546, "arrival": 1711329870.6531787}, "models": {"yolo": {"arrival": 1711329870.3394654, "serving": 1711329870.4492536}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.38816877+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4650915}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.388015068+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4396303}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4869318, "arrival": 1711329871.0896697}, "models": {"yolo": {"arrival": 1711329870.4617472, "serving": 1711329871.065911}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.464902, "arrival": 1711329869.6832335}, "models": {"yolo": {"arrival": 1711329869.0401802, "serving": 1711329869.6689813}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.466995216+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4866054}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.388314991+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4866998}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.088236041+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4653027}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.472975455+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4867296}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.08829073+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4655514}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.081535407+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.438788}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4653847, "arrival": 1711329870.1527452}, "models": {"yolo": {"arrival": 1711329869.7566824, "serving": 1711329870.1343973}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.374203641+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4391167}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4652843, "arrival": 1711329870.917737}, "models": {"yolo": {"arrival": 1711329870.479672, "serving": 1711329870.8728027}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4392595, "arrival": 1711329869.382335}, "models": {"yolo": {"arrival": 1711329869.0181525, "serving": 1711329869.370726}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.081621348+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4389198}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.088137581+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.465029}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.421265031+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4398024}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4397252, "arrival": 1711329869.6797752}, "models": {"yolo": {"arrival": 1711329869.0401802, "serving": 1711329869.6689813}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4389374, "arrival": 1711329868.9417913}, "models": {"yolo": {"arrival": 1711329868.070488, "serving": 1711329868.9081762}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4647648, "arrival": 1711329869.7699277}, "models": {"yolo": {"arrival": 1711329869.3807964, "serving": 1711329869.7475889}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4393816, "arrival": 1711329869.3840039}, "models": {"yolo": {"arrival": 1711329869.0181525, "serving": 1711329869.370726}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.088356788+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4869027}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4651475, "arrival": 1711329870.9165044}, "models": {"yolo": {"arrival": 1711329870.479672, "serving": 1711329870.8728027}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4393365, "arrival": 1711329869.834131}, "models": {"yolo": {"arrival": 1711329869.5843687, "serving": 1711329869.8157547}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.504236, "arrival": 1711329867.1783519}, "models": {"yolo": {"arrival": 1711329866.491513, "serving": 1711329867.1525996}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.088334963+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.486764}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.506816963+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4397564}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523207683+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.465353}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.439443, "arrival": 1711329869.0417428}, "models": {"yolo": {"arrival": 1711329868.9201746, "serving": 1711329869.02682}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.43887, "arrival": 1711329869.0374205}, "models": {"yolo": {"arrival": 1711329868.508836, "serving": 1711329869.0081499}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4398468, "arrival": 1711329869.680656}, "models": {"yolo": {"arrival": 1711329869.0401802, "serving": 1711329869.6689813}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.081833828+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4401731}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.374230905+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4392438}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4396005, "arrival": 1711329870.3528907}, "models": {"yolo": {"arrival": 1711329869.8265502, "serving": 1711329870.3309264}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.506386087+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4389699}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.506686718+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4394739}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.439661, "arrival": 1711329869.7636206}, "models": {"yolo": {"arrival": 1711329869.3807964, "serving": 1711329869.7475889}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.081811173+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4398327}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.506496762+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.439101}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.406678233+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4393961}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.486918, "arrival": 1711329869.7789733}, "models": {"yolo": {"arrival": 1711329869.6780932, "serving": 1711329869.764323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.464974, "arrival": 1711329869.7715442}, "models": {"yolo": {"arrival": 1711329869.3807964, "serving": 1711329869.7475889}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.402093648+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4392748}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.506908294+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4398797}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.398297176+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4390182}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.506748907+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.439615}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4399402, "arrival": 1711329870.6590986}, "models": {"yolo": {"arrival": 1711329870.1956108, "serving": 1711329870.468765}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.461628702+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4655213}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4398174, "arrival": 1711329870.6588655}, "models": {"yolo": {"arrival": 1711329870.1956108, "serving": 1711329870.468765}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.388285875+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4865704}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 23.64 GiB total capacity; 2.54 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:19.544206815+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.438854}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.0817179+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4393063}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.088312711+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.486637}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.388358762+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4868352}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4394107, "arrival": 1711329870.658166}, "models": {"yolo": {"arrival": 1711329870.1956108, "serving": 1711329870.468765}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.537584721+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.5041163}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.524942936+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4654748}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4397407, "arrival": 1711329870.3577669}, "models": {"yolo": {"arrival": 1711329869.8265502, "serving": 1711329870.3309264}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.387903277+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.439367}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4651775, "arrival": 1711329869.687958}, "models": {"yolo": {"arrival": 1711329869.0401802, "serving": 1711329869.6689813}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.5231524+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4650755}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.08176469+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4395595}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.464919, "arrival": 1711329870.3600156}, "models": {"yolo": {"arrival": 1711329869.8265502, "serving": 1711329870.3309264}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523085486+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4649348}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.388256212+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4654891}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4655364, "arrival": 1711329870.9256425}, "models": {"yolo": {"arrival": 1711329870.479672, "serving": 1711329870.8728027}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.445649797+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.465265}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4390345, "arrival": 1711329870.2019138}, "models": {"yolo": {"arrival": 1711329869.8439093, "serving": 1711329870.1849847}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.081695216+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4391804}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4655056, "arrival": 1711329870.1534305}, "models": {"yolo": {"arrival": 1711329869.7566824, "serving": 1711329870.1343973}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4399095, "arrival": 1711329869.7695167}, "models": {"yolo": {"arrival": 1711329869.3807964, "serving": 1711329869.7475889}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.387978653+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4394894}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4402068, "arrival": 1711329870.3593838}, "models": {"yolo": {"arrival": 1711329869.8265502, "serving": 1711329870.3309264}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.439003, "arrival": 1711329869.3788111}, "models": {"yolo": {"arrival": 1711329869.0181525, "serving": 1711329869.370726}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.5042539, "arrival": 1711329871.1831367}, "models": {"yolo": {"arrival": 1711329870.8829782, "serving": 1711329871.1624537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.388392689+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.504191}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4392116, "arrival": 1711329869.8334236}, "models": {"yolo": {"arrival": 1711329869.5843687, "serving": 1711329869.8157547}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.439084, "arrival": 1711329869.8308299}, "models": {"yolo": {"arrival": 1711329869.5843687, "serving": 1711329869.8157547}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4867144, "arrival": 1711329870.6545303}, "models": {"yolo": {"arrival": 1711329870.1425407, "serving": 1711329870.4518704}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.388076442+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4398942}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.19 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.38819871+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4652252}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.48667, "arrival": 1711329870.6539927}, "models": {"yolo": {"arrival": 1711329870.3394654, "serving": 1711329870.4492536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.4390666, "arrival": 1711329868.942322}, "models": {"yolo": {"arrival": 1711329868.070488, "serving": 1711329868.9081762}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.433590984+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4648411}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4392908, "arrival": 1711329870.2047908}, "models": {"yolo": {"arrival": 1711329869.8439093, "serving": 1711329870.1849847}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.445610994+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 748.00 MiB (GPU 0; 23.64 GiB total capacity; 1.85 GiB already allocated; 16.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4651277}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.088187983+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4651625}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4391959, "arrival": 1711329869.0392356}, "models": {"yolo": {"arrival": 1711329868.9201746, "serving": 1711329869.02682}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.088075622+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.464883}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4395857, "arrival": 1711329869.0425398}, "models": {"yolo": {"arrival": 1711329868.9201746, "serving": 1711329869.02682}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523185868+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.465208}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 23.64 GiB total capacity; 2.35 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.081741+00:00\"}\"\n>", "times": {"request": {"sending": 1711329859.4394255}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.525134005+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4865496}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4653378, "arrival": 1711329870.3630283}, "models": {"yolo": {"arrival": 1711329869.8265502, "serving": 1711329870.3309264}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.506976958+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329859.4402204}}, "outputs": []}, {"times": {"request": {"sending": 1711329859.4865873, "arrival": 1711329870.1579437}, "models": {"yolo": {"arrival": 1711329869.7566824, "serving": 1711329870.1343973}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329859.5042157, "arrival": 1711329870.655021}, "models": {"yolo": {"arrival": 1711329870.1425407, "serving": 1711329870.4518704}}}, "model_name": "yolo", "outputs": []}], [{"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.589783075+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4188163}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4188657, "arrival": 1711329870.9187174}, "models": {"yolo": {"arrival": 1711329870.4641113, "serving": 1711329870.8726528}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.603133216+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4178762}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4367657, "arrival": 1711329867.478723}, "models": {"yolo": {"arrival": 1711329867.1665423, "serving": 1711329867.4033976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4406393, "arrival": 1711329867.8176537}, "models": {"yolo": {"arrival": 1711329867.4163926, "serving": 1711329867.7873971}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.456123, "arrival": 1711329869.393249}, "models": {"yolo": {"arrival": 1711329868.9977741, "serving": 1711329869.3779259}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4369388, "arrival": 1711329870.6027958}, "models": {"yolo": {"arrival": 1711329869.7845132, "serving": 1711329870.3872993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4171584, "arrival": 1711329871.183566}, "models": {"yolo": {"arrival": 1711329870.8829782, "serving": 1711329871.1624537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.610894922+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4177783}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.610987497+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4189394}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4180243, "arrival": 1711329871.0987663}, "models": {"yolo": {"arrival": 1711329870.4617472, "serving": 1711329871.065911}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.417728, "arrival": 1711329867.1857693}, "models": {"yolo": {"arrival": 1711329866.491513, "serving": 1711329867.1525996}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4175568, "arrival": 1711329871.1990142}, "models": {"yolo": {"arrival": 1711329870.8829782, "serving": 1711329871.1624537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4366853, "arrival": 1711329870.6021402}, "models": {"yolo": {"arrival": 1711329869.7845132, "serving": 1711329870.3872993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4170156, "arrival": 1711329871.0923154}, "models": {"yolo": {"arrival": 1711329870.4617472, "serving": 1711329871.065911}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.55987629+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4178517}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4179254, "arrival": 1711329867.1863143}, "models": {"yolo": {"arrival": 1711329866.491513, "serving": 1711329867.1525996}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4173338, "arrival": 1711329867.1808858}, "models": {"yolo": {"arrival": 1711329866.491513, "serving": 1711329867.1525996}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4182932, "arrival": 1711329870.9152563}, "models": {"yolo": {"arrival": 1711329870.4641113, "serving": 1711329870.8726528}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.617442431+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4404821}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.617507517+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.440858}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4409506, "arrival": 1711329871.9625723}, "models": {"yolo": {"arrival": 1711329871.1795366, "serving": 1711329871.9339647}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.610913806+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4179735}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4408274, "arrival": 1711329871.962309}, "models": {"yolo": {"arrival": 1711329871.1795366, "serving": 1711329871.9339647}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.60295037+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.417183}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4171066, "arrival": 1711329870.6554708}, "models": {"yolo": {"arrival": 1711329870.1425407, "serving": 1711329870.4518704}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4174092, "arrival": 1711329869.784714}, "models": {"yolo": {"arrival": 1711329869.6780932, "serving": 1711329869.764323}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.537627283+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4170504}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4169383, "arrival": 1711329869.7837324}, "models": {"yolo": {"arrival": 1711329869.6780932, "serving": 1711329869.764323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4188905, "arrival": 1711329867.4389307}, "models": {"yolo": {"arrival": 1711329867.1665423, "serving": 1711329867.4033976}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.570280125+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4180481}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4185243, "arrival": 1711329867.1876972}, "models": {"yolo": {"arrival": 1711329866.491513, "serving": 1711329867.1525996}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.617419311+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4403389}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4366152, "arrival": 1711329871.1728191}, "models": {"yolo": {"arrival": 1711329870.8795004, "serving": 1711329871.1536636}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4175324, "arrival": 1711329867.1816044}, "models": {"yolo": {"arrival": 1711329866.491513, "serving": 1711329867.1525996}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4185486, "arrival": 1711329871.7113779}, "models": {"yolo": {"arrival": 1711329871.1728, "serving": 1711329871.6913738}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4409056, "arrival": 1711329872.2675326}, "models": {"yolo": {"arrival": 1711329871.9335034, "serving": 1711329872.2379665}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.610970584+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.418572}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4406857, "arrival": 1711329870.8130975}, "models": {"yolo": {"arrival": 1711329870.3982818, "serving": 1711329870.7941048}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4578667, "arrival": 1711329873.0415297}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4185002, "arrival": 1711329870.9171221}, "models": {"yolo": {"arrival": 1711329870.4641113, "serving": 1711329870.8726528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.418318, "arrival": 1711329867.1871324}, "models": {"yolo": {"arrival": 1711329866.491513, "serving": 1711329867.1525996}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.617336075+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.436734}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.43675, "arrival": 1711329871.1745913}, "models": {"yolo": {"arrival": 1711329870.8795004, "serving": 1711329871.1536636}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.610931821+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4181697}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4177039, "arrival": 1711329870.9130116}, "models": {"yolo": {"arrival": 1711329870.4641113, "serving": 1711329870.8726528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4181206, "arrival": 1711329867.1867301}, "models": {"yolo": {"arrival": 1711329866.491513, "serving": 1711329867.1525996}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4172094, "arrival": 1711329869.7844455}, "models": {"yolo": {"arrival": 1711329869.6780932, "serving": 1711329869.764323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.417753, "arrival": 1711329871.1996212}, "models": {"yolo": {"arrival": 1711329870.8829782, "serving": 1711329871.1624537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4403942, "arrival": 1711329871.9443552}, "models": {"yolo": {"arrival": 1711329871.699936, "serving": 1711329871.9243057}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.436529, "arrival": 1711329871.191342}, "models": {"yolo": {"arrival": 1711329871.0735512, "serving": 1711329871.166539}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.610857133+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.435324}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.610795524+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4182687}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4181452, "arrival": 1711329871.7093527}, "models": {"yolo": {"arrival": 1711329871.1728, "serving": 1711329871.6913738}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.436633, "arrival": 1711329867.4397957}, "models": {"yolo": {"arrival": 1711329867.1665423, "serving": 1711329867.4033976}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.581930693+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.418452}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.602762635+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4170787}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.611005358+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4353948}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.602996363+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4173832}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.55425639+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4176557}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4173586, "arrival": 1711329871.1982436}, "models": {"yolo": {"arrival": 1711329870.8829782, "serving": 1711329871.1624537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4367015, "arrival": 1711329871.1922228}, "models": {"yolo": {"arrival": 1711329871.0735512, "serving": 1711329871.166539}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4187622, "arrival": 1711329870.6011665}, "models": {"yolo": {"arrival": 1711329869.7845132, "serving": 1711329870.3872993}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.610951914+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4183671}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.440576, "arrival": 1711329871.9608443}, "models": {"yolo": {"arrival": 1711329871.1795366, "serving": 1711329871.9339647}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4561417, "arrival": 1711329861.9200485}, "models": {"yolo": {"arrival": 1711329861.4421902, "serving": 1711329861.901113}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.603983041+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4365773}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4179008, "arrival": 1711329870.913895}, "models": {"yolo": {"arrival": 1711329870.4641113, "serving": 1711329870.8726528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4404335, "arrival": 1711329870.603117}, "models": {"yolo": {"arrival": 1711329869.7845132, "serving": 1711329870.3872993}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.617596717+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4405444}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.617574267+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.440411}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.610743306+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4180722}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4181955, "arrival": 1711329870.453828}, "models": {"yolo": {"arrival": 1711329869.7845132, "serving": 1711329870.3872993}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.602894946+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4172838}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.418, "arrival": 1711329870.4152544}, "models": {"yolo": {"arrival": 1711329869.7845132, "serving": 1711329870.3872993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4178276, "arrival": 1711329871.0982425}, "models": {"yolo": {"arrival": 1711329870.4617472, "serving": 1711329871.065911}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4178038, "arrival": 1711329870.414762}, "models": {"yolo": {"arrival": 1711329869.7845132, "serving": 1711329870.3872993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4407494, "arrival": 1711329873.039313}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4174337, "arrival": 1711329871.0935743}, "models": {"yolo": {"arrival": 1711329870.4617472, "serving": 1711329871.065911}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.610818051+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.418476}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4189148, "arrival": 1711329871.7134593}, "models": {"yolo": {"arrival": 1711329871.1728, "serving": 1711329871.6913738}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4171326, "arrival": 1711329867.179132}, "models": {"yolo": {"arrival": 1711329866.491513, "serving": 1711329867.1525996}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.554152507+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.417259}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4352171, "arrival": 1711329871.1013086}, "models": {"yolo": {"arrival": 1711329870.4617472, "serving": 1711329871.065911}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4369068, "arrival": 1711329871.9436672}, "models": {"yolo": {"arrival": 1711329871.699936, "serving": 1711329871.9243057}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4354117, "arrival": 1711329870.6018271}, "models": {"yolo": {"arrival": 1711329869.7845132, "serving": 1711329870.3872993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4172344, "arrival": 1711329871.0930824}, "models": {"yolo": {"arrival": 1711329870.4617472, "serving": 1711329871.065911}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.418343, "arrival": 1711329871.710143}, "models": {"yolo": {"arrival": 1711329871.1728, "serving": 1711329871.6913738}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.41879, "arrival": 1711329871.1009052}, "models": {"yolo": {"arrival": 1711329870.4617472, "serving": 1711329871.065911}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4404976, "arrival": 1711329871.180153}, "models": {"yolo": {"arrival": 1711329870.8795004, "serving": 1711329871.1536636}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.61083737+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4188411}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4409657, "arrival": 1711329869.3920214}, "models": {"yolo": {"arrival": 1711329868.9977741, "serving": 1711329869.3779259}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.617617565+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4406705}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4405913, "arrival": 1711329869.3878899}, "models": {"yolo": {"arrival": 1711329868.9977741, "serving": 1711329869.3779259}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4405127, "arrival": 1711329867.8127768}, "models": {"yolo": {"arrival": 1711329867.4163926, "serving": 1711329867.7873971}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.554222979+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.417458}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4579093, "arrival": 1711329867.8243067}, "models": {"yolo": {"arrival": 1711329867.4163926, "serving": 1711329867.7873971}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.617551601+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4369218}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4369543, "arrival": 1711329871.9601521}, "models": {"yolo": {"arrival": 1711329871.1795366, "serving": 1711329871.9339647}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.608856844+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4367175}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4405606, "arrival": 1711329870.812293}, "models": {"yolo": {"arrival": 1711329870.3982818, "serving": 1711329870.7941048}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.440811, "arrival": 1711329870.813758}, "models": {"yolo": {"arrival": 1711329870.3982818, "serving": 1711329870.7941048}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4353623, "arrival": 1711329867.43952}, "models": {"yolo": {"arrival": 1711329867.1665423, "serving": 1711329867.4033976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4182196, "arrival": 1711329871.0992448}, "models": {"yolo": {"arrival": 1711329870.4617472, "serving": 1711329871.065911}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4404495, "arrival": 1711329871.960576}, "models": {"yolo": {"arrival": 1711329871.1795366, "serving": 1711329871.9339647}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4179497, "arrival": 1711329871.7068405}, "models": {"yolo": {"arrival": 1711329871.1728, "serving": 1711329871.6913738}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4173086, "arrival": 1711329870.6572511}, "models": {"yolo": {"arrival": 1711329870.1425407, "serving": 1711329870.4518704}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4407012, "arrival": 1711329871.9610972}, "models": {"yolo": {"arrival": 1711329871.1795366, "serving": 1711329871.9339647}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.418427, "arrival": 1711329871.0996828}, "models": {"yolo": {"arrival": 1711329870.4617472, "serving": 1711329871.065911}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4403594, "arrival": 1711329871.1796815}, "models": {"yolo": {"arrival": 1711329870.8795004, "serving": 1711329871.1536636}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4180963, "arrival": 1711329870.9145923}, "models": {"yolo": {"arrival": 1711329870.4641113, "serving": 1711329870.8726528}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.603063065+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.417482}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4175076, "arrival": 1711329870.6574833}, "models": {"yolo": {"arrival": 1711329870.1425407, "serving": 1711329870.4518704}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.603098201+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.41768}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.440935, "arrival": 1711329870.8143775}, "models": {"yolo": {"arrival": 1711329870.3982818, "serving": 1711329870.7941048}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4184022, "arrival": 1711329870.6008377}, "models": {"yolo": {"arrival": 1711329869.7845132, "serving": 1711329870.3872993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4402864, "arrival": 1711329869.001695}, "models": {"yolo": {"arrival": 1711329868.6124635, "serving": 1711329868.9905167}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.603028828+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4175816}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4368155, "arrival": 1711329870.6024768}, "models": {"yolo": {"arrival": 1711329869.7845132, "serving": 1711329870.3872993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.440376, "arrival": 1711329867.479453}, "models": {"yolo": {"arrival": 1711329867.1665423, "serving": 1711329867.4033976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4176073, "arrival": 1711329870.414083}, "models": {"yolo": {"arrival": 1711329869.7845132, "serving": 1711329870.3872993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4548988, "arrival": 1711329872.268512}, "models": {"yolo": {"arrival": 1711329871.9335034, "serving": 1711329872.2379665}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4407175, "arrival": 1711329869.3899674}, "models": {"yolo": {"arrival": 1711329868.9977741, "serving": 1711329869.3779259}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.581862085+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4182444}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.603903751+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 1.50 GiB already allocated; 82.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4353018}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4176311, "arrival": 1711329871.0974705}, "models": {"yolo": {"arrival": 1711329870.4617472, "serving": 1711329871.065911}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.611022359+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4366672}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4408898, "arrival": 1711329867.8218255}, "models": {"yolo": {"arrival": 1711329867.4163926, "serving": 1711329867.7873971}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.435343, "arrival": 1711329870.9190993}, "models": {"yolo": {"arrival": 1711329870.4641113, "serving": 1711329870.8726528}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4579458, "arrival": 1711329861.9250033}, "models": {"yolo": {"arrival": 1711329861.4451828, "serving": 1711329861.9016492}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4409811, "arrival": 1711329861.9175663}, "models": {"yolo": {"arrival": 1711329861.4421902, "serving": 1711329861.901113}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.617660406+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4409206}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.610876408+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.436597}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4409962, "arrival": 1711329873.0411115}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4579284, "arrival": 1711329872.2750876}, "models": {"yolo": {"arrival": 1711329871.9335034, "serving": 1711329872.2379665}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.440873, "arrival": 1711329873.039899}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4368765, "arrival": 1711329871.178815}, "models": {"yolo": {"arrival": 1711329870.8795004, "serving": 1711329871.1536636}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4410112, "arrival": 1711329867.8223636}, "models": {"yolo": {"arrival": 1711329867.4163926, "serving": 1711329867.7873971}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4367812, "arrival": 1711329871.9411743}, "models": {"yolo": {"arrival": 1711329871.699936, "serving": 1711329871.9243057}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4405289, "arrival": 1711329871.9450724}, "models": {"yolo": {"arrival": 1711329871.699936, "serving": 1711329871.9243057}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.617485078+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.4407334}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.617463143+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4406068}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4368308, "arrival": 1711329871.1926708}, "models": {"yolo": {"arrival": 1711329871.0735512, "serving": 1711329871.166539}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.440622, "arrival": 1711329873.0379796}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4189644, "arrival": 1711329870.6015038}, "models": {"yolo": {"arrival": 1711329869.7845132, "serving": 1711329870.3872993}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4368913, "arrival": 1711329867.4791112}, "models": {"yolo": {"arrival": 1711329867.1665423, "serving": 1711329867.4033976}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4408424, "arrival": 1711329869.3907602}, "models": {"yolo": {"arrival": 1711329868.9977741, "serving": 1711329869.3779259}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4353793, "arrival": 1711329871.714095}, "models": {"yolo": {"arrival": 1711329871.1728, "serving": 1711329871.6913738}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:20.617392374+00:00\"}\"\n>", "times": {"request": {"sending": 1711329860.436861}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4366498, "arrival": 1711329871.9404557}, "models": {"yolo": {"arrival": 1711329871.699936, "serving": 1711329871.9243057}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4404662, "arrival": 1711329869.002426}, "models": {"yolo": {"arrival": 1711329868.6124635, "serving": 1711329868.9905167}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.617528537+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.436797}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4407792, "arrival": 1711329872.2655647}, "models": {"yolo": {"arrival": 1711329871.9335034, "serving": 1711329872.2379665}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4549828, "arrival": 1711329861.924196}, "models": {"yolo": {"arrival": 1711329861.4451828, "serving": 1711329861.9016492}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4407644, "arrival": 1711329867.8212574}, "models": {"yolo": {"arrival": 1711329867.4163926, "serving": 1711329867.7873971}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4561012, "arrival": 1711329871.9637825}, "models": {"yolo": {"arrival": 1711329871.1795366, "serving": 1711329871.9339647}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:20.61763885+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329860.4407954}}, "outputs": []}, {"times": {"request": {"sending": 1711329860.4368463, "arrival": 1711329869.000491}, "models": {"yolo": {"arrival": 1711329868.6124635, "serving": 1711329868.9905167}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.4560437, "arrival": 1711329871.0797777}, "models": {"yolo": {"arrival": 1711329870.8040729, "serving": 1711329871.057645}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329860.440655, "arrival": 1711329872.2649283}, "models": {"yolo": {"arrival": 1711329871.9335034, "serving": 1711329872.2379665}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329861.4376147, "arrival": 1711329872.5112875}, "models": {"yolo": {"arrival": 1711329872.1999254, "serving": 1711329872.4736674}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.532384379+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.437706}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4323037, "arrival": 1711329871.2027557}, "models": {"yolo": {"arrival": 1711329871.0667236, "serving": 1711329871.1754053}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4123268, "arrival": 1711329871.0818934}, "models": {"yolo": {"arrival": 1711329870.8040729, "serving": 1711329871.057645}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.412649, "arrival": 1711329872.2766125}, "models": {"yolo": {"arrival": 1711329871.9335034, "serving": 1711329872.2379665}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4131727, "arrival": 1711329873.057261}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.413105, "arrival": 1711329869.7655733}, "models": {"yolo": {"arrival": 1711329869.3875878, "serving": 1711329869.7490473}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.469942, "arrival": 1711329873.105397}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4374921, "arrival": 1711329870.1760435}, "models": {"yolo": {"arrival": 1711329869.7579744, "serving": 1711329870.143501}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.517092454+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.4327178}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4131978, "arrival": 1711329868.0613976}, "models": {"yolo": {"arrival": 1711329867.7977242, "serving": 1711329868.0438714}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.43782, "arrival": 1711329872.7918954}, "models": {"yolo": {"arrival": 1711329872.4803095, "serving": 1711329872.760733}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.517119297+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.4375136}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4127276, "arrival": 1711329871.0829523}, "models": {"yolo": {"arrival": 1711329870.8040729, "serving": 1711329871.057645}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.517018251+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4133546}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4125693, "arrival": 1711329873.0561333}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.412766, "arrival": 1711329871.9642816}, "models": {"yolo": {"arrival": 1711329871.1795366, "serving": 1711329871.9339647}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4386604, "arrival": 1711329873.0992048}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.432739, "arrival": 1711329873.076687}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4375987, "arrival": 1711329871.9477527}, "models": {"yolo": {"arrival": 1711329871.1921167, "serving": 1711329871.9268353}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.53241883+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4380772}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4298213, "arrival": 1711329868.0739384}, "models": {"yolo": {"arrival": 1711329867.7977242, "serving": 1711329868.0438714}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4327598, "arrival": 1711329868.5085423}, "models": {"yolo": {"arrival": 1711329868.0539484, "serving": 1711329868.4809768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4381533, "arrival": 1711329873.0839775}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.413484, "arrival": 1711329871.0872934}, "models": {"yolo": {"arrival": 1711329870.8040729, "serving": 1711329871.057645}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.41338, "arrival": 1711329873.0576}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.532401457+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.437836}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4385693, "arrival": 1711329872.799211}, "models": {"yolo": {"arrival": 1711329872.4803095, "serving": 1711329872.760733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4130678, "arrival": 1711329871.9661524}, "models": {"yolo": {"arrival": 1711329871.1795366, "serving": 1711329871.9339647}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4124324, "arrival": 1711329871.964053}, "models": {"yolo": {"arrival": 1711329871.1795366, "serving": 1711329871.9339647}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4384768, "arrival": 1711329871.9618375}, "models": {"yolo": {"arrival": 1711329871.1921167, "serving": 1711329871.9268353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4327822, "arrival": 1711329872.5170717}, "models": {"yolo": {"arrival": 1711329872.2500136, "serving": 1711329872.4688804}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.413535, "arrival": 1711329869.7719526}, "models": {"yolo": {"arrival": 1711329869.3875878, "serving": 1711329869.7490473}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4382691, "arrival": 1711329873.0982199}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4128785, "arrival": 1711329873.0568933}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4126878, "arrival": 1711329861.9256363}, "models": {"yolo": {"arrival": 1711329861.4451828, "serving": 1711329861.9016492}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4129157, "arrival": 1711329867.8273635}, "models": {"yolo": {"arrival": 1711329867.4163926, "serving": 1711329867.7873971}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4297566, "arrival": 1711329869.7724614}, "models": {"yolo": {"arrival": 1711329869.3875878, "serving": 1711329869.7490473}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4326982, "arrival": 1711329870.1751714}, "models": {"yolo": {"arrival": 1711329869.7579744, "serving": 1711329870.143501}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.532292181+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4324725}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4326158, "arrival": 1711329872.5161502}, "models": {"yolo": {"arrival": 1711329872.2500136, "serving": 1711329872.4688804}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4298437, "arrival": 1711329872.5041456}, "models": {"yolo": {"arrival": 1711329872.2500136, "serving": 1711329872.4688804}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.412528, "arrival": 1711329861.9208286}, "models": {"yolo": {"arrival": 1711329861.4421902, "serving": 1711329861.901113}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4124854, "arrival": 1711329869.7640958}, "models": {"yolo": {"arrival": 1711329869.3875878, "serving": 1711329869.7490473}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4380934, "arrival": 1711329871.948925}, "models": {"yolo": {"arrival": 1711329871.1921167, "serving": 1711329871.9268353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.432535, "arrival": 1711329870.174767}, "models": {"yolo": {"arrival": 1711329869.7579744, "serving": 1711329870.143501}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4723926, "arrival": 1711329873.0938258}, "models": {"yolo": {"arrival": 1711329872.7714992, "serving": 1711329873.047358}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.438109, "arrival": 1711329872.5128336}, "models": {"yolo": {"arrival": 1711329872.1999254, "serving": 1711329872.4736674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4328504, "arrival": 1711329872.2101712}, "models": {"yolo": {"arrival": 1711329871.9469297, "serving": 1711329872.1882768}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.532310019+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.4326367}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.432825, "arrival": 1711329871.204676}, "models": {"yolo": {"arrival": 1711329871.0667236, "serving": 1711329871.1754053}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.51706269+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4323795}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.520186143+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4377706}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4324021, "arrival": 1711329873.0751488}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4132762, "arrival": 1711329871.0866601}, "models": {"yolo": {"arrival": 1711329870.8040729, "serving": 1711329871.057645}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4385545, "arrival": 1711329869.0077865}, "models": {"yolo": {"arrival": 1711329868.4933264, "serving": 1711329868.982656}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.5202438+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.438255}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4375632, "arrival": 1711329872.7901218}, "models": {"yolo": {"arrival": 1711329872.4803095, "serving": 1711329872.760733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4126103, "arrival": 1711329867.8249664}, "models": {"yolo": {"arrival": 1711329867.4163926, "serving": 1711329867.7873971}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4128034, "arrival": 1711329869.7644749}, "models": {"yolo": {"arrival": 1711329869.3875878, "serving": 1711329869.7490473}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.438299, "arrival": 1711329872.7985075}, "models": {"yolo": {"arrival": 1711329872.4803095, "serving": 1711329872.760733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4325752, "arrival": 1711329873.0761244}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4386752, "arrival": 1711329869.0083888}, "models": {"yolo": {"arrival": 1711329868.4933264, "serving": 1711329868.982656}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4133015, "arrival": 1711329871.9669092}, "models": {"yolo": {"arrival": 1711329871.1795366, "serving": 1711329871.9339647}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.517034593+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4135606}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4134307, "arrival": 1711329872.502999}, "models": {"yolo": {"arrival": 1711329872.2500136, "serving": 1711329872.4688804}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4323559, "arrival": 1711329870.1743445}, "models": {"yolo": {"arrival": 1711329869.7579744, "serving": 1711329870.143501}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4373994, "arrival": 1711329868.5090456}, "models": {"yolo": {"arrival": 1711329868.0539484, "serving": 1711329868.4809768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4679606, "arrival": 1711329871.9654143}, "models": {"yolo": {"arrival": 1711329871.1921167, "serving": 1711329871.9268353}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.520271083+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.4385219}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.532351448+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4374416}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4376907, "arrival": 1711329872.7905846}, "models": {"yolo": {"arrival": 1711329872.4803095, "serving": 1711329872.760733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4297082, "arrival": 1711329871.2024844}, "models": {"yolo": {"arrival": 1711329871.0667236, "serving": 1711329871.1754053}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.43792, "arrival": 1711329873.083288}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.437737, "arrival": 1711329872.5118513}, "models": {"yolo": {"arrival": 1711329872.1999254, "serving": 1711329872.4736674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4378915, "arrival": 1711329870.6563997}, "models": {"yolo": {"arrival": 1711329870.1528559, "serving": 1711329870.456786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4128413, "arrival": 1711329861.9214585}, "models": {"yolo": {"arrival": 1711329861.4421902, "serving": 1711329861.901113}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4129527, "arrival": 1711329872.2770722}, "models": {"yolo": {"arrival": 1711329871.9335034, "serving": 1711329872.2379665}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.413329, "arrival": 1711329869.766017}, "models": {"yolo": {"arrival": 1711329869.3875878, "serving": 1711329869.7490473}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4129899, "arrival": 1711329861.9262311}, "models": {"yolo": {"arrival": 1711329861.4451828, "serving": 1711329861.9016492}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4381666, "arrival": 1711329869.0057411}, "models": {"yolo": {"arrival": 1711329868.4933264, "serving": 1711329868.982656}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4377863, "arrival": 1711329873.082497}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4378767, "arrival": 1711329872.5123572}, "models": {"yolo": {"arrival": 1711329872.1999254, "serving": 1711329872.4736674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.437863, "arrival": 1711329871.9485688}, "models": {"yolo": {"arrival": 1711329871.1921167, "serving": 1711329871.9268353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4679787, "arrival": 1711329872.7738702}, "models": {"yolo": {"arrival": 1711329872.4900768, "serving": 1711329872.7516842}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.532506322+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4679387}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4326777, "arrival": 1711329872.208}, "models": {"yolo": {"arrival": 1711329871.9469297, "serving": 1711329872.1882768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4377525, "arrival": 1711329870.6559808}, "models": {"yolo": {"arrival": 1711329870.1528559, "serving": 1711329870.456786}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.532368336+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.4375832}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4698505, "arrival": 1711329870.9243703}, "models": {"yolo": {"arrival": 1711329870.4667761, "serving": 1711329870.8720162}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.41303, "arrival": 1711329871.0856411}, "models": {"yolo": {"arrival": 1711329870.8040729, "serving": 1711329871.057645}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.516969478+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.413147}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4381819, "arrival": 1711329872.7959855}, "models": {"yolo": {"arrival": 1711329872.4803095, "serving": 1711329872.760733}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.532488+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.4385831}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.532189381+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.413249}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.432332, "arrival": 1711329872.2065773}, "models": {"yolo": {"arrival": 1711329871.9469297, "serving": 1711329872.1882768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4678366, "arrival": 1711329873.0932846}, "models": {"yolo": {"arrival": 1711329872.7714992, "serving": 1711329873.047358}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.532228113+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.413457}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4374764, "arrival": 1711329872.2108562}, "models": {"yolo": {"arrival": 1711329871.9469297, "serving": 1711329872.1882768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4382262, "arrival": 1711329872.516725}, "models": {"yolo": {"arrival": 1711329872.1999254, "serving": 1711329872.4736674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4296527, "arrival": 1711329872.5036228}, "models": {"yolo": {"arrival": 1711329872.2500136, "serving": 1711329872.4688804}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4386315, "arrival": 1711329870.9194772}, "models": {"yolo": {"arrival": 1711329870.4667761, "serving": 1711329870.8720162}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4134052, "arrival": 1711329868.0623276}, "models": {"yolo": {"arrival": 1711329867.7977242, "serving": 1711329868.0438714}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.432515, "arrival": 1711329872.207356}, "models": {"yolo": {"arrival": 1711329871.9469297, "serving": 1711329872.1882768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4132233, "arrival": 1711329872.2774832}, "models": {"yolo": {"arrival": 1711329871.9335034, "serving": 1711329872.2379665}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.438285, "arrival": 1711329869.0064976}, "models": {"yolo": {"arrival": 1711329868.4933264, "serving": 1711329868.982656}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.53252276+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.472438}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4379523, "arrival": 1711329872.792361}, "models": {"yolo": {"arrival": 1711329872.4803095, "serving": 1711329872.760733}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.532335019+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4328053}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4384913, "arrival": 1711329872.5180783}, "models": {"yolo": {"arrival": 1711329872.1999254, "serving": 1711329872.4736674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4326575, "arrival": 1711329871.2042363}, "models": {"yolo": {"arrival": 1711329871.0667236, "serving": 1711329871.1754053}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4386027, "arrival": 1711329871.9620821}, "models": {"yolo": {"arrival": 1711329871.1921167, "serving": 1711329871.9268353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.438507, "arrival": 1711329870.6579375}, "models": {"yolo": {"arrival": 1711329870.1528559, "serving": 1711329870.456786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.438617, "arrival": 1711329872.518424}, "models": {"yolo": {"arrival": 1711329872.1999254, "serving": 1711329872.4736674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4379365, "arrival": 1711329868.5117948}, "models": {"yolo": {"arrival": 1711329868.0539484, "serving": 1711329868.4809768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4378018, "arrival": 1711329868.5115175}, "models": {"yolo": {"arrival": 1711329868.0539484, "serving": 1711329868.4809768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4384308, "arrival": 1711329869.0071425}, "models": {"yolo": {"arrival": 1711329868.4933264, "serving": 1711329868.982656}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.517105813+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4328952}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4324946, "arrival": 1711329871.2030194}, "models": {"yolo": {"arrival": 1711329871.0667236, "serving": 1711329871.1754053}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4374208, "arrival": 1711329872.517412}, "models": {"yolo": {"arrival": 1711329872.2500136, "serving": 1711329872.4688804}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.532454995+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.4383135}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.520296587+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4699214}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4383998, "arrival": 1711329873.0985794}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.520284178+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4386458}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4384453, "arrival": 1711329872.798891}, "models": {"yolo": {"arrival": 1711329872.4803095, "serving": 1711329872.760733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4377224, "arrival": 1711329871.9482005}, "models": {"yolo": {"arrival": 1711329871.1921167, "serving": 1711329871.9268353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4376307, "arrival": 1711329870.1764147}, "models": {"yolo": {"arrival": 1711329869.7579744, "serving": 1711329870.143501}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.517049062+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.429777}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.429733, "arrival": 1711329872.203316}, "models": {"yolo": {"arrival": 1711329871.9469297, "serving": 1711329872.1882768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4374595, "arrival": 1711329871.2049477}, "models": {"yolo": {"arrival": 1711329871.0667236, "serving": 1711329871.1754053}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4381237, "arrival": 1711329870.6567667}, "models": {"yolo": {"arrival": 1711329870.1528559, "serving": 1711329870.456786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4383602, "arrival": 1711329870.657709}, "models": {"yolo": {"arrival": 1711329870.1528559, "serving": 1711329870.456786}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4699595, "arrival": 1711329869.0096052}, "models": {"yolo": {"arrival": 1711329868.4933264, "serving": 1711329868.982656}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.53227653+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.4322324}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.520257654+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4383776}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4383426, "arrival": 1711329872.5177388}, "models": {"yolo": {"arrival": 1711329872.1999254, "serving": 1711329872.4736674}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4135854, "arrival": 1711329873.0580516}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4376762, "arrival": 1711329868.5112233}, "models": {"yolo": {"arrival": 1711329868.0539484, "serving": 1711329868.4809768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4383278, "arrival": 1711329871.9615765}, "models": {"yolo": {"arrival": 1711329871.1921167, "serving": 1711329871.9268353}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.532436494+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.438196}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4373314, "arrival": 1711329873.0771716}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4376612, "arrival": 1711329873.081892}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.438212, "arrival": 1711329871.961339}, "models": {"yolo": {"arrival": 1711329871.1921167, "serving": 1711329871.9268353}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.517133498+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.4376464}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.520216452+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.437906}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4385383, "arrival": 1711329873.0988925}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.532251038+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4296825}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.437547, "arrival": 1711329868.5108547}, "models": {"yolo": {"arrival": 1711329868.0539484, "serving": 1711329868.4809768}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4325962, "arrival": 1711329868.0841038}, "models": {"yolo": {"arrival": 1711329867.7977242, "serving": 1711329868.0438714}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.429552, "arrival": 1711329868.073011}, "models": {"yolo": {"arrival": 1711329867.7977242, "serving": 1711329868.0438714}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4135098, "arrival": 1711329871.9671686}, "models": {"yolo": {"arrival": 1711329871.1795366, "serving": 1711329871.9339647}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.520230745+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.4381387}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.532471868+00:00\"}\"\n>", "times": {"request": {"sending": 1711329861.43846}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.429799, "arrival": 1711329873.0627046}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4324477, "arrival": 1711329872.5093825}, "models": {"yolo": {"arrival": 1711329872.2500136, "serving": 1711329872.4688804}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4328728, "arrival": 1711329870.1756084}, "models": {"yolo": {"arrival": 1711329869.7579744, "serving": 1711329870.143501}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4375312, "arrival": 1711329873.0794573}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.517078357+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329861.4325545}}, "outputs": []}, {"times": {"request": {"sending": 1711329861.4324255, "arrival": 1711329868.0746443}, "models": {"yolo": {"arrival": 1711329867.7977242, "serving": 1711329868.0438714}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329861.4382408, "arrival": 1711329870.657008}, "models": {"yolo": {"arrival": 1711329870.1528559, "serving": 1711329870.456786}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329862.4120462, "arrival": 1711329873.1079857}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.438058, "arrival": 1711329873.1156309}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.411929, "arrival": 1711329872.7824948}, "models": {"yolo": {"arrival": 1711329872.4900768, "serving": 1711329872.7516842}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4379504, "arrival": 1711329873.3679357}, "models": {"yolo": {"arrival": 1711329873.0596187, "serving": 1711329873.3232436}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.951566501+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4384649}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.426961, "arrival": 1711329873.105808}, "models": {"yolo": {"arrival": 1711329872.7714992, "serving": 1711329873.047358}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.940414758+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4271705}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.024590969+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.4391534}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.024544043+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.438888}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4115965, "arrival": 1711329872.7818074}, "models": {"yolo": {"arrival": 1711329872.4900768, "serving": 1711329872.7516842}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4107945, "arrival": 1711329871.9656985}, "models": {"yolo": {"arrival": 1711329871.1921167, "serving": 1711329871.9268353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4791555, "arrival": 1711329874.0041742}, "models": {"yolo": {"arrival": 1711329873.6386092, "serving": 1711329873.9626906}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.437933, "arrival": 1711329869.7747366}, "models": {"yolo": {"arrival": 1711329869.4302013, "serving": 1711329869.7620754}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.411144, "arrival": 1711329873.0943773}, "models": {"yolo": {"arrival": 1711329872.7714992, "serving": 1711329873.047358}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.520309017+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4110174}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.437762, "arrival": 1711329873.1150517}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4387898, "arrival": 1711329873.3595858}, "models": {"yolo": {"arrival": 1711329873.0628567, "serving": 1711329873.323479}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4377956, "arrival": 1711329873.3674493}, "models": {"yolo": {"arrival": 1711329873.0596187, "serving": 1711329873.3232436}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4115572, "arrival": 1711329872.188462}, "models": {"yolo": {"arrival": 1711329871.9392426, "serving": 1711329872.17166}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4288993, "arrival": 1711329872.1927361}, "models": {"yolo": {"arrival": 1711329871.9392426, "serving": 1711329872.17166}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.95154971+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.4383185}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4383705, "arrival": 1711329873.6734858}, "models": {"yolo": {"arrival": 1711329873.3336053, "serving": 1711329873.62909}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4387078, "arrival": 1711329873.1193385}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.95149989+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4378834}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4384966, "arrival": 1711329869.7841542}, "models": {"yolo": {"arrival": 1711329869.4302013, "serving": 1711329869.7620754}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.308100655+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.438023}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4116416, "arrival": 1711329870.9267998}, "models": {"yolo": {"arrival": 1711329870.4667761, "serving": 1711329870.8720162}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.951620823+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4389513}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4376268, "arrival": 1711329869.7732356}, "models": {"yolo": {"arrival": 1711329869.4302013, "serving": 1711329869.7620754}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4383378, "arrival": 1711329873.1160905}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.021237203+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.4381135}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.410917, "arrival": 1711329872.7801762}, "models": {"yolo": {"arrival": 1711329872.4900768, "serving": 1711329872.7516842}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.438724, "arrival": 1711329870.1598735}, "models": {"yolo": {"arrival": 1711329869.7779026, "serving": 1711329870.136852}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.428947, "arrival": 1711329871.1249833}, "models": {"yolo": {"arrival": 1711329870.8782597, "serving": 1711329871.1171372}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.021290923+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4386246}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.951463807+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.4293404}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4379902, "arrival": 1711329872.492073}, "models": {"yolo": {"arrival": 1711329872.180776, "serving": 1711329872.4605417}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.020992881+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4118478}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4380739, "arrival": 1711329869.7754736}, "models": {"yolo": {"arrival": 1711329869.4302013, "serving": 1711329869.7620754}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4292934, "arrival": 1711329873.0963812}, "models": {"yolo": {"arrival": 1711329872.7608404, "serving": 1711329873.0482035}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.314794391+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.439061}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4290385, "arrival": 1711329873.1065133}, "models": {"yolo": {"arrival": 1711329872.7714992, "serving": 1711329873.047358}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4109712, "arrival": 1711329870.924854}, "models": {"yolo": {"arrival": 1711329870.4667761, "serving": 1711329870.8720162}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4391685, "arrival": 1711329872.7896893}, "models": {"yolo": {"arrival": 1711329872.4715679, "serving": 1711329872.7498302}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4382305, "arrival": 1711329873.3687482}, "models": {"yolo": {"arrival": 1711329873.0596187, "serving": 1711329873.3232436}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.940311518+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.4116893}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.411277, "arrival": 1711329872.7810874}, "models": {"yolo": {"arrival": 1711329872.4900768, "serving": 1711329872.7516842}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4110613, "arrival": 1711329873.1068041}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4111025, "arrival": 1711329869.4597151}, "models": {"yolo": {"arrival": 1711329868.9925323, "serving": 1711329869.4186869}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4291348, "arrival": 1711329871.128783}, "models": {"yolo": {"arrival": 1711329870.8782597, "serving": 1711329871.1171372}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4385116, "arrival": 1711329873.6739194}, "models": {"yolo": {"arrival": 1711329873.3336053, "serving": 1711329873.62909}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4384308, "arrival": 1711329873.35848}, "models": {"yolo": {"arrival": 1711329873.0628567, "serving": 1711329873.323479}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4379148, "arrival": 1711329873.1153502}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.428814, "arrival": 1711329869.4612935}, "models": {"yolo": {"arrival": 1711329868.9925323, "serving": 1711329869.4186869}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.9516382+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.439081}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.308139633+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.4381652}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4113185, "arrival": 1711329870.9252517}, "models": {"yolo": {"arrival": 1711329870.4667761, "serving": 1711329870.8720162}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4387743, "arrival": 1711329872.78587}, "models": {"yolo": {"arrival": 1711329872.4715679, "serving": 1711329872.7498302}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.439185, "arrival": 1711329873.6500895}, "models": {"yolo": {"arrival": 1711329873.3319669, "serving": 1711329873.6292217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4119675, "arrival": 1711329870.9271781}, "models": {"yolo": {"arrival": 1711329870.4667761, "serving": 1711329870.8720162}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.532539354+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4111843}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4791155, "arrival": 1711329873.120765}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.322295227+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.479073}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.940383978+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4120069}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4291832, "arrival": 1711329873.114304}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4291117, "arrival": 1711329873.0960376}, "models": {"yolo": {"arrival": 1711329872.7608404, "serving": 1711329873.0482035}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.024576253+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.439014}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.43866, "arrival": 1711329873.3591168}, "models": {"yolo": {"arrival": 1711329873.0628567, "serving": 1711329873.323479}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.314750385+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4388058}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4112265, "arrival": 1711329871.9659276}, "models": {"yolo": {"arrival": 1711329871.1921167, "serving": 1711329871.9268353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4392304, "arrival": 1711329873.1204872}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.021102305+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.429061}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.43903, "arrival": 1711329872.7891715}, "models": {"yolo": {"arrival": 1711329872.4715679, "serving": 1711329872.7498302}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.021157731+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4376693}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.427094, "arrival": 1711329872.192101}, "models": {"yolo": {"arrival": 1711329871.9392426, "serving": 1711329872.17166}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4790034, "arrival": 1711329873.0777452}, "models": {"yolo": {"arrival": 1711329872.7618132, "serving": 1711329873.0326653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4289238, "arrival": 1711329873.0954807}, "models": {"yolo": {"arrival": 1711329872.7608404, "serving": 1711329873.0482035}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4389665, "arrival": 1711329873.1199498}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4377098, "arrival": 1711329873.0966704}, "models": {"yolo": {"arrival": 1711329872.7608404, "serving": 1711329873.0482035}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4118896, "arrival": 1711329872.191047}, "models": {"yolo": {"arrival": 1711329871.9392426, "serving": 1711329872.17166}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4293153, "arrival": 1711329871.1293676}, "models": {"yolo": {"arrival": 1711329870.8782597, "serving": 1711329871.1171372}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.951532511+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.438181}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.429016, "arrival": 1711329869.4615226}, "models": {"yolo": {"arrival": 1711329868.9925323, "serving": 1711329869.4186869}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.438148, "arrival": 1711329873.0978866}, "models": {"yolo": {"arrival": 1711329872.7608404, "serving": 1711329873.0482035}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.314685351+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4384465}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4390454, "arrival": 1711329873.3606453}, "models": {"yolo": {"arrival": 1711329873.0628567, "serving": 1711329873.323479}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4791386, "arrival": 1711329870.6033492}, "models": {"yolo": {"arrival": 1711329870.1440556, "serving": 1711329870.4050272}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.438481, "arrival": 1711329873.1164467}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.411397, "arrival": 1711329873.1071155}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4287465, "arrival": 1711329873.1135836}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.411436, "arrival": 1711329869.4603705}, "models": {"yolo": {"arrival": 1711329868.9925323, "serving": 1711329869.4186869}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.021255682+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.438247}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.308012324+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4378664}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4375627, "arrival": 1711329873.1147387}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4378335, "arrival": 1711329872.491527}, "models": {"yolo": {"arrival": 1711329872.180776, "serving": 1711329872.4605417}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4377277, "arrival": 1711329871.1302722}, "models": {"yolo": {"arrival": 1711329870.8782597, "serving": 1711329871.1171372}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4382858, "arrival": 1711329873.3479874}, "models": {"yolo": {"arrival": 1711329873.0628567, "serving": 1711329873.323479}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.314773582+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4389362}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.951672209+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.4790971}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.520325762+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.411358}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.439137, "arrival": 1711329873.684059}, "models": {"yolo": {"arrival": 1711329873.3336053, "serving": 1711329873.62909}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4790535, "arrival": 1711329873.6552498}, "models": {"yolo": {"arrival": 1711329873.3319669, "serving": 1711329873.6292217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.438197, "arrival": 1711329873.1158698}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4387422, "arrival": 1711329873.6743298}, "models": {"yolo": {"arrival": 1711329873.3336053, "serving": 1711329873.62909}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4388406, "arrival": 1711329873.1196618}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4376886, "arrival": 1711329872.490828}, "models": {"yolo": {"arrival": 1711329872.180776, "serving": 1711329872.4605417}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.437779, "arrival": 1711329869.7739584}, "models": {"yolo": {"arrival": 1711329869.4302013, "serving": 1711329869.7620754}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.021309147+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4387584}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4391136, "arrival": 1711329870.168325}, "models": {"yolo": {"arrival": 1711329869.7779026, "serving": 1711329870.136852}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4378493, "arrival": 1711329873.0970619}, "models": {"yolo": {"arrival": 1711329872.7608404, "serving": 1711329873.0482035}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.532557261+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4115152}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.438097, "arrival": 1711329873.368337}, "models": {"yolo": {"arrival": 1711329873.0596187, "serving": 1711329873.3232436}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4114757, "arrival": 1711329873.094942}, "models": {"yolo": {"arrival": 1711329872.7714992, "serving": 1711329873.047358}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4384143, "arrival": 1711329872.7840583}, "models": {"yolo": {"arrival": 1711329872.4715679, "serving": 1711329872.7498302}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4271212, "arrival": 1711329872.7832763}, "models": {"yolo": {"arrival": 1711329872.4900768, "serving": 1711329872.7516842}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4292057, "arrival": 1711329869.4617639}, "models": {"yolo": {"arrival": 1711329868.9925323, "serving": 1711329869.4186869}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.951516227+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4380417}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4118085, "arrival": 1711329873.1050599}, "models": {"yolo": {"arrival": 1711329872.7714992, "serving": 1711329873.047358}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4389966, "arrival": 1711329873.6834652}, "models": {"yolo": {"arrival": 1711329873.3336053, "serving": 1711329873.62909}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4386427, "arrival": 1711329872.7845418}, "models": {"yolo": {"arrival": 1711329872.4715679, "serving": 1711329872.7498302}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4117293, "arrival": 1711329873.107401}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4392576, "arrival": 1711329870.1737075}, "models": {"yolo": {"arrival": 1711329869.7779026, "serving": 1711329870.136852}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.314612709+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.4383025}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4271467, "arrival": 1711329870.9275641}, "models": {"yolo": {"arrival": 1711329870.4667761, "serving": 1711329870.8720162}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.951420814+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4289708}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4120846, "arrival": 1711329869.4610653}, "models": {"yolo": {"arrival": 1711329868.9925323, "serving": 1711329869.4186869}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.021182193+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4378166}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.02461681+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.479174}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4382637, "arrival": 1711329872.510062}, "models": {"yolo": {"arrival": 1711329872.180776, "serving": 1711329872.4605417}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4117694, "arrival": 1711329869.460805}, "models": {"yolo": {"arrival": 1711329868.9925323, "serving": 1711329869.4186869}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4292285, "arrival": 1711329873.3633263}, "models": {"yolo": {"arrival": 1711329873.0596187, "serving": 1711329873.3232436}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4389048, "arrival": 1711329872.7863586}, "models": {"yolo": {"arrival": 1711329872.4715679, "serving": 1711329872.7498302}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:22.951584662+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.438692}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.439096, "arrival": 1711329873.1202228}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4292715, "arrival": 1711329872.4903293}, "models": {"yolo": {"arrival": 1711329872.180776, "serving": 1711329872.4605417}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4389205, "arrival": 1711329873.3601933}, "models": {"yolo": {"arrival": 1711329873.0628567, "serving": 1711329873.323479}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.02107733+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.428873}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.951603154+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4388247}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4381316, "arrival": 1711329872.4925318}, "models": {"yolo": {"arrival": 1711329872.180776, "serving": 1711329872.4605417}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4289937, "arrival": 1711329873.1138618}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.021273342+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.4383862}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.314725019+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4386756}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4376497, "arrival": 1711329873.3637257}, "models": {"yolo": {"arrival": 1711329873.0596187, "serving": 1711329873.3232436}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.951481368+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4377446}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4382133, "arrival": 1711329869.7768788}, "models": {"yolo": {"arrival": 1711329869.4302013, "serving": 1711329869.7620754}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.438873, "arrival": 1711329873.6748037}, "models": {"yolo": {"arrival": 1711329873.3336053, "serving": 1711329873.62909}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.021208472+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.4379687}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.951446438+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4291604}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:22.951655235+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.439215}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.322241508+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4391997}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4389815, "arrival": 1711329870.1621675}, "models": {"yolo": {"arrival": 1711329869.7779026, "serving": 1711329870.136852}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4388561, "arrival": 1711329870.1605875}, "models": {"yolo": {"arrival": 1711329869.7779026, "serving": 1711329870.136852}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.024603823+00:00\"}\"\n>", "times": {"request": {"sending": 1711329862.4780424}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.021134242+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.4292498}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4380066, "arrival": 1711329873.0974686}, "models": {"yolo": {"arrival": 1711329872.7608404, "serving": 1711329873.0482035}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.021051636+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329862.42706}}, "outputs": []}, {"times": {"request": {"sending": 1711329862.4779527, "arrival": 1711329874.0038786}, "models": {"yolo": {"arrival": 1711329873.6386092, "serving": 1711329873.9626906}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4383547, "arrival": 1711329869.7797241}, "models": {"yolo": {"arrival": 1711329869.4302013, "serving": 1711329869.7620754}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.4288466, "arrival": 1711329873.1061983}, "models": {"yolo": {"arrival": 1711329872.7714992, "serving": 1711329873.047358}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329862.429088, "arrival": 1711329872.4897096}, "models": {"yolo": {"arrival": 1711329872.180776, "serving": 1711329872.4605417}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329863.4183047, "arrival": 1711329873.6568136}, "models": {"yolo": {"arrival": 1711329873.3319669, "serving": 1711329873.6292217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4188726, "arrival": 1711329873.0805728}, "models": {"yolo": {"arrival": 1711329872.7618132, "serving": 1711329873.0326653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.632597558+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.436942}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.418115, "arrival": 1711329873.121036}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.648177922+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4189763}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4181912, "arrival": 1711329874.0044546}, "models": {"yolo": {"arrival": 1711329873.6386092, "serving": 1711329873.9626906}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4459906, "arrival": 1711329874.3086376}, "models": {"yolo": {"arrival": 1711329873.973017, "serving": 1711329874.2604613}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.632559067+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.419116}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.336940594+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4490123}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.449165, "arrival": 1711329871.201856}, "models": {"yolo": {"arrival": 1711329870.8746135, "serving": 1711329871.16305}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4178872, "arrival": 1711329873.078311}, "models": {"yolo": {"arrival": 1711329872.7618132, "serving": 1711329873.0326653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4189074, "arrival": 1711329873.658066}, "models": {"yolo": {"arrival": 1711329873.3319669, "serving": 1711329873.6292217}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.418153, "arrival": 1711329870.6035888}, "models": {"yolo": {"arrival": 1711329870.1440556, "serving": 1711329870.4050272}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4459565, "arrival": 1711329873.67772}, "models": {"yolo": {"arrival": 1711329873.3207316, "serving": 1711329873.6332297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4492712, "arrival": 1711329871.202183}, "models": {"yolo": {"arrival": 1711329870.8746135, "serving": 1711329871.16305}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4182668, "arrival": 1711329873.078881}, "models": {"yolo": {"arrival": 1711329872.7618132, "serving": 1711329873.0326653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4179797, "arrival": 1711329873.6561472}, "models": {"yolo": {"arrival": 1711329873.3319669, "serving": 1711329873.6292217}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.648364283+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.446325}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4457982, "arrival": 1711329874.2908254}, "models": {"yolo": {"arrival": 1711329873.973017, "serving": 1711329874.2604613}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4490387, "arrival": 1711329873.6848412}, "models": {"yolo": {"arrival": 1711329873.3207316, "serving": 1711329873.6332297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4367225, "arrival": 1711329873.1235056}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4415214, "arrival": 1711329873.3290796}, "models": {"yolo": {"arrival": 1711329873.0427969, "serving": 1711329873.3092184}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4495037, "arrival": 1711329864.786309}, "models": {"yolo": {"arrival": 1711329864.4315803, "serving": 1711329864.758022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.631924235+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4180737}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4410703, "arrival": 1711329870.8856747}, "models": {"yolo": {"arrival": 1711329870.416551, "serving": 1711329870.8649597}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.648324464+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.445938}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.322550703+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4411716}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4409785, "arrival": 1711329873.9898415}, "models": {"yolo": {"arrival": 1711329873.6377995, "serving": 1711329873.9542434}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.441232, "arrival": 1711329870.8868246}, "models": {"yolo": {"arrival": 1711329870.416551, "serving": 1711329870.8649597}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4369044, "arrival": 1711329870.8824325}, "models": {"yolo": {"arrival": 1711329870.416551, "serving": 1711329870.8649597}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.612451611+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4182277}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4187334, "arrival": 1711329873.1229448}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.322578568+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4413323}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.631958091+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.418379}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4187682, "arrival": 1711329870.6209798}, "models": {"yolo": {"arrival": 1711329870.1440556, "serving": 1711329870.4050272}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.322321529+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4180305}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.632642627+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4414246}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4489536, "arrival": 1711329874.6870246}, "models": {"yolo": {"arrival": 1711329874.2728205, "serving": 1711329874.6324153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4491518, "arrival": 1711329873.6850953}, "models": {"yolo": {"arrival": 1711329873.3207316, "serving": 1711329873.6332297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.436763, "arrival": 1711329874.0102572}, "models": {"yolo": {"arrival": 1711329873.6386092, "serving": 1711329873.9626906}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4417028, "arrival": 1711329870.8903677}, "models": {"yolo": {"arrival": 1711329870.416551, "serving": 1711329870.8649597}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4494526, "arrival": 1711329864.7941923}, "models": {"yolo": {"arrival": 1711329864.4290068, "serving": 1711329864.7589974}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.648377949+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4490259}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4412513, "arrival": 1711329874.2875917}, "models": {"yolo": {"arrival": 1711329873.973017, "serving": 1711329874.2604613}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4492176, "arrival": 1711329874.3292024}, "models": {"yolo": {"arrival": 1711329873.9635146, "serving": 1711329874.266058}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4492576, "arrival": 1711329873.6853516}, "models": {"yolo": {"arrival": 1711329873.3207316, "serving": 1711329873.6332297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.446279, "arrival": 1711329873.6615007}, "models": {"yolo": {"arrival": 1711329873.3340316, "serving": 1711329873.6283083}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4413884, "arrival": 1711329870.8894384}, "models": {"yolo": {"arrival": 1711329870.416551, "serving": 1711329870.8649597}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4411519, "arrival": 1711329873.9905407}, "models": {"yolo": {"arrival": 1711329873.6377995, "serving": 1711329873.9542434}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4625137, "arrival": 1711329871.7230659}, "models": {"yolo": {"arrival": 1711329871.1761587, "serving": 1711329871.699126}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4184878, "arrival": 1711329870.62059}, "models": {"yolo": {"arrival": 1711329870.1440556, "serving": 1711329870.4050272}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4494147, "arrival": 1711329873.9955046}, "models": {"yolo": {"arrival": 1711329873.6396024, "serving": 1711329873.9529965}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.612500235+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4185574}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4186285, "arrival": 1711329873.6574452}, "models": {"yolo": {"arrival": 1711329873.3319669, "serving": 1711329873.6292217}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.648299952+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.441502}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.322345258+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4183414}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4365902, "arrival": 1711329873.9800386}, "models": {"yolo": {"arrival": 1711329873.6377995, "serving": 1711329873.9542434}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4490526, "arrival": 1711329871.196189}, "models": {"yolo": {"arrival": 1711329870.8746135, "serving": 1711329871.16305}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.449491, "arrival": 1711329874.6980762}, "models": {"yolo": {"arrival": 1711329874.2728205, "serving": 1711329874.6324153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4185226, "arrival": 1711329874.0047338}, "models": {"yolo": {"arrival": 1711329873.6386092, "serving": 1711329873.9626906}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.648242878+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.436866}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.462465, "arrival": 1711329873.9938264}, "models": {"yolo": {"arrival": 1711329873.643312, "serving": 1711329873.9540617}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.648350836+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4461985}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.449376, "arrival": 1711329871.7200928}, "models": {"yolo": {"arrival": 1711329871.1761587, "serving": 1711329871.699126}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4490993, "arrival": 1711329873.6728776}, "models": {"yolo": {"arrival": 1711329873.3340316, "serving": 1711329873.6283083}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.64841624+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4493504}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.648337186+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4460728}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.35372583+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4495418}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4460886, "arrival": 1711329873.6781049}, "models": {"yolo": {"arrival": 1711329873.3207316, "serving": 1711329873.6332297}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.322386959+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.418663}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4185932, "arrival": 1711329873.0800273}, "models": {"yolo": {"arrival": 1711329872.7618132, "serving": 1711329873.0326653}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4190462, "arrival": 1711329870.621232}, "models": {"yolo": {"arrival": 1711329870.1440556, "serving": 1711329870.4050272}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4459035, "arrival": 1711329874.317002}, "models": {"yolo": {"arrival": 1711329873.9635146, "serving": 1711329874.266058}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4490654, "arrival": 1711329874.687448}, "models": {"yolo": {"arrival": 1711329874.2728205, "serving": 1711329874.6324153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4461524, "arrival": 1711329873.6593368}, "models": {"yolo": {"arrival": 1711329873.3340316, "serving": 1711329873.6283083}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4188025, "arrival": 1711329874.0093632}, "models": {"yolo": {"arrival": 1711329873.6386092, "serving": 1711329873.9626906}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4461677, "arrival": 1711329874.3272674}, "models": {"yolo": {"arrival": 1711329873.9635146, "serving": 1711329874.266058}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.44137, "arrival": 1711329873.328304}, "models": {"yolo": {"arrival": 1711329873.0427969, "serving": 1711329873.3092184}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4410906, "arrival": 1711329874.2856047}, "models": {"yolo": {"arrival": 1711329873.973017, "serving": 1711329874.2604613}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.322493996+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4368467}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4488945, "arrival": 1711329873.680035}, "models": {"yolo": {"arrival": 1711329873.3207316, "serving": 1711329873.6332297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4190109, "arrival": 1711329873.1232204}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.64827133+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4411922}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.632762138+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4492977}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.328320499+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4460561}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4184513, "arrival": 1711329873.122639}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4190812, "arrival": 1711329874.0099847}, "models": {"yolo": {"arrival": 1711329873.6386092, "serving": 1711329873.9626906}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4411318, "arrival": 1711329873.362447}, "models": {"yolo": {"arrival": 1711329873.0436668, "serving": 1711329873.3223054}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.631971365+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4186985}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4412928, "arrival": 1711329873.3629296}, "models": {"yolo": {"arrival": 1711329873.0436668, "serving": 1711329873.3223054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4462135, "arrival": 1711329873.6795633}, "models": {"yolo": {"arrival": 1711329873.3207316, "serving": 1711329873.6332297}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4458845, "arrival": 1711329873.3727505}, "models": {"yolo": {"arrival": 1711329873.0436668, "serving": 1711329873.3223054}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.322520339+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4410048}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.44604, "arrival": 1711329874.317413}, "models": {"yolo": {"arrival": 1711329873.9635146, "serving": 1711329874.266058}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.63271737+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4489682}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.632627359+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.441271}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4462948, "arrival": 1711329874.3277707}, "models": {"yolo": {"arrival": 1711329873.9635146, "serving": 1711329874.266058}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.446104, "arrival": 1711329871.1930854}, "models": {"yolo": {"arrival": 1711329870.8746135, "serving": 1711329871.16305}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4369228, "arrival": 1711329874.0105448}, "models": {"yolo": {"arrival": 1711329873.6386092, "serving": 1711329873.9626906}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.322467388+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.436674}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4412124, "arrival": 1711329873.3261838}, "models": {"yolo": {"arrival": 1711329873.0427969, "serving": 1711329873.3092184}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.632702916+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4462616}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4462457, "arrival": 1711329874.6864884}, "models": {"yolo": {"arrival": 1711329874.2728205, "serving": 1711329874.6324153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4493122, "arrival": 1711329873.994886}, "models": {"yolo": {"arrival": 1711329873.6396024, "serving": 1711329873.9529965}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.648403465+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4492445}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4368043, "arrival": 1711329873.3612516}, "models": {"yolo": {"arrival": 1711329873.0436668, "serving": 1711329873.3223054}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.340930159+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4491255}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.648227755+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4367008}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.334800541+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4463098}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.648390645+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4491386}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.612515764+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4188368}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.632747196+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4491904}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.349767188+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.449337}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.632612648+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4411097}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4461193, "arrival": 1711329874.3112104}, "models": {"yolo": {"arrival": 1711329873.973017, "serving": 1711329874.2604613}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4619617, "arrival": 1711329864.7947335}, "models": {"yolo": {"arrival": 1711329864.4290068, "serving": 1711329864.7589974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4414456, "arrival": 1711329873.372473}, "models": {"yolo": {"arrival": 1711329873.0436668, "serving": 1711329873.3223054}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.648257512+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4410274}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.449284, "arrival": 1711329874.6954548}, "models": {"yolo": {"arrival": 1711329874.2728205, "serving": 1711329874.6324153}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.322605307+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4414833}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.63258242+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4367847}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.322433023+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.418942}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.345095692+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4492307}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4408958, "arrival": 1711329873.3618503}, "models": {"yolo": {"arrival": 1711329873.0436668, "serving": 1711329873.3223054}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4367437, "arrival": 1711329870.8809621}, "models": {"yolo": {"arrival": 1711329870.416551, "serving": 1711329870.8649597}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4493248, "arrival": 1711329874.6848981}, "models": {"yolo": {"arrival": 1711329874.275795, "serving": 1711329874.6306205}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.632658337+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4458628}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4462283, "arrival": 1711329871.1950796}, "models": {"yolo": {"arrival": 1711329870.8746135, "serving": 1711329871.16305}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.463061, "arrival": 1711329874.933186}, "models": {"yolo": {"arrival": 1711329874.6408784, "serving": 1711329874.8879979}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.332265269+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4461834}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4492047, "arrival": 1711329873.6753845}, "models": {"yolo": {"arrival": 1711329873.3340316, "serving": 1711329873.6283083}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.632688315+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.446135}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.632673552+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4460065}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4489849, "arrival": 1711329873.6623144}, "models": {"yolo": {"arrival": 1711329873.3340316, "serving": 1711329873.6283083}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4495165, "arrival": 1711329873.997115}, "models": {"yolo": {"arrival": 1711329873.6396024, "serving": 1711329873.9529965}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4491127, "arrival": 1711329874.3289027}, "models": {"yolo": {"arrival": 1711329873.9635146, "serving": 1711329874.266058}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4493887, "arrival": 1711329874.6977792}, "models": {"yolo": {"arrival": 1711329874.2728205, "serving": 1711329874.6324153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4491777, "arrival": 1711329874.6878498}, "models": {"yolo": {"arrival": 1711329874.2728205, "serving": 1711329874.6324153}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4413128, "arrival": 1711329873.9913042}, "models": {"yolo": {"arrival": 1711329873.6377995, "serving": 1711329873.9542434}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4631002, "arrival": 1711329864.7872622}, "models": {"yolo": {"arrival": 1711329864.4315803, "serving": 1711329864.758022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4368255, "arrival": 1711329873.9889812}, "models": {"yolo": {"arrival": 1711329873.6377995, "serving": 1711329873.9542434}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4489987, "arrival": 1711329874.328347}, "models": {"yolo": {"arrival": 1711329873.9635146, "serving": 1711329874.266058}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4414062, "arrival": 1711329874.2900283}, "models": {"yolo": {"arrival": 1711329873.973017, "serving": 1711329874.2604613}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4410493, "arrival": 1711329873.3252492}, "models": {"yolo": {"arrival": 1711329873.0427969, "serving": 1711329873.3092184}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4459743, "arrival": 1711329870.8908622}, "models": {"yolo": {"arrival": 1711329870.416551, "serving": 1711329870.8649597}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.352024634+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4494405}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4494648, "arrival": 1711329873.9932225}, "models": {"yolo": {"arrival": 1711329873.643312, "serving": 1711329873.9540617}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.446024, "arrival": 1711329873.658719}, "models": {"yolo": {"arrival": 1711329873.3340316, "serving": 1711329873.6283083}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4494274, "arrival": 1711329874.685287}, "models": {"yolo": {"arrival": 1711329874.275795, "serving": 1711329874.6306205}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4493632, "arrival": 1711329873.9926329}, "models": {"yolo": {"arrival": 1711329873.643312, "serving": 1711329873.9540617}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:23.632732362+00:00\"}\"\n>", "times": {"request": {"sending": 1711329863.4490817}}, "outputs": []}, {"times": {"request": {"sending": 1711329863.4489367, "arrival": 1711329871.1957655}, "models": {"yolo": {"arrival": 1711329870.8746135, "serving": 1711329871.16305}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4495294, "arrival": 1711329874.6856809}, "models": {"yolo": {"arrival": 1711329874.275795, "serving": 1711329874.6306205}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.449478, "arrival": 1711329871.722409}, "models": {"yolo": {"arrival": 1711329871.1761587, "serving": 1711329871.699126}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4414651, "arrival": 1711329873.9919193}, "models": {"yolo": {"arrival": 1711329873.6377995, "serving": 1711329873.9542434}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.4368856, "arrival": 1711329873.1237693}, "models": {"yolo": {"arrival": 1711329871.202602, "serving": 1711329873.0092428}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329863.419152, "arrival": 1711329873.0811424}, "models": {"yolo": {"arrival": 1711329872.7618132, "serving": 1711329873.0326653}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.323287177+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.445921}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.632776666+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.4494014}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:23.648284691+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.25 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329863.441352}}, "outputs": []}], [{"times": {"request": {"sending": 1711329864.4430337, "arrival": 1711329874.9356906}, "models": {"yolo": {"arrival": 1711329874.6478896, "serving": 1711329874.887564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4145, "arrival": 1711329874.3297782}, "models": {"yolo": {"arrival": 1711329873.9662266, "serving": 1711329874.261092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.406644, "arrival": 1711329873.9942498}, "models": {"yolo": {"arrival": 1711329873.643312, "serving": 1711329873.9540617}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.406541, "arrival": 1711329873.9975748}, "models": {"yolo": {"arrival": 1711329873.6396024, "serving": 1711329873.9529965}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4066594, "arrival": 1711329871.725455}, "models": {"yolo": {"arrival": 1711329871.1761587, "serving": 1711329871.699126}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4137702, "arrival": 1711329874.9369419}, "models": {"yolo": {"arrival": 1711329874.6408784, "serving": 1711329874.8879979}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4335418, "arrival": 1711329875.6244502}, "models": {"yolo": {"arrival": 1711329875.2216759, "serving": 1711329875.5771666}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4374046, "arrival": 1711329874.684491}, "models": {"yolo": {"arrival": 1711329874.2690146, "serving": 1711329874.6249564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4138222, "arrival": 1711329874.895436}, "models": {"yolo": {"arrival": 1711329874.6393874, "serving": 1711329874.88046}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.534319988+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.414546}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4097157, "arrival": 1711329874.6951313}, "models": {"yolo": {"arrival": 1711329874.275795, "serving": 1711329874.6306205}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.505828534+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.41411}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.534206218+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4141762}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.436950162+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4426055}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4427044, "arrival": 1711329874.9377246}, "models": {"yolo": {"arrival": 1711329874.634155, "serving": 1711329874.8908896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4136164, "arrival": 1711329871.726685}, "models": {"yolo": {"arrival": 1711329871.1761587, "serving": 1711329871.699126}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4431276, "arrival": 1711329875.9257762}, "models": {"yolo": {"arrival": 1711329875.5879982, "serving": 1711329875.8772924}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4430027, "arrival": 1711329875.9254353}, "models": {"yolo": {"arrival": 1711329875.5879982, "serving": 1711329875.8772924}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.520222527+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4427574}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.414678, "arrival": 1711329874.682022}, "models": {"yolo": {"arrival": 1711329874.2690146, "serving": 1711329874.6249564}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.373592189+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4138367}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4135985, "arrival": 1711329874.0060847}, "models": {"yolo": {"arrival": 1711329873.643312, "serving": 1711329873.9540617}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4430492, "arrival": 1711329875.623809}, "models": {"yolo": {"arrival": 1711329875.2269886, "serving": 1711329875.5777419}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.414205, "arrival": 1711329871.9358327}, "models": {"yolo": {"arrival": 1711329871.7113438, "serving": 1711329871.9180427}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4374356, "arrival": 1711329875.6268733}, "models": {"yolo": {"arrival": 1711329875.2216759, "serving": 1711329875.5771666}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.36664579+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.41352}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.54684974+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4375155}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.534233566+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4142966}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4428728, "arrival": 1711329875.9251008}, "models": {"yolo": {"arrival": 1711329875.5879982, "serving": 1711329875.8772924}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.505853366+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4142354}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.505799844+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.413787}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.520281074+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4428914}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4142668, "arrival": 1711329874.8967838}, "models": {"yolo": {"arrival": 1711329874.6393874, "serving": 1711329874.88046}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.437219, "arrival": 1711329874.6970005}, "models": {"yolo": {"arrival": 1711329874.2752545, "serving": 1711329874.6381207}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4430962, "arrival": 1711329874.9439495}, "models": {"yolo": {"arrival": 1711329874.634155, "serving": 1711329874.8908896}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.382022505+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4142811}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.403285779+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4147675}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4136684, "arrival": 1711329874.3148103}, "models": {"yolo": {"arrival": 1711329873.9662266, "serving": 1711329874.261092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4066288, "arrival": 1711329864.7951887}, "models": {"yolo": {"arrival": 1711329864.4290068, "serving": 1711329864.7589974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4373443, "arrival": 1711329874.6989288}, "models": {"yolo": {"arrival": 1711329874.2752545, "serving": 1711329874.6381207}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.534118246+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.413718}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.534175584+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.413852}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4137356, "arrival": 1711329874.2801886}, "models": {"yolo": {"arrival": 1711329873.9629993, "serving": 1711329874.259588}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.398994624+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4146502}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4429865, "arrival": 1711329872.4946723}, "models": {"yolo": {"arrival": 1711329872.2514246, "serving": 1711329872.460239}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4142516, "arrival": 1711329874.3164585}, "models": {"yolo": {"arrival": 1711329873.9662266, "serving": 1711329874.261092}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.51017774+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4144833}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4065928, "arrival": 1711329874.6860683}, "models": {"yolo": {"arrival": 1711329874.275795, "serving": 1711329874.6306205}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4147396, "arrival": 1711329874.6960163}, "models": {"yolo": {"arrival": 1711329874.2752545, "serving": 1711329874.6381207}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4370296, "arrival": 1711329874.696666}, "models": {"yolo": {"arrival": 1711329874.2752545, "serving": 1711329874.6381207}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.546974974+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4429548}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4066732, "arrival": 1711329874.933972}, "models": {"yolo": {"arrival": 1711329874.6408784, "serving": 1711329874.8879979}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4427898, "arrival": 1711329875.6224635}, "models": {"yolo": {"arrival": 1711329875.2269886, "serving": 1711329875.5777419}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4110591, "arrival": 1711329864.7935176}, "models": {"yolo": {"arrival": 1711329864.4315803, "serving": 1711329864.758022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.377825577+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.414162}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.411041, "arrival": 1711329874.9344227}, "models": {"yolo": {"arrival": 1711329874.6408784, "serving": 1711329874.8879979}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.520161913+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4375784}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4067035, "arrival": 1711329874.006357}, "models": {"yolo": {"arrival": 1711329873.6396024, "serving": 1711329873.9529965}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4147077, "arrival": 1711329875.2483802}, "models": {"yolo": {"arrival": 1711329874.8978007, "serving": 1711329875.2100096}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.459360383+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.443065}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4372976, "arrival": 1711329872.2744026}, "models": {"yolo": {"arrival": 1711329871.9292145, "serving": 1711329872.240279}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4110782, "arrival": 1711329874.0066335}, "models": {"yolo": {"arrival": 1711329873.6396024, "serving": 1711329873.9529965}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.387374489+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4144099}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.407427723+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4334745}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.370939365+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4137022}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.534386113+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.433273}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.392112118+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4145308}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.414574, "arrival": 1711329872.2634416}, "models": {"yolo": {"arrival": 1711329871.9292145, "serving": 1711329872.240279}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4427218, "arrival": 1711329872.4755905}, "models": {"yolo": {"arrival": 1711329872.2514246, "serving": 1711329872.460239}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4375303, "arrival": 1711329874.9373367}, "models": {"yolo": {"arrival": 1711329874.634155, "serving": 1711329874.8908896}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.546806977+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4373894}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.534010477+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4135768}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.546627708+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4334908}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4146929, "arrival": 1711329872.2642725}, "models": {"yolo": {"arrival": 1711329871.9292145, "serving": 1711329872.240279}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.413753, "arrival": 1711329871.9285102}, "models": {"yolo": {"arrival": 1711329871.7113438, "serving": 1711329871.9180427}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4334006, "arrival": 1711329875.6215272}, "models": {"yolo": {"arrival": 1711329875.2216759, "serving": 1711329875.5771666}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.546888972+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4426835}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.505732296+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4136496}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.41097, "arrival": 1711329874.0033922}, "models": {"yolo": {"arrival": 1711329873.643312, "serving": 1711329873.9540617}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.43259989+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4373732}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4429066, "arrival": 1711329874.9352689}, "models": {"yolo": {"arrival": 1711329874.6478896, "serving": 1711329874.887564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.433525, "arrival": 1711329872.2730808}, "models": {"yolo": {"arrival": 1711329871.9292145, "serving": 1711329872.240279}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.534265831+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.414425}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4098027, "arrival": 1711329864.795634}, "models": {"yolo": {"arrival": 1711329864.4290068, "serving": 1711329864.7589974}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.414621, "arrival": 1711329874.6947832}, "models": {"yolo": {"arrival": 1711329874.2752545, "serving": 1711329874.6381207}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4138997, "arrival": 1711329875.2192895}, "models": {"yolo": {"arrival": 1711329874.8978007, "serving": 1711329875.2100096}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4144397, "arrival": 1711329874.3095698}, "models": {"yolo": {"arrival": 1711329873.9629993, "serving": 1711329874.259588}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.520089286+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4374506}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.414341, "arrival": 1711329875.2408807}, "models": {"yolo": {"arrival": 1711329874.8978007, "serving": 1711329875.2100096}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.437154, "arrival": 1711329874.6836503}, "models": {"yolo": {"arrival": 1711329874.2690146, "serving": 1711329874.6249564}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.357255511+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4066117}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4428406, "arrival": 1711329874.9381151}, "models": {"yolo": {"arrival": 1711329874.634155, "serving": 1711329874.8908896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.414326, "arrival": 1711329871.9367714}, "models": {"yolo": {"arrival": 1711329871.7113438, "serving": 1711329871.9180427}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4373136, "arrival": 1711329875.6259477}, "models": {"yolo": {"arrival": 1711329875.2216759, "serving": 1711329875.5771666}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4373589, "arrival": 1711329875.589346}, "models": {"yolo": {"arrival": 1711329875.2269886, "serving": 1711329875.5777419}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.546933309+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4428241}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.432669855+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4375005}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4372818, "arrival": 1711329874.6840718}, "models": {"yolo": {"arrival": 1711329874.2690146, "serving": 1711329874.6249564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4138677, "arrival": 1711329874.2844462}, "models": {"yolo": {"arrival": 1711329873.9629993, "serving": 1711329874.259588}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.41413, "arrival": 1711329874.3159015}, "models": {"yolo": {"arrival": 1711329873.9662266, "serving": 1711329874.261092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4147537, "arrival": 1711329875.255341}, "models": {"yolo": {"arrival": 1711329874.889875, "serving": 1711329875.2159274}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.51026706+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4147232}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4371855, "arrival": 1711329875.625343}, "models": {"yolo": {"arrival": 1711329875.2216759, "serving": 1711329875.5771666}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.519985977+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4373279}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4334571, "arrival": 1711329875.2624476}, "models": {"yolo": {"arrival": 1711329874.889875, "serving": 1711329875.2159274}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.413404568+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4371178}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4143865, "arrival": 1711329875.2538886}, "models": {"yolo": {"arrival": 1711329874.889875, "serving": 1711329875.2159274}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.433508, "arrival": 1711329874.6832228}, "models": {"yolo": {"arrival": 1711329874.2690146, "serving": 1711329874.6249564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4143722, "arrival": 1711329874.3294857}, "models": {"yolo": {"arrival": 1711329873.9662266, "serving": 1711329874.261092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.437546, "arrival": 1711329872.4749396}, "models": {"yolo": {"arrival": 1711329872.2514246, "serving": 1711329872.460239}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4141462, "arrival": 1711329874.8961222}, "models": {"yolo": {"arrival": 1711329874.6393874, "serving": 1711329874.88046}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4374692, "arrival": 1711329874.6992006}, "models": {"yolo": {"arrival": 1711329874.2752545, "serving": 1711329874.6381207}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.43717, "arrival": 1711329872.2738416}, "models": {"yolo": {"arrival": 1711329871.9292145, "serving": 1711329872.240279}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.414589, "arrival": 1711329875.2479088}, "models": {"yolo": {"arrival": 1711329874.8978007, "serving": 1711329875.2100096}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4372344, "arrival": 1711329875.263977}, "models": {"yolo": {"arrival": 1711329874.889875, "serving": 1711329875.2159274}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.417632823+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4372506}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.546766018+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4372659}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.443112, "arrival": 1711329872.4952676}, "models": {"yolo": {"arrival": 1711329872.2514246, "serving": 1711329872.460239}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.510307531+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4334207}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.41422, "arrival": 1711329875.2348888}, "models": {"yolo": {"arrival": 1711329874.8978007, "serving": 1711329875.2100096}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4110212, "arrival": 1711329871.7260995}, "models": {"yolo": {"arrival": 1711329871.1761587, "serving": 1711329871.699126}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4376242, "arrival": 1711329875.5922992}, "models": {"yolo": {"arrival": 1711329875.2269886, "serving": 1711329875.5777419}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.547014577+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4430802}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.43344, "arrival": 1711329874.6963015}, "models": {"yolo": {"arrival": 1711329874.2752545, "serving": 1711329874.6381207}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4144535, "arrival": 1711329872.2595274}, "models": {"yolo": {"arrival": 1711329871.9292145, "serving": 1711329872.240279}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4374847, "arrival": 1711329875.5914896}, "models": {"yolo": {"arrival": 1711329875.2269886, "serving": 1711329875.5777419}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4066875, "arrival": 1711329864.788036}, "models": {"yolo": {"arrival": 1711329864.4315803, "serving": 1711329864.758022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4429226, "arrival": 1711329875.6231763}, "models": {"yolo": {"arrival": 1711329875.2269886, "serving": 1711329875.5777419}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.546711059+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4371357}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4141903, "arrival": 1711329874.286273}, "models": {"yolo": {"arrival": 1711329873.9629993, "serving": 1711329874.259588}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.510356479+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4335585}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4136329, "arrival": 1711329874.9348526}, "models": {"yolo": {"arrival": 1711329874.6408784, "serving": 1711329874.8879979}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4429705, "arrival": 1711329874.9385562}, "models": {"yolo": {"arrival": 1711329874.634155, "serving": 1711329874.8908896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4374201, "arrival": 1711329872.2783546}, "models": {"yolo": {"arrival": 1711329871.9292145, "serving": 1711329872.240279}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.53434664+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4146638}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.454223761+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4429388}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4333813, "arrival": 1711329872.272274}, "models": {"yolo": {"arrival": 1711329871.9292145, "serving": 1711329872.240279}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.51043897+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4372013}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.510242485+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.4146032}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4145596, "arrival": 1711329874.3104212}, "models": {"yolo": {"arrival": 1711329873.9629993, "serving": 1711329874.259588}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.361354694+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4097815}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4110944, "arrival": 1711329874.6972675}, "models": {"yolo": {"arrival": 1711329874.275795, "serving": 1711329874.6306205}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.413685, "arrival": 1711329874.894477}, "models": {"yolo": {"arrival": 1711329874.6393874, "serving": 1711329874.88046}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4376094, "arrival": 1711329874.699471}, "models": {"yolo": {"arrival": 1711329874.2752545, "serving": 1711329874.6381207}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4143105, "arrival": 1711329874.28697}, "models": {"yolo": {"arrival": 1711329873.9629993, "serving": 1711329874.259588}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.520337989+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4430177}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.4427402, "arrival": 1711329875.9247584}, "models": {"yolo": {"arrival": 1711329875.5879982, "serving": 1711329875.8772924}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4427736, "arrival": 1711329874.9211683}, "models": {"yolo": {"arrival": 1711329874.6478896, "serving": 1711329874.887564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4146352, "arrival": 1711329875.2549214}, "models": {"yolo": {"arrival": 1711329874.889875, "serving": 1711329875.2159274}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4144676, "arrival": 1711329875.24744}, "models": {"yolo": {"arrival": 1711329874.8978007, "serving": 1711329875.2100096}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4138064, "arrival": 1711329874.315365}, "models": {"yolo": {"arrival": 1711329873.9662266, "serving": 1711329874.261092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4333582, "arrival": 1711329874.682767}, "models": {"yolo": {"arrival": 1711329874.2690146, "serving": 1711329874.6249564}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.520395753+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.4431434}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.505875922+00:00\"}\"\n>", "times": {"request": {"sending": 1711329864.414355}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.437561, "arrival": 1711329875.6401207}, "models": {"yolo": {"arrival": 1711329875.2216759, "serving": 1711329875.5771666}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.437097, "arrival": 1711329875.2636588}, "models": {"yolo": {"arrival": 1711329874.889875, "serving": 1711329875.2159274}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4145153, "arrival": 1711329875.2544208}, "models": {"yolo": {"arrival": 1711329874.889875, "serving": 1711329875.2159274}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.447378348+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329864.442807}}, "outputs": []}, {"times": {"request": {"sending": 1711329864.442856, "arrival": 1711329872.4759142}, "models": {"yolo": {"arrival": 1711329872.2514246, "serving": 1711329872.460239}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329864.4138837, "arrival": 1711329871.9324443}, "models": {"yolo": {"arrival": 1711329871.7113438, "serving": 1711329871.9180427}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329865.4109669, "arrival": 1711329875.2422142}, "models": {"yolo": {"arrival": 1711329874.8994908, "serving": 1711329875.2095482}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4100208, "arrival": 1711329874.9361293}, "models": {"yolo": {"arrival": 1711329874.6478896, "serving": 1711329874.887564}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.465242, "arrival": 1711329875.9295712}, "models": {"yolo": {"arrival": 1711329875.5937958, "serving": 1711329875.8808944}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.465397, "arrival": 1711329877.178262}, "models": {"yolo": {"arrival": 1711329876.919908, "serving": 1711329877.1696074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4425502, "arrival": 1711329877.4999297}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.156360781+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4638371}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4641654, "arrival": 1711329873.0995133}, "models": {"yolo": {"arrival": 1711329872.7815928, "serving": 1711329873.0479429}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.464828, "arrival": 1711329875.9236193}, "models": {"yolo": {"arrival": 1711329875.5937958, "serving": 1711329875.8808944}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.411314, "arrival": 1711329875.889226}, "models": {"yolo": {"arrival": 1711329875.585136, "serving": 1711329875.8711643}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.856616405+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4378633}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.61038123+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4649136}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.51959275+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4116852}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.437886, "arrival": 1711329875.2589104}, "models": {"yolo": {"arrival": 1711329874.8974326, "serving": 1711329875.2202377}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.151125608+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4427094}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.46421, "arrival": 1711329875.6286569}, "models": {"yolo": {"arrival": 1711329875.231086, "serving": 1711329875.577565}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.15647997+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4646554}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4110367, "arrival": 1711329872.7963383}, "models": {"yolo": {"arrival": 1711329872.4730222, "serving": 1711329872.767019}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4101646, "arrival": 1711329875.8855894}, "models": {"yolo": {"arrival": 1711329875.585136, "serving": 1711329875.8711643}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.6102547+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4643753}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4647598, "arrival": 1711329875.6409326}, "models": {"yolo": {"arrival": 1711329875.231086, "serving": 1711329875.577565}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.52048404+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4111738}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4650166, "arrival": 1711329877.5926013}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4652717, "arrival": 1711329876.9529562}, "models": {"yolo": {"arrival": 1711329876.6438863, "serving": 1711329876.9111536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4637017, "arrival": 1711329875.2653508}, "models": {"yolo": {"arrival": 1711329874.8974326, "serving": 1711329875.2202377}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.865810927+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.464871}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.465383613+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4102423}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4646075, "arrival": 1711329875.6403906}, "models": {"yolo": {"arrival": 1711329875.231086, "serving": 1711329875.577565}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.547049482+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4103148}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.465074, "arrival": 1711329873.1133394}, "models": {"yolo": {"arrival": 1711329872.7815928, "serving": 1711329873.0479429}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4648852, "arrival": 1711329875.9119732}, "models": {"yolo": {"arrival": 1711329875.5875785, "serving": 1711329875.8777342}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.410479, "arrival": 1711329872.4957347}, "models": {"yolo": {"arrival": 1711329872.2514246, "serving": 1711329872.460239}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.865772203+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4647412}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4111044, "arrival": 1711329875.9264748}, "models": {"yolo": {"arrival": 1711329875.5879982, "serving": 1711329875.8772924}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4425967, "arrival": 1711329875.2454276}, "models": {"yolo": {"arrival": 1711329874.8994908, "serving": 1711329875.2095482}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.621894984+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4651468}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4430027, "arrival": 1711329876.6636388}, "models": {"yolo": {"arrival": 1711329876.353333, "serving": 1711329876.6351752}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4644523, "arrival": 1711329876.6771252}, "models": {"yolo": {"arrival": 1711329876.353333, "serving": 1711329876.6351752}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.442679, "arrival": 1711329877.5003755}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4429884, "arrival": 1711329872.8020844}, "models": {"yolo": {"arrival": 1711329872.4730222, "serving": 1711329872.767019}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4648993, "arrival": 1711329877.5549831}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.464857, "arrival": 1711329876.9284494}, "models": {"yolo": {"arrival": 1711329876.6438863, "serving": 1711329876.9111536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4107587, "arrival": 1711329875.888133}, "models": {"yolo": {"arrival": 1711329875.585136, "serving": 1711329875.8711643}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4426267, "arrival": 1711329876.3925927}, "models": {"yolo": {"arrival": 1711329875.886728, "serving": 1711329876.3437266}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4640896, "arrival": 1711329875.627948}, "models": {"yolo": {"arrival": 1711329875.231086, "serving": 1711329875.577565}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4105499, "arrival": 1711329875.9261143}, "models": {"yolo": {"arrival": 1711329875.5879982, "serving": 1711329875.8772924}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.150956362+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4113965}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.463796, "arrival": 1711329877.5092044}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4424977, "arrival": 1711329876.3889992}, "models": {"yolo": {"arrival": 1711329875.886728, "serving": 1711329876.3437266}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.547085988+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.31 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.410898}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.519689238+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4426937}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4650025, "arrival": 1711329875.9127572}, "models": {"yolo": {"arrival": 1711329875.5875785, "serving": 1711329875.8777342}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.610309729+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.464516}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4103854, "arrival": 1711329874.9445133}, "models": {"yolo": {"arrival": 1711329874.634155, "serving": 1711329874.8908896}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4653811, "arrival": 1711329873.3694305}, "models": {"yolo": {"arrival": 1711329873.0602767, "serving": 1711329873.327069}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.151080233+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.442446}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4643166, "arrival": 1711329876.6694715}, "models": {"yolo": {"arrival": 1711329876.353333, "serving": 1711329876.6351752}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4646218, "arrival": 1711329877.511392}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4106896, "arrival": 1711329874.9365404}, "models": {"yolo": {"arrival": 1711329874.6478896, "serving": 1711329874.887564}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.520439536+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.41062}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4426115, "arrival": 1711329872.801365}, "models": {"yolo": {"arrival": 1711329872.4730222, "serving": 1711329872.767019}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.442354, "arrival": 1711329876.3883429}, "models": {"yolo": {"arrival": 1711329875.886728, "serving": 1711329876.3437266}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.856550993+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4115646}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.470870917+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 862.00 MiB (GPU 0; 23.64 GiB total capacity; 1.66 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4108286}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.151026593+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.437686}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.519734818+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.463818}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4429731, "arrival": 1711329875.6392426}, "models": {"yolo": {"arrival": 1711329875.2199318, "serving": 1711329875.5809436}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.442482, "arrival": 1711329872.8010106}, "models": {"yolo": {"arrival": 1711329872.4730222, "serving": 1711329872.767019}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.519525618+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.411357}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4642246, "arrival": 1711329877.5099525}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4645443, "arrival": 1711329875.6419928}, "models": {"yolo": {"arrival": 1711329875.2199318, "serving": 1711329875.5809436}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.464672, "arrival": 1711329875.923255}, "models": {"yolo": {"arrival": 1711329875.5937958, "serving": 1711329875.8808944}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.519648295+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4424295}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.865877861+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4651036}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.610358674+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4647977}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4116452, "arrival": 1711329875.8910365}, "models": {"yolo": {"arrival": 1711329875.585136, "serving": 1711329875.8711643}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4641814, "arrival": 1711329876.6687264}, "models": {"yolo": {"arrival": 1711329876.353333, "serving": 1711329876.6351752}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4112499, "arrival": 1711329875.2570183}, "models": {"yolo": {"arrival": 1711329874.8974326, "serving": 1711329875.2202377}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4649754, "arrival": 1711329876.9382713}, "models": {"yolo": {"arrival": 1711329876.6438863, "serving": 1711329876.9111536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4378407, "arrival": 1711329876.3864238}, "models": {"yolo": {"arrival": 1711329875.886728, "serving": 1711329876.3437266}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.610430399+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.465031}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.15657037+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4652257}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4649446, "arrival": 1711329875.9240391}, "models": {"yolo": {"arrival": 1711329875.5937958, "serving": 1711329875.8808944}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.610334169+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4646392}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4116056, "arrival": 1711329875.2574344}, "models": {"yolo": {"arrival": 1711329874.8974326, "serving": 1711329875.2202377}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.519754721+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4641206}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.156385834+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.464135}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4651325, "arrival": 1711329877.59297}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4428482, "arrival": 1711329875.6389692}, "models": {"yolo": {"arrival": 1711329875.2199318, "serving": 1711329875.5809436}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4115248, "arrival": 1711329876.385999}, "models": {"yolo": {"arrival": 1711329875.886728, "serving": 1711329876.3437266}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4425328, "arrival": 1711329875.2633624}, "models": {"yolo": {"arrival": 1711329874.8974326, "serving": 1711329875.2202377}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.15654815+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4650457}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4645584, "arrival": 1711329873.1004095}, "models": {"yolo": {"arrival": 1711329872.7815928, "serving": 1711329873.0479429}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.465256, "arrival": 1711329873.3691323}, "models": {"yolo": {"arrival": 1711329873.0602767, "serving": 1711329873.327069}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.156301954+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4429584}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.860019607+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.442647}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.60029189+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4642396}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4650877, "arrival": 1711329876.939439}, "models": {"yolo": {"arrival": 1711329876.6438863, "serving": 1711329876.9111536}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4653647, "arrival": 1711329875.9299674}, "models": {"yolo": {"arrival": 1711329875.5937958, "serving": 1711329875.8808944}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.519669211+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4425654}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4641495, "arrival": 1711329875.6398306}, "models": {"yolo": {"arrival": 1711329875.2199318, "serving": 1711329875.5809436}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4650602, "arrival": 1711329875.924399}, "models": {"yolo": {"arrival": 1711329875.5937958, "serving": 1711329875.8808944}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4379063, "arrival": 1711329875.8918293}, "models": {"yolo": {"arrival": 1711329875.585136, "serving": 1711329875.8711643}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4642706, "arrival": 1711329875.6414664}, "models": {"yolo": {"arrival": 1711329875.2199318, "serving": 1711329875.5809436}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.464706, "arrival": 1711329873.1076875}, "models": {"yolo": {"arrival": 1711329872.7815928, "serving": 1711329873.0479429}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4423974, "arrival": 1711329875.2593641}, "models": {"yolo": {"arrival": 1711329874.8974326, "serving": 1711329875.2202377}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4114773, "arrival": 1711329872.7966607}, "models": {"yolo": {"arrival": 1711329872.4730222, "serving": 1711329872.767019}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.156457165+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4645295}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4426632, "arrival": 1711329875.2648044}, "models": {"yolo": {"arrival": 1711329874.8974326, "serving": 1711329875.2202377}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4641051, "arrival": 1711329877.509636}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.156593505+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4653463}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4428813, "arrival": 1711329876.3945515}, "models": {"yolo": {"arrival": 1711329875.886728, "serving": 1711329876.3437266}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4429288, "arrival": 1711329877.5008264}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.465302, "arrival": 1711329875.9227643}, "models": {"yolo": {"arrival": 1711329875.5875785, "serving": 1711329875.8777342}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.442313, "arrival": 1711329875.2444675}, "models": {"yolo": {"arrival": 1711329874.8994908, "serving": 1711329875.2095482}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.86279311+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4428957}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.862847954+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4430168}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4643462, "arrival": 1711329875.636426}, "models": {"yolo": {"arrival": 1711329875.231086, "serving": 1711329875.577565}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4651177, "arrival": 1711329875.9138608}, "models": {"yolo": {"arrival": 1711329875.5875785, "serving": 1711329875.8777342}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.156502218+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4648135}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4653156, "arrival": 1711329877.5933504}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4424655, "arrival": 1711329875.2450132}, "models": {"yolo": {"arrival": 1711329874.8994908, "serving": 1711329875.2095482}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.862877402+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4640684}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4642854, "arrival": 1711329873.0998027}, "models": {"yolo": {"arrival": 1711329872.7815928, "serving": 1711329873.0479429}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.865946722+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4654133}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4638581, "arrival": 1711329875.6395657}, "models": {"yolo": {"arrival": 1711329875.2199318, "serving": 1711329875.5809436}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.519621031+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4379272}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.437816, "arrival": 1711329872.796939}, "models": {"yolo": {"arrival": 1711329872.4730222, "serving": 1711329872.767019}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4377847, "arrival": 1711329875.243748}, "models": {"yolo": {"arrival": 1711329874.8994908, "serving": 1711329875.2095482}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4645011, "arrival": 1711329877.5105479}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.156526114+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4649298}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.156435371+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.464402}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4648414, "arrival": 1711329873.1127443}, "models": {"yolo": {"arrival": 1711329872.7815928, "serving": 1711329873.0479429}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4114363, "arrival": 1711329875.2430675}, "models": {"yolo": {"arrival": 1711329874.8994908, "serving": 1711329875.2095482}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4640317, "arrival": 1711329872.8023672}, "models": {"yolo": {"arrival": 1711329872.4730222, "serving": 1711329872.767019}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.621981011+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4653327}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4429114, "arrival": 1711329875.2650785}, "models": {"yolo": {"arrival": 1711329874.8974326, "serving": 1711329875.2202377}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.865706332+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.464591}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4423351, "arrival": 1711329872.7972126}, "models": {"yolo": {"arrival": 1711329872.4730222, "serving": 1711329872.767019}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.156410509+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4642563}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.86591419+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.465288}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4647238, "arrival": 1711329876.9275978}, "models": {"yolo": {"arrival": 1711329876.6438863, "serving": 1711329876.9111536}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.151054776+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4422317}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4649603, "arrival": 1711329873.1130898}, "models": {"yolo": {"arrival": 1711329872.7815928, "serving": 1711329873.0479429}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4643602, "arrival": 1711329877.510258}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.151103617+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4425807}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.862903676+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4641957}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.464777, "arrival": 1711329877.5116491}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.862926851+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4643304}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.859990914+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.442515}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4644198, "arrival": 1711329875.6417308}, "models": {"yolo": {"arrival": 1711329875.2199318, "serving": 1711329875.5809436}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4644372, "arrival": 1711329873.1001139}, "models": {"yolo": {"arrival": 1711329872.7815928, "serving": 1711329873.0479429}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4640508, "arrival": 1711329876.6645377}, "models": {"yolo": {"arrival": 1711329876.353333, "serving": 1711329876.6351752}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4645755, "arrival": 1711329876.9248025}, "models": {"yolo": {"arrival": 1711329876.6438863, "serving": 1711329876.9111536}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.865846314+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4649887}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:25.86294933+00:00\"}\"\n>", "times": {"request": {"sending": 1711329865.4644663}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.442413, "arrival": 1711329877.4995103}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.519712521+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4429436}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:25.859934237+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329865.4423711}}, "outputs": []}, {"times": {"request": {"sending": 1711329865.4644833, "arrival": 1711329875.6369133}, "models": {"yolo": {"arrival": 1711329875.231086, "serving": 1711329875.577565}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329865.4428647, "arrival": 1711329872.8017933}, "models": {"yolo": {"arrival": 1711329872.4730222, "serving": 1711329872.767019}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329866.4188192, "arrival": 1711329875.927156}, "models": {"yolo": {"arrival": 1711329875.5875785, "serving": 1711329875.8777342}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.622035018+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4189756}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4633522, "arrival": 1711329877.588577}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4192998, "arrival": 1711329877.5939398}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617329497+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.463004}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.45252, "arrival": 1711329877.586668}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.418921, "arrival": 1711329877.5936563}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4632118, "arrival": 1711329876.9410796}, "models": {"yolo": {"arrival": 1711329876.6516876, "serving": 1711329876.912254}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4203384, "arrival": 1711329876.3819501}, "models": {"yolo": {"arrival": 1711329875.887037, "serving": 1711329876.3435488}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4195013, "arrival": 1711329877.1994503}, "models": {"yolo": {"arrival": 1711329876.919908, "serving": 1711329877.1696074}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.617473612+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.463568}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.452797, "arrival": 1711329876.6791742}, "models": {"yolo": {"arrival": 1711329876.3523932, "serving": 1711329876.641859}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4527483, "arrival": 1711329873.6843271}, "models": {"yolo": {"arrival": 1711329873.3398054, "serving": 1711329873.6348298}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4199216, "arrival": 1711329876.3851695}, "models": {"yolo": {"arrival": 1711329875.891245, "serving": 1711329876.3436136}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4191077, "arrival": 1711329873.3697214}, "models": {"yolo": {"arrival": 1711329873.0602767, "serving": 1711329873.327069}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4626086, "arrival": 1711329877.8338516}, "models": {"yolo": {"arrival": 1711329877.456729, "serving": 1711329877.8168595}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.597657118+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4190216}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4625735, "arrival": 1711329876.680991}, "models": {"yolo": {"arrival": 1711329876.35364, "serving": 1711329876.6406353}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.622268742+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4200752}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4198208, "arrival": 1711329876.3800092}, "models": {"yolo": {"arrival": 1711329875.887037, "serving": 1711329876.3435488}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.419065, "arrival": 1711329875.930312}, "models": {"yolo": {"arrival": 1711329875.5937958, "serving": 1711329875.8808944}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.622088597+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4193406}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.61708023+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4192166}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.622501532+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4522703}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4202302, "arrival": 1711329877.2219536}, "models": {"yolo": {"arrival": 1711329876.919908, "serving": 1711329877.1696074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4637215, "arrival": 1711329877.5893052}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.419769, "arrival": 1711329877.22107}, "models": {"yolo": {"arrival": 1711329876.919908, "serving": 1711329877.1696074}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4191725, "arrival": 1711329877.1982496}, "models": {"yolo": {"arrival": 1711329876.919908, "serving": 1711329877.1696074}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.617256041+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.452143}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.619727149+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4633853}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617166526+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4450488}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.419259, "arrival": 1711329875.927812}, "models": {"yolo": {"arrival": 1711329875.5875785, "serving": 1711329875.8777342}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617204815+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4198964}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4194617, "arrival": 1711329873.3720248}, "models": {"yolo": {"arrival": 1711329873.0602767, "serving": 1711329873.327069}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4199727, "arrival": 1711329877.2215447}, "models": {"yolo": {"arrival": 1711329876.919908, "serving": 1711329877.1696074}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:27.529021222+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4636288}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4524393, "arrival": 1711329876.6713157}, "models": {"yolo": {"arrival": 1711329876.35364, "serving": 1711329876.6406353}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:27.529092359+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.463751}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.680197346+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4633684}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.617366431+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4523559}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.622788219+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.462677}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4201832, "arrival": 1711329873.373686}, "models": {"yolo": {"arrival": 1711329873.0602767, "serving": 1711329873.327069}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4635828, "arrival": 1711329876.9537663}, "models": {"yolo": {"arrival": 1711329876.6516876, "serving": 1711329876.912254}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617128044+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4197936}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.597704596+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4193804}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.419719, "arrival": 1711329876.3847444}, "models": {"yolo": {"arrival": 1711329875.891245, "serving": 1711329876.3436136}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4624693, "arrival": 1711329877.8324873}, "models": {"yolo": {"arrival": 1711329877.456729, "serving": 1711329877.8168595}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617413128+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.462625}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4194212, "arrival": 1711329876.3842778}, "models": {"yolo": {"arrival": 1711329875.891245, "serving": 1711329876.3436136}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4634013, "arrival": 1711329876.95701}, "models": {"yolo": {"arrival": 1711329876.650932, "serving": 1711329876.9244323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4521966, "arrival": 1711329877.4704664}, "models": {"yolo": {"arrival": 1711329877.1799319, "serving": 1711329877.447607}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617193258+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4196942}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4521616, "arrival": 1711329876.6700883}, "models": {"yolo": {"arrival": 1711329876.35364, "serving": 1711329876.6406353}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617113879+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.419541}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4632747, "arrival": 1711329876.9564424}, "models": {"yolo": {"arrival": 1711329876.650932, "serving": 1711329876.9244323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4449449, "arrival": 1711329876.3919826}, "models": {"yolo": {"arrival": 1711329875.891245, "serving": 1711329876.3436136}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.452305, "arrival": 1711329876.6707149}, "models": {"yolo": {"arrival": 1711329876.35364, "serving": 1711329876.6406353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4636443, "arrival": 1711329876.9658592}, "models": {"yolo": {"arrival": 1711329876.650932, "serving": 1711329876.9244323}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.622451037+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4521213}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.622621893+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4525363}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4527287, "arrival": 1711329876.6719015}, "models": {"yolo": {"arrival": 1711329876.35364, "serving": 1711329876.6406353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4197445, "arrival": 1711329873.3730183}, "models": {"yolo": {"arrival": 1711329873.0602767, "serving": 1711329873.327069}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4626422, "arrival": 1711329876.681824}, "models": {"yolo": {"arrival": 1711329876.3523932, "serving": 1711329876.641859}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.622669876+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.462289}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.622316557+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4447887}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4636905, "arrival": 1711329867.7291865}, "models": {"yolo": {"arrival": 1711329867.4958222, "serving": 1711329867.715172}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4195824, "arrival": 1711329876.3724346}, "models": {"yolo": {"arrival": 1711329875.887037, "serving": 1711329876.3435488}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4523895, "arrival": 1711329877.586141}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.617377893+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4524848}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4523716, "arrival": 1711329876.6784263}, "models": {"yolo": {"arrival": 1711329876.3523932, "serving": 1711329876.641859}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.45247, "arrival": 1711329877.4724119}, "models": {"yolo": {"arrival": 1711329877.1799319, "serving": 1711329877.447607}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4632893, "arrival": 1711329874.0058177}, "models": {"yolo": {"arrival": 1711329873.6460247, "serving": 1711329873.9633944}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.452501, "arrival": 1711329876.6788034}, "models": {"yolo": {"arrival": 1711329876.3523932, "serving": 1711329876.641859}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4625225, "arrival": 1711329877.587332}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4635372, "arrival": 1711329874.0108194}, "models": {"yolo": {"arrival": 1711329873.6460247, "serving": 1711329873.9633944}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.61734196+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4631364}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4634166, "arrival": 1711329874.0097065}, "models": {"yolo": {"arrival": 1711329873.6460247, "serving": 1711329873.9633944}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4199476, "arrival": 1711329873.373289}, "models": {"yolo": {"arrival": 1711329873.0602767, "serving": 1711329873.327069}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617424039+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4630718}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4200242, "arrival": 1711329876.3808632}, "models": {"yolo": {"arrival": 1711329875.887037, "serving": 1711329876.3435488}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.622371296+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4451506}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.692034547+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4634917}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4626608, "arrival": 1711329877.5876362}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.420133, "arrival": 1711329876.3855836}, "models": {"yolo": {"arrival": 1711329875.891245, "serving": 1711329876.3436136}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.62215811+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4196682}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617141048+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4199977}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.617153017+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.420283}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4790535, "arrival": 1711329867.7300737}, "models": {"yolo": {"arrival": 1711329867.4958222, "serving": 1711329867.715172}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.617388476+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4527802}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4637833, "arrival": 1711329874.0115035}, "models": {"yolo": {"arrival": 1711329873.6460247, "serving": 1711329873.9633944}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.463706, "arrival": 1711329876.954386}, "models": {"yolo": {"arrival": 1711329876.6516876, "serving": 1711329876.912254}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4196403, "arrival": 1711329877.5825775}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.452233, "arrival": 1711329876.6780257}, "models": {"yolo": {"arrival": 1711329876.3523932, "serving": 1711329876.641859}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.42005, "arrival": 1711329877.5836482}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4634318, "arrival": 1711329877.8467999}, "models": {"yolo": {"arrival": 1711329877.456729, "serving": 1711329877.8168595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4637666, "arrival": 1711329876.9666438}, "models": {"yolo": {"arrival": 1711329876.650932, "serving": 1711329876.9244323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4198463, "arrival": 1711329877.5831034}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617282773+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4524214}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4630241, "arrival": 1711329876.9552674}, "models": {"yolo": {"arrival": 1711329876.650932, "serving": 1711329876.9244323}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.617243618+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4451828}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4521801, "arrival": 1711329873.6767666}, "models": {"yolo": {"arrival": 1711329873.3398054, "serving": 1711329873.6348298}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.463337, "arrival": 1711329876.943109}, "models": {"yolo": {"arrival": 1711329876.6516876, "serving": 1711329876.912254}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617294203+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4525526}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.61723133+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4449055}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.463659, "arrival": 1711329874.0112169}, "models": {"yolo": {"arrival": 1711329873.6460247, "serving": 1711329873.9633944}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.692090114+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4636142}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617180443+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4453115}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4635983, "arrival": 1711329877.5818224}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.622569127+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4524057}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4452152, "arrival": 1711329876.3940005}, "models": {"yolo": {"arrival": 1711329875.891245, "serving": 1711329876.3436136}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4523218, "arrival": 1711329873.677154}, "models": {"yolo": {"arrival": 1711329873.3398054, "serving": 1711329873.6348298}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.622211618+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4198716}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617447797+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4633205}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4633052, "arrival": 1711329877.8462508}, "models": {"yolo": {"arrival": 1711329877.456729, "serving": 1711329877.8168595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4528146, "arrival": 1711329877.5869904}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4452484, "arrival": 1711329873.6763716}, "models": {"yolo": {"arrival": 1711329873.3398054, "serving": 1711329873.6348298}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4636736, "arrival": 1711329878.1151946}, "models": {"yolo": {"arrival": 1711329877.8258111, "serving": 1711329878.1024678}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4452806, "arrival": 1711329877.4701157}, "models": {"yolo": {"arrival": 1711329877.1799319, "serving": 1711329877.447607}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.625616291+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.463245}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4449797, "arrival": 1711329873.675965}, "models": {"yolo": {"arrival": 1711329873.3398054, "serving": 1711329873.6348298}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.462591, "arrival": 1711329874.005013}, "models": {"yolo": {"arrival": 1711329873.6460247, "serving": 1711329873.9633944}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4631512, "arrival": 1711329876.9558585}, "models": {"yolo": {"arrival": 1711329876.650932, "serving": 1711329876.9244323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4625034, "arrival": 1711329876.6814325}, "models": {"yolo": {"arrival": 1711329876.3523932, "serving": 1711329876.641859}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4203904, "arrival": 1711329877.5842407}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.617218244+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.420101}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4453416, "arrival": 1711329876.3836393}, "models": {"yolo": {"arrival": 1711329875.887037, "serving": 1711329876.3435488}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.617353561+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.452216}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4789631, "arrival": 1711329878.1166975}, "models": {"yolo": {"arrival": 1711329877.8258111, "serving": 1711329878.1024678}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4632268, "arrival": 1711329877.5882726}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4450147, "arrival": 1711329877.4694686}, "models": {"yolo": {"arrival": 1711329877.1799319, "serving": 1711329877.447607}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4450803, "arrival": 1711329876.3829498}, "models": {"yolo": {"arrival": 1711329875.887037, "serving": 1711329876.3435488}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.452046, "arrival": 1711329877.5852885}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4635527, "arrival": 1711329878.1132538}, "models": {"yolo": {"arrival": 1711329877.8258111, "serving": 1711329878.1024678}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4523385, "arrival": 1711329877.4719532}, "models": {"yolo": {"arrival": 1711329877.1799319, "serving": 1711329877.447607}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.69211813+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4637365}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4630413, "arrival": 1711329874.0052857}, "models": {"yolo": {"arrival": 1711329873.6460247, "serving": 1711329873.9633944}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.619741701+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4635067}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4631817, "arrival": 1711329877.8392239}, "models": {"yolo": {"arrival": 1711329877.456729, "serving": 1711329877.8168595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4451141, "arrival": 1711329877.5848577}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4524553, "arrival": 1711329873.6837873}, "models": {"yolo": {"arrival": 1711329873.3398054, "serving": 1711329873.6348298}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4634771, "arrival": 1711329877.5889096}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.623153169+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4631186}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4630868, "arrival": 1711329876.9403524}, "models": {"yolo": {"arrival": 1711329876.6516876, "serving": 1711329876.912254}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.462451, "arrival": 1711329873.684587}, "models": {"yolo": {"arrival": 1711329873.3398054, "serving": 1711329873.6348298}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4522533, "arrival": 1711329877.5857182}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.619693002+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.46326}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.61730624+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4623694}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617317392+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4625566}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617270887+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4522872}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:26.617436258+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329866.4631963}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4634624, "arrival": 1711329876.9439595}, "models": {"yolo": {"arrival": 1711329876.6516876, "serving": 1711329876.912254}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 408.00 MiB (GPU 0; 23.64 GiB total capacity; 1.57 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.622732774+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.462539}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.617401776+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.4624863}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.463056, "arrival": 1711329877.8381789}, "models": {"yolo": {"arrival": 1711329877.456729, "serving": 1711329877.8168595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4624302, "arrival": 1711329876.677619}, "models": {"yolo": {"arrival": 1711329876.35364, "serving": 1711329876.6406353}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4631014, "arrival": 1711329877.5879579}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4631667, "arrival": 1711329874.0055523}, "models": {"yolo": {"arrival": 1711329873.6460247, "serving": 1711329873.9633944}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 14.44 MiB free; 2.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:26.617461149+00:00\"}\"\n>", "times": {"request": {"sending": 1711329866.463447}}, "outputs": []}, {"times": {"request": {"sending": 1711329866.4527645, "arrival": 1711329877.4727073}, "models": {"yolo": {"arrival": 1711329877.1799319, "serving": 1711329877.447607}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329866.4635217, "arrival": 1711329876.9653847}, "models": {"yolo": {"arrival": 1711329876.650932, "serving": 1711329876.9244323}}}, "model_name": "yolo", "outputs": []}], [{"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.69217589+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4119189}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:27.843141792+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4370823}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4118388, "arrival": 1711329877.2226439}, "models": {"yolo": {"arrival": 1711329876.9231908, "serving": 1711329877.1759014}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523437203+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4381242}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4385667, "arrival": 1711329877.9303417}, "models": {"yolo": {"arrival": 1711329877.473114, "serving": 1711329877.9088356}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.760004631+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4385362}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4385815, "arrival": 1711329874.9436593}, "models": {"yolo": {"arrival": 1711329874.6445613, "serving": 1711329874.891616}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4123695, "arrival": 1711329874.3134525}, "models": {"yolo": {"arrival": 1711329873.9732904, "serving": 1711329874.2644124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4386976, "arrival": 1711329874.9442315}, "models": {"yolo": {"arrival": 1711329874.6445613, "serving": 1711329874.891616}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.760027164+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4386544}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4114118, "arrival": 1711329877.2223365}, "models": {"yolo": {"arrival": 1711329876.9231908, "serving": 1711329877.1759014}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4898238, "arrival": 1711329878.1618674}, "models": {"yolo": {"arrival": 1711329877.8111165, "serving": 1711329878.117562}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4371474, "arrival": 1711329878.4852211}, "models": {"yolo": {"arrival": 1711329878.1108959, "serving": 1711329878.4316492}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.411755, "arrival": 1711329878.1194844}, "models": {"yolo": {"arrival": 1711329877.8258111, "serving": 1711329878.1024678}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4379692, "arrival": 1711329878.9536767}, "models": {"yolo": {"arrival": 1711329878.4429636, "serving": 1711329878.898733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4301474, "arrival": 1711329877.2165837}, "models": {"yolo": {"arrival": 1711329876.9344246, "serving": 1711329877.171243}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4384487, "arrival": 1711329877.9298177}, "models": {"yolo": {"arrival": 1711329877.473114, "serving": 1711329877.9088356}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4380047, "arrival": 1711329877.8348112}, "models": {"yolo": {"arrival": 1711329877.4568696, "serving": 1711329877.8180094}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4376884, "arrival": 1711329877.5948977}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.437608, "arrival": 1711329874.695735}, "models": {"yolo": {"arrival": 1711329874.2759054, "serving": 1711329874.6325953}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.412089, "arrival": 1711329878.1213405}, "models": {"yolo": {"arrival": 1711329877.8258111, "serving": 1711329878.1024678}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4388018, "arrival": 1711329877.9311824}, "models": {"yolo": {"arrival": 1711329877.473114, "serving": 1711329877.9088356}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4126468, "arrival": 1711329877.2087777}, "models": {"yolo": {"arrival": 1711329876.9344246, "serving": 1711329877.171243}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4380877, "arrival": 1711329874.9389496}, "models": {"yolo": {"arrival": 1711329874.6445613, "serving": 1711329874.891616}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4298906, "arrival": 1711329877.2237258}, "models": {"yolo": {"arrival": 1711329876.9231908, "serving": 1711329877.1759014}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4297438, "arrival": 1711329877.2097225}, "models": {"yolo": {"arrival": 1711329876.9344246, "serving": 1711329877.171243}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4379356, "arrival": 1711329874.6986458}, "models": {"yolo": {"arrival": 1711329874.2759054, "serving": 1711329874.6325953}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.437845, "arrival": 1711329877.5951834}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4117951, "arrival": 1711329867.7307823}, "models": {"yolo": {"arrival": 1711329867.4958222, "serving": 1711329867.715172}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4385073, "arrival": 1711329877.8444946}, "models": {"yolo": {"arrival": 1711329877.4568696, "serving": 1711329877.8180094}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4381058, "arrival": 1711329878.9542089}, "models": {"yolo": {"arrival": 1711329878.4429636, "serving": 1711329878.898733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4374309, "arrival": 1711329877.487384}, "models": {"yolo": {"arrival": 1711329877.1803606, "serving": 1711329877.460092}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:27.843151234+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4372523}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.412049, "arrival": 1711329874.3127186}, "models": {"yolo": {"arrival": 1711329873.9732904, "serving": 1711329874.2644124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4370358, "arrival": 1711329877.5922155}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:27.529119563+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4116268}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4375312, "arrival": 1711329877.5945733}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.438379, "arrival": 1711329877.8406053}, "models": {"yolo": {"arrival": 1711329877.4568696, "serving": 1711329877.8180094}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4373147, "arrival": 1711329878.9512503}, "models": {"yolo": {"arrival": 1711329878.4429636, "serving": 1711329878.898733}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.759982963+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4384177}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4374495, "arrival": 1711329874.6894846}, "models": {"yolo": {"arrival": 1711329874.2759054, "serving": 1711329874.6325953}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4372094, "arrival": 1711329877.5954695}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4382021, "arrival": 1711329877.5060303}, "models": {"yolo": {"arrival": 1711329877.1803606, "serving": 1711329877.460092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4115255, "arrival": 1711329877.5896037}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.437669, "arrival": 1711329877.498571}, "models": {"yolo": {"arrival": 1711329877.1852486, "serving": 1711329877.4488127}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:27.855158785+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4386685}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.759898773+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4380386}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.523454959+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4383643}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.412489, "arrival": 1711329877.2231843}, "models": {"yolo": {"arrival": 1711329876.9231908, "serving": 1711329877.1759014}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4120007, "arrival": 1711329877.2066689}, "models": {"yolo": {"arrival": 1711329876.9344246, "serving": 1711329877.171243}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.753669451+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4372294}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4299955, "arrival": 1711329874.3286228}, "models": {"yolo": {"arrival": 1711329873.9732904, "serving": 1711329874.2644124}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.760070475+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4898422}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4370093, "arrival": 1711329877.4848478}, "models": {"yolo": {"arrival": 1711329877.1852486, "serving": 1711329877.4488127}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.412528, "arrival": 1711329877.5906997}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523545735+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.5010781}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.523473978+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4384928}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4384036, "arrival": 1711329877.8218563}, "models": {"yolo": {"arrival": 1711329877.484983, "serving": 1711329877.8016932}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4300852, "arrival": 1711329877.5918505}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.692144505+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.41158}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.523382618+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.43765}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523344158+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4373348}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.411714, "arrival": 1711329874.3119688}, "models": {"yolo": {"arrival": 1711329873.9732904, "serving": 1711329874.2644124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4385214, "arrival": 1711329877.8228645}, "models": {"yolo": {"arrival": 1711329877.484983, "serving": 1711329877.8016932}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.429842, "arrival": 1711329878.484471}, "models": {"yolo": {"arrival": 1711329878.1108959, "serving": 1711329878.4316492}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.489895, "arrival": 1711329875.2557623}, "models": {"yolo": {"arrival": 1711329874.9020033, "serving": 1711329875.2149348}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4383488, "arrival": 1711329878.9544883}, "models": {"yolo": {"arrival": 1711329878.4429636, "serving": 1711329878.898733}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:27.846665047+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4381876}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4784505, "arrival": 1711329874.94481}, "models": {"yolo": {"arrival": 1711329874.6445613, "serving": 1711329874.891616}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4281268, "arrival": 1711329877.2234519}, "models": {"yolo": {"arrival": 1711329876.9231908, "serving": 1711329877.1759014}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:27.84309486+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4126072}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.523326562+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4371681}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4116704, "arrival": 1711329876.9670103}, "models": {"yolo": {"arrival": 1711329876.650932, "serving": 1711329876.9244323}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:27.843129432+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4301264}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4377868, "arrival": 1711329878.952349}, "models": {"yolo": {"arrival": 1711329878.4429636, "serving": 1711329878.898733}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.52336383+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4374914}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:27.846681436+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4385507}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4384782, "arrival": 1711329879.1961405}, "models": {"yolo": {"arrival": 1711329878.9078324, "serving": 1711329879.1843162}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.692295516+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4301054}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4299128, "arrival": 1711329877.5914855}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4281514, "arrival": 1711329877.591089}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:27.845876731+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.43773}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4118786, "arrival": 1711329877.589896}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523418374+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4379866}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4384637, "arrival": 1711329874.9432893}, "models": {"yolo": {"arrival": 1711329874.6445613, "serving": 1711329874.891616}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.759950209+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.438171}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4371886, "arrival": 1711329877.485445}, "models": {"yolo": {"arrival": 1711329877.1852486, "serving": 1711329877.4488127}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.692245896+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4281735}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.430167, "arrival": 1711329874.6882815}, "models": {"yolo": {"arrival": 1711329874.2759054, "serving": 1711329874.6325953}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4381545, "arrival": 1711329877.8195443}, "models": {"yolo": {"arrival": 1711329877.484983, "serving": 1711329877.8016932}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4785228, "arrival": 1711329879.2070813}, "models": {"yolo": {"arrival": 1711329878.9078324, "serving": 1711329879.1843162}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4377491, "arrival": 1711329877.4990127}, "models": {"yolo": {"arrival": 1711329877.1803606, "serving": 1711329877.460092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4298165, "arrival": 1711329874.3280644}, "models": {"yolo": {"arrival": 1711329873.9732904, "serving": 1711329874.2644124}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:27.85518962+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4387872}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523228041+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4124475}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4379132, "arrival": 1711329877.504942}, "models": {"yolo": {"arrival": 1711329877.1803606, "serving": 1711329877.460092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4375117, "arrival": 1711329877.4863162}, "models": {"yolo": {"arrival": 1711329877.1852486, "serving": 1711329877.4488127}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:27.846673501+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.438433}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.523400981+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.437807}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.437127, "arrival": 1711329874.6886818}, "models": {"yolo": {"arrival": 1711329874.2759054, "serving": 1711329874.6325953}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.759824665+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4378684}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4124088, "arrival": 1711329878.4837496}, "models": {"yolo": {"arrival": 1711329878.1108959, "serving": 1711329878.4316492}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4378254, "arrival": 1711329877.5012517}, "models": {"yolo": {"arrival": 1711329877.1852486, "serving": 1711329877.4488127}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4373534, "arrival": 1711329877.4858832}, "models": {"yolo": {"arrival": 1711329877.1852486, "serving": 1711329877.4488127}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523307738+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.436931}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:27.843338306+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4375682}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.753735891+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4373925}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:27.846654081+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4380527}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4373734, "arrival": 1711329877.594216}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.7537626+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4375496}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.437294, "arrival": 1711329874.6890848}, "models": {"yolo": {"arrival": 1711329874.2759054, "serving": 1711329874.6325953}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523248461+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4280944}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.692268122+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.429934}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.523527856+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4897206}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.437469, "arrival": 1711329878.9515343}, "models": {"yolo": {"arrival": 1711329878.4429636, "serving": 1711329878.898733}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.753784712+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4377093}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.692198538+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.41225}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4376311, "arrival": 1711329878.952083}, "models": {"yolo": {"arrival": 1711329878.4429636, "serving": 1711329878.898733}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.489801, "arrival": 1711329878.1486986}, "models": {"yolo": {"arrival": 1711329877.8287466, "serving": 1711329878.1090922}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:27.52914476+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4119604}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4375885, "arrival": 1711329877.4876664}, "models": {"yolo": {"arrival": 1711329877.1803606, "serving": 1711329877.460092}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:27.843063102+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4122884}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4386392, "arrival": 1711329878.1612453}, "models": {"yolo": {"arrival": 1711329877.8111165, "serving": 1711329878.117562}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.430062, "arrival": 1711329877.223997}, "models": {"yolo": {"arrival": 1711329876.9231908, "serving": 1711329877.1759014}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4372735, "arrival": 1711329877.4870327}, "models": {"yolo": {"arrival": 1711329877.1803606, "serving": 1711329877.460092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4385958, "arrival": 1711329879.198568}, "models": {"yolo": {"arrival": 1711329878.9078324, "serving": 1711329879.1843162}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4121277, "arrival": 1711329867.7313643}, "models": {"yolo": {"arrival": 1711329867.4958222, "serving": 1711329867.715172}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4898758, "arrival": 1711329877.9334495}, "models": {"yolo": {"arrival": 1711329877.473114, "serving": 1711329877.9088356}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4122107, "arrival": 1711329877.5902705}, "models": {"yolo": {"arrival": 1711329875.9264839, "serving": 1711329877.457039}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4121714, "arrival": 1711329877.2229168}, "models": {"yolo": {"arrival": 1711329876.9231908, "serving": 1711329877.1759014}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:27.843160947+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4374118}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.76004902+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4387732}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.427996, "arrival": 1711329878.4842045}, "models": {"yolo": {"arrival": 1711329878.1108959, "serving": 1711329878.4316492}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4371057, "arrival": 1711329877.4866781}, "models": {"yolo": {"arrival": 1711329877.1803606, "serving": 1711329877.460092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4383314, "arrival": 1711329874.93952}, "models": {"yolo": {"arrival": 1711329874.6445613, "serving": 1711329874.891616}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4381385, "arrival": 1711329877.8356614}, "models": {"yolo": {"arrival": 1711329877.4568696, "serving": 1711329877.8180094}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4299753, "arrival": 1711329877.215943}, "models": {"yolo": {"arrival": 1711329876.9344246, "serving": 1711329877.171243}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.523508588+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.43873}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4300156, "arrival": 1711329878.484716}, "models": {"yolo": {"arrival": 1711329878.1108959, "serving": 1711329878.4316492}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4377677, "arrival": 1711329874.6983557}, "models": {"yolo": {"arrival": 1711329874.2759054, "serving": 1711329874.6325953}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.523288369+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4300365}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:27.843107705+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4281952}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.6922221+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4125671}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.692317187+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.61 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4370599}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4123282, "arrival": 1711329877.2078037}, "models": {"yolo": {"arrival": 1711329876.9344246, "serving": 1711329877.171243}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:27.845888161+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4378872}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4126854, "arrival": 1711329874.3141878}, "models": {"yolo": {"arrival": 1711329873.9732904, "serving": 1711329874.2644124}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4387586, "arrival": 1711329878.1615593}, "models": {"yolo": {"arrival": 1711329877.8111165, "serving": 1711329878.117562}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4899113, "arrival": 1711329879.207794}, "models": {"yolo": {"arrival": 1711329878.9078324, "serving": 1711329879.1843162}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4387438, "arrival": 1711329877.845692}, "models": {"yolo": {"arrival": 1711329877.4568696, "serving": 1711329877.8180094}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523269274+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4298663}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4386246, "arrival": 1711329877.8451097}, "models": {"yolo": {"arrival": 1711329877.4568696, "serving": 1711329877.8180094}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.430186, "arrival": 1711329878.484965}, "models": {"yolo": {"arrival": 1711329878.1108959, "serving": 1711329878.4316492}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:27.843119208+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329867.4299545}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.523491383+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4386103}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.30 GiB already allocated; 14.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:27.855201402+00:00\"}\"\n>", "times": {"request": {"sending": 1711329867.4898593}}, "outputs": []}, {"times": {"request": {"sending": 1711329867.4380705, "arrival": 1711329877.5057082}, "models": {"yolo": {"arrival": 1711329877.1803606, "serving": 1711329877.460092}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4380233, "arrival": 1711329877.8186061}, "models": {"yolo": {"arrival": 1711329877.484983, "serving": 1711329877.8016932}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4387124, "arrival": 1711329879.1994102}, "models": {"yolo": {"arrival": 1711329878.9078324, "serving": 1711329879.1843162}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329867.4386828, "arrival": 1711329877.9307775}, "models": {"yolo": {"arrival": 1711329877.473114, "serving": 1711329877.9088356}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329868.422405, "arrival": 1711329875.8988895}, "models": {"yolo": {"arrival": 1711329875.5876408, "serving": 1711329875.876059}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.760129464+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4068258}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4223127, "arrival": 1711329878.9503703}, "models": {"yolo": {"arrival": 1711329878.4500797, "serving": 1711329878.8936563}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.646421616+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4210017}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.079839846+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4179246}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.41557, "arrival": 1711329877.9348187}, "models": {"yolo": {"arrival": 1711329877.473114, "serving": 1711329877.9088356}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.763700564+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4069793}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4211621, "arrival": 1711329878.4898448}, "models": {"yolo": {"arrival": 1711329878.130258, "serving": 1711329878.4417934}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.781285841+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4222224}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4067085, "arrival": 1711329877.933918}, "models": {"yolo": {"arrival": 1711329877.473114, "serving": 1711329877.9088356}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4637663, "arrival": 1711329878.9301913}, "models": {"yolo": {"arrival": 1711329878.4509437, "serving": 1711329878.89403}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.421908, "arrival": 1711329880.024605}, "models": {"yolo": {"arrival": 1711329879.5318973, "serving": 1711329879.9766765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4224253, "arrival": 1711329880.0289412}, "models": {"yolo": {"arrival": 1711329879.5318973, "serving": 1711329879.9766765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4156554, "arrival": 1711329879.5558007}, "models": {"yolo": {"arrival": 1711329879.1963663, "serving": 1711329879.5193782}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4067874, "arrival": 1711329878.1501362}, "models": {"yolo": {"arrival": 1711329877.8287466, "serving": 1711329878.1090922}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.092517435+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4574914}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.406577, "arrival": 1711329878.1493723}, "models": {"yolo": {"arrival": 1711329877.8287466, "serving": 1711329878.1090922}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.652774289+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4575868}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4214063, "arrival": 1711329878.4889426}, "models": {"yolo": {"arrival": 1711329878.1187687, "serving": 1711329878.4412544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4572513, "arrival": 1711329875.9003727}, "models": {"yolo": {"arrival": 1711329875.5876408, "serving": 1711329875.876059}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4636712, "arrival": 1711329878.9539406}, "models": {"yolo": {"arrival": 1711329878.4500797, "serving": 1711329878.8936563}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.089036653+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.421924}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.092570346+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4636428}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4208562, "arrival": 1711329875.2642527}, "models": {"yolo": {"arrival": 1711329874.9020033, "serving": 1711329875.2149348}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4067485, "arrival": 1711329879.5552356}, "models": {"yolo": {"arrival": 1711329879.1963663, "serving": 1711329879.5193782}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.770608566+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.421716}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4685462, "arrival": 1711329876.4807057}, "models": {"yolo": {"arrival": 1711329875.8856041, "serving": 1711329876.4616015}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.846658024+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4575636}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.422328, "arrival": 1711329878.94171}, "models": {"yolo": {"arrival": 1711329878.4521005, "serving": 1711329878.892668}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.406864, "arrival": 1711329877.934421}, "models": {"yolo": {"arrival": 1711329877.473114, "serving": 1711329877.9088356}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4222832, "arrival": 1711329880.0286613}, "models": {"yolo": {"arrival": 1711329879.5318973, "serving": 1711329879.9766765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4223752, "arrival": 1711329878.4880633}, "models": {"yolo": {"arrival": 1711329878.12266, "serving": 1711329878.440292}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.468365, "arrival": 1711329876.4802773}, "models": {"yolo": {"arrival": 1711329875.8856041, "serving": 1711329876.4616015}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4636967, "arrival": 1711329878.9531531}, "models": {"yolo": {"arrival": 1711329878.4521005, "serving": 1711329878.892668}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.853062643+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4637191}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.420831, "arrival": 1711329878.1472824}, "models": {"yolo": {"arrival": 1711329877.9182456, "serving": 1711329878.1129856}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4215488, "arrival": 1711329878.4891934}, "models": {"yolo": {"arrival": 1711329878.1187687, "serving": 1711329878.4412544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4069018, "arrival": 1711329879.5555232}, "models": {"yolo": {"arrival": 1711329879.1963663, "serving": 1711329879.5193782}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4572196, "arrival": 1711329878.4935722}, "models": {"yolo": {"arrival": 1711329878.12266, "serving": 1711329878.440292}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.649313349+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4217336}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.406639, "arrival": 1711329878.1622272}, "models": {"yolo": {"arrival": 1711329877.8111165, "serving": 1711329878.117562}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.0890131+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4217958}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4208822, "arrival": 1711329879.5565004}, "models": {"yolo": {"arrival": 1711329879.1963663, "serving": 1711329879.5193782}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.768676486+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4215827}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4068828, "arrival": 1711329875.2564425}, "models": {"yolo": {"arrival": 1711329874.9020033, "serving": 1711329875.2149348}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4225955, "arrival": 1711329878.9429662}, "models": {"yolo": {"arrival": 1711329878.4521005, "serving": 1711329878.892668}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4217, "arrival": 1711329878.4943159}, "models": {"yolo": {"arrival": 1711329878.130258, "serving": 1711329878.4417934}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4225183, "arrival": 1711329878.493257}, "models": {"yolo": {"arrival": 1711329878.12266, "serving": 1711329878.440292}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4575408, "arrival": 1711329878.9528868}, "models": {"yolo": {"arrival": 1711329878.4521005, "serving": 1711329878.892668}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4178126, "arrival": 1711329878.1634672}, "models": {"yolo": {"arrival": 1711329877.8111165, "serving": 1711329878.117562}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.649208954+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4214633}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.42175, "arrival": 1711329878.4871593}, "models": {"yolo": {"arrival": 1711329878.12266, "serving": 1711329878.440292}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4218907, "arrival": 1711329875.6406555}, "models": {"yolo": {"arrival": 1711329875.2245872, "serving": 1711329875.576285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4207332, "arrival": 1711329878.1637704}, "models": {"yolo": {"arrival": 1711329877.8111165, "serving": 1711329878.117562}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4685013, "arrival": 1711329869.7374697}, "models": {"yolo": {"arrival": 1711329869.4331722, "serving": 1711329869.723909}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.646346133+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4208055}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523563442+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4067676}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.092682112+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.477652}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4684346, "arrival": 1711329879.246608}, "models": {"yolo": {"arrival": 1711329878.9047065, "serving": 1711329879.1911378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4216132, "arrival": 1711329878.1598594}, "models": {"yolo": {"arrival": 1711329877.9182456, "serving": 1711329878.1129856}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.089081457+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4222972}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.652107221+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4223604}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.766680311+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4214473}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4225328, "arrival": 1711329875.8996878}, "models": {"yolo": {"arrival": 1711329875.5876408, "serving": 1711329875.876059}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4218287, "arrival": 1711329878.4948046}, "models": {"yolo": {"arrival": 1711329878.130258, "serving": 1711329878.4417934}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4225497, "arrival": 1711329880.3029625}, "models": {"yolo": {"arrival": 1711329879.986287, "serving": 1711329880.2607684}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.646143894+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4066882}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.42216, "arrival": 1711329880.0281956}, "models": {"yolo": {"arrival": 1711329879.5318973, "serving": 1711329879.9766765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4067285, "arrival": 1711329875.2560685}, "models": {"yolo": {"arrival": 1711329874.9020033, "serving": 1711329875.2149348}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4069605, "arrival": 1711329878.1631186}, "models": {"yolo": {"arrival": 1711329877.8111165, "serving": 1711329878.117562}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.083732199+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4210982}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4215665, "arrival": 1711329878.4905195}, "models": {"yolo": {"arrival": 1711329878.130258, "serving": 1711329878.4417934}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.649272463+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4215975}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4210265, "arrival": 1711329878.1480384}, "models": {"yolo": {"arrival": 1711329877.9182456, "serving": 1711329878.1129856}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4635248, "arrival": 1711329880.3040888}, "models": {"yolo": {"arrival": 1711329879.986287, "serving": 1711329880.2607684}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.088913336+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4213688}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.760091421+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4066658}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.421497, "arrival": 1711329875.637926}, "models": {"yolo": {"arrival": 1711329875.2245872, "serving": 1711329875.576285}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.089059325+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.422175}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.422207, "arrival": 1711329878.9411519}, "models": {"yolo": {"arrival": 1711329878.4521005, "serving": 1711329878.892668}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.853145067+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4684794}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4683864, "arrival": 1711329880.3145509}, "models": {"yolo": {"arrival": 1711329879.986287, "serving": 1711329880.2607684}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.468263, "arrival": 1711329879.2435153}, "models": {"yolo": {"arrival": 1711329878.9026186, "serving": 1711329879.1912286}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.773095789+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4218452}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.079773636+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4156928}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4209569, "arrival": 1711329878.4894843}, "models": {"yolo": {"arrival": 1711329878.130258, "serving": 1711329878.4417934}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4214807, "arrival": 1711329878.1592522}, "models": {"yolo": {"arrival": 1711329877.9182456, "serving": 1711329878.1129856}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4683404, "arrival": 1711329878.9310915}, "models": {"yolo": {"arrival": 1711329878.4509437, "serving": 1711329878.89403}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.421051, "arrival": 1711329875.2645335}, "models": {"yolo": {"arrival": 1711329874.9020033, "serving": 1711329875.2149348}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4219403, "arrival": 1711329878.4945643}, "models": {"yolo": {"arrival": 1711329878.1187687, "serving": 1711329878.4412544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4222522, "arrival": 1711329878.4878402}, "models": {"yolo": {"arrival": 1711329878.12266, "serving": 1711329878.440292}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.652148366+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4225032}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.652827092+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.463743}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4222674, "arrival": 1711329875.8976972}, "models": {"yolo": {"arrival": 1711329875.5876408, "serving": 1711329875.876059}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.421429, "arrival": 1711329878.4902055}, "models": {"yolo": {"arrival": 1711329878.130258, "serving": 1711329878.4417934}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4213247, "arrival": 1711329879.5578327}, "models": {"yolo": {"arrival": 1711329879.1963663, "serving": 1711329879.5193782}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4685678, "arrival": 1711329880.3157482}, "models": {"yolo": {"arrival": 1711329879.986287, "serving": 1711329880.2607684}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4068067, "arrival": 1711329878.1627042}, "models": {"yolo": {"arrival": 1711329877.8111165, "serving": 1711329878.117562}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.846595192+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4226105}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.763843472+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4211931}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4179068, "arrival": 1711329879.5562096}, "models": {"yolo": {"arrival": 1711329879.1963663, "serving": 1711329879.5193782}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.523581582+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4069211}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.421514, "arrival": 1711329880.0234618}, "models": {"yolo": {"arrival": 1711329879.5318973, "serving": 1711329879.9766765}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.422473, "arrival": 1711329878.942237}, "models": {"yolo": {"arrival": 1711329878.4521005, "serving": 1711329878.892668}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.853118147+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4682868}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4210749, "arrival": 1711329879.5575137}, "models": {"yolo": {"arrival": 1711329879.1963663, "serving": 1711329879.5193782}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.646232778+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.406845}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.092641265+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4684095}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4218128, "arrival": 1711329878.4940786}, "models": {"yolo": {"arrival": 1711329878.1187687, "serving": 1711329878.4412544}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.088961469+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4215305}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4217656, "arrival": 1711329875.6386602}, "models": {"yolo": {"arrival": 1711329875.2245872, "serving": 1711329875.576285}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.652185364+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.457103}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4684577, "arrival": 1711329879.2439413}, "models": {"yolo": {"arrival": 1711329878.9026186, "serving": 1711329879.1912286}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.763753787+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4178352}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4177392, "arrival": 1711329878.1513531}, "models": {"yolo": {"arrival": 1711329877.8287466, "serving": 1711329878.1090922}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.417872, "arrival": 1711329878.146019}, "models": {"yolo": {"arrival": 1711329877.9182456, "serving": 1711329878.1129856}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.421781, "arrival": 1711329880.0242324}, "models": {"yolo": {"arrival": 1711329879.5318973, "serving": 1711329879.9766765}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.763816994+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.420979}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4218762, "arrival": 1711329878.4873874}, "models": {"yolo": {"arrival": 1711329878.12266, "serving": 1711329878.440292}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.420933, "arrival": 1711329878.488391}, "models": {"yolo": {"arrival": 1711329878.1187687, "serving": 1711329878.4412544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4219563, "arrival": 1711329878.4950376}, "models": {"yolo": {"arrival": 1711329878.130258, "serving": 1711329878.4417934}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.781340136+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4223433}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.646273283+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4154565}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.468317, "arrival": 1711329869.7365444}, "models": {"yolo": {"arrival": 1711329869.4331722, "serving": 1711329869.723909}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4576092, "arrival": 1711329878.92846}, "models": {"yolo": {"arrival": 1711329878.4509437, "serving": 1711329878.89403}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4225805, "arrival": 1711329878.950978}, "models": {"yolo": {"arrival": 1711329878.4500797, "serving": 1711329878.8936563}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.420642, "arrival": 1711329878.157841}, "models": {"yolo": {"arrival": 1711329877.8287466, "serving": 1711329878.1090922}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4221451, "arrival": 1711329875.6411998}, "models": {"yolo": {"arrival": 1711329875.2245872, "serving": 1711329875.576285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.457518, "arrival": 1711329878.9534144}, "models": {"yolo": {"arrival": 1711329878.4500797, "serving": 1711329878.8936563}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4224565, "arrival": 1711329878.9506948}, "models": {"yolo": {"arrival": 1711329878.4500797, "serving": 1711329878.8936563}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4069412, "arrival": 1711329878.150838}, "models": {"yolo": {"arrival": 1711329877.8287466, "serving": 1711329878.1090922}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4682374, "arrival": 1711329879.2460968}, "models": {"yolo": {"arrival": 1711329878.9047065, "serving": 1711329879.1911378}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.08910328+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.422441}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.083656641+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.420907}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4681077, "arrival": 1711329880.304791}, "models": {"yolo": {"arrival": 1711329879.986287, "serving": 1711329880.2607684}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4211266, "arrival": 1711329878.4887033}, "models": {"yolo": {"arrival": 1711329878.1187687, "serving": 1711329878.4412544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.415615, "arrival": 1711329875.2567356}, "models": {"yolo": {"arrival": 1711329874.9020033, "serving": 1711329875.2149348}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.088988495+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.421666}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4216843, "arrival": 1711329878.4937978}, "models": {"yolo": {"arrival": 1711329878.1187687, "serving": 1711329878.4412544}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4212883, "arrival": 1711329875.637516}, "models": {"yolo": {"arrival": 1711329875.2245872, "serving": 1711329875.576285}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.776278883+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4220977}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4637904, "arrival": 1711329875.927482}, "models": {"yolo": {"arrival": 1711329875.5876408, "serving": 1711329875.876059}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4221904, "arrival": 1711329878.9498913}, "models": {"yolo": {"arrival": 1711329878.4500797, "serving": 1711329878.8936563}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.783738141+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4224877}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.652038969+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4222372}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.42163, "arrival": 1711329875.6383228}, "models": {"yolo": {"arrival": 1711329875.2245872, "serving": 1711329875.576285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.457632, "arrival": 1711329875.9268153}, "models": {"yolo": {"arrival": 1711329875.5876408, "serving": 1711329875.876059}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:28.649384994+00:00\"}\"\n>", "times": {"request": {"sending": 1711329868.4221148}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.092605481+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4682088}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.42213, "arrival": 1711329878.4876204}, "models": {"yolo": {"arrival": 1711329878.12266, "serving": 1711329878.440292}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4212513, "arrival": 1711329878.158572}, "models": {"yolo": {"arrival": 1711329877.9182456, "serving": 1711329878.1129856}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.763793682+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.420771}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.468524, "arrival": 1711329878.931747}, "models": {"yolo": {"arrival": 1711329878.4509437, "serving": 1711329878.89403}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.457466, "arrival": 1711329880.3035536}, "models": {"yolo": {"arrival": 1711329879.986287, "serving": 1711329880.2607684}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.646311492+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4178548}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.649350135+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4218607}}, "outputs": []}, {"times": {"request": {"sending": 1711329868.4178889, "arrival": 1711329875.2629266}, "models": {"yolo": {"arrival": 1711329874.9020033, "serving": 1711329875.2149348}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329868.4216464, "arrival": 1711329880.0238562}, "models": {"yolo": {"arrival": 1711329879.5318973, "serving": 1711329879.9766765}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:28.646461213+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4212244}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.089124674+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329868.4225652}}, "outputs": []}], [{"times": {"request": {"sending": 1711329869.4389653, "arrival": 1711329880.0137577}, "models": {"yolo": {"arrival": 1711329879.522773, "serving": 1711329879.9696193}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4151082, "arrival": 1711329879.2470202}, "models": {"yolo": {"arrival": 1711329878.9047065, "serving": 1711329879.1911378}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.607566434+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4382436}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4372957, "arrival": 1711329879.5449815}, "models": {"yolo": {"arrival": 1711329879.2006438, "serving": 1711329879.5138516}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.922414093+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4386911}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4387507, "arrival": 1711329881.5793686}, "models": {"yolo": {"arrival": 1711329881.3992157, "serving": 1711329881.5568352}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4380682, "arrival": 1711329879.5464075}, "models": {"yolo": {"arrival": 1711329879.1983132, "serving": 1711329879.5136154}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4155443, "arrival": 1711329876.4809558}, "models": {"yolo": {"arrival": 1711329875.8856041, "serving": 1711329876.4616015}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.510214976+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4374793}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.853168576+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4152174}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.853224193+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4158976}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4155169, "arrival": 1711329878.932359}, "models": {"yolo": {"arrival": 1711329878.4509437, "serving": 1711329878.89403}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.601935057+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4376867}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4319463, "arrival": 1711329879.2474086}, "models": {"yolo": {"arrival": 1711329878.9024606, "serving": 1711329879.18991}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.907277133+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.437736}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.510239429+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.437619}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.922238896+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4381642}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4154842, "arrival": 1711329869.7381287}, "models": {"yolo": {"arrival": 1711329869.4331722, "serving": 1711329869.723909}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.607637534+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4386482}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4156482, "arrival": 1711329879.247827}, "models": {"yolo": {"arrival": 1711329878.9047065, "serving": 1711329879.1911378}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.907330694+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.438036}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4315844, "arrival": 1711329881.0407643}, "models": {"yolo": {"arrival": 1711329880.2730882, "serving": 1711329881.0193796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.415183, "arrival": 1711329879.2443485}, "models": {"yolo": {"arrival": 1711329878.9026186, "serving": 1711329879.1912286}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4373882, "arrival": 1711329881.4070833}, "models": {"yolo": {"arrival": 1711329881.0280886, "serving": 1711329881.3902273}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.607488234+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4379869}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.509979805+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4159217}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4383214, "arrival": 1711329879.5485613}, "models": {"yolo": {"arrival": 1711329879.1983132, "serving": 1711329879.5136154}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4376502, "arrival": 1711329876.9575589}, "models": {"yolo": {"arrival": 1711329876.6452632, "serving": 1711329876.9246252}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.858339422+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4161463}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4386623, "arrival": 1711329880.026103}, "models": {"yolo": {"arrival": 1711329879.528513, "serving": 1711329879.9700882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4375691, "arrival": 1711329879.563289}, "models": {"yolo": {"arrival": 1711329879.2020795, "serving": 1711329879.5192595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4384773, "arrival": 1711329880.010102}, "models": {"yolo": {"arrival": 1711329879.522773, "serving": 1711329879.9696193}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.607592131+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4383678}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4157484, "arrival": 1711329878.9329607}, "models": {"yolo": {"arrival": 1711329878.4509437, "serving": 1711329878.89403}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.858678055+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4373147}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.607707511+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4594672}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.922340976+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.438445}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.517853078+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4380524}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4314806, "arrival": 1711329879.5441504}, "models": {"yolo": {"arrival": 1711329879.2006438, "serving": 1711329879.5138516}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.858703375+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4374604}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4377193, "arrival": 1711329879.9790251}, "models": {"yolo": {"arrival": 1711329879.5259907, "serving": 1711329879.9703066}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4389803, "arrival": 1711329877.2116203}, "models": {"yolo": {"arrival": 1711329876.9375963, "serving": 1711329877.1708195}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.607614967+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4385257}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.858554315+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4315014}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.858508758+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4313157}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4387834, "arrival": 1711329880.2795873}, "models": {"yolo": {"arrival": 1711329879.9808578, "serving": 1711329880.2538095}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.438148, "arrival": 1711329879.986843}, "models": {"yolo": {"arrival": 1711329879.5259907, "serving": 1711329879.9703066}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.85320145+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4156997}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.601581392+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4158225}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4292665, "arrival": 1711329879.2421966}, "models": {"yolo": {"arrival": 1711329878.9024606, "serving": 1711329879.18991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.45956, "arrival": 1711329880.2959957}, "models": {"yolo": {"arrival": 1711329879.9766026, "serving": 1711329880.259904}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4626403, "arrival": 1711329880.0226421}, "models": {"yolo": {"arrival": 1711329879.522773, "serving": 1711329879.9696193}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4380043, "arrival": 1711329880.0168724}, "models": {"yolo": {"arrival": 1711329879.528513, "serving": 1711329879.9700882}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.510099215+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.431342}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.510051478+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4161713}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4381325, "arrival": 1711329880.0174284}, "models": {"yolo": {"arrival": 1711329879.528513, "serving": 1711329879.9700882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.43777, "arrival": 1711329879.5432873}, "models": {"yolo": {"arrival": 1711329879.1983132, "serving": 1711329879.5136154}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4380841, "arrival": 1711329876.960234}, "models": {"yolo": {"arrival": 1711329876.6452632, "serving": 1711329876.9246252}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4157975, "arrival": 1711329881.032096}, "models": {"yolo": {"arrival": 1711329880.2730882, "serving": 1711329881.0193796}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.601801508+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.437187}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4315434, "arrival": 1711329879.2430923}, "models": {"yolo": {"arrival": 1711329878.9024606, "serving": 1711329879.18991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4626958, "arrival": 1711329870.8048608}, "models": {"yolo": {"arrival": 1711329870.4483318, "serving": 1711329870.7937944}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.437634, "arrival": 1711329879.5427957}, "models": {"yolo": {"arrival": 1711329879.1983132, "serving": 1711329879.5136154}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4157245, "arrival": 1711329869.7387948}, "models": {"yolo": {"arrival": 1711329869.4331722, "serving": 1711329869.723909}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4374967, "arrival": 1711329879.541824}, "models": {"yolo": {"arrival": 1711329879.1983132, "serving": 1711329879.5136154}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4387984, "arrival": 1711329880.278995}, "models": {"yolo": {"arrival": 1711329879.9766026, "serving": 1711329880.259904}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4159462, "arrival": 1711329879.2417574}, "models": {"yolo": {"arrival": 1711329878.9024606, "serving": 1711329879.18991}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.607660471+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4387674}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4372735, "arrival": 1711329879.554881}, "models": {"yolo": {"arrival": 1711329879.2020795, "serving": 1711329879.5192595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4160936, "arrival": 1711329879.2568407}, "models": {"yolo": {"arrival": 1711329878.9047065, "serving": 1711329879.1911378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4626794, "arrival": 1711329881.8240535}, "models": {"yolo": {"arrival": 1711329881.5690866, "serving": 1711329881.8048663}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.462661, "arrival": 1711329877.212553}, "models": {"yolo": {"arrival": 1711329876.9375963, "serving": 1711329877.1708195}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4564836, "arrival": 1711329881.8234859}, "models": {"yolo": {"arrival": 1711329881.5690866, "serving": 1711329881.8048663}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4376674, "arrival": 1711329881.411048}, "models": {"yolo": {"arrival": 1711329881.0280886, "serving": 1711329881.3902273}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4314597, "arrival": 1711329879.5539374}, "models": {"yolo": {"arrival": 1711329879.2020795, "serving": 1711329879.5192595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4158487, "arrival": 1711329879.2561667}, "models": {"yolo": {"arrival": 1711329878.9047065, "serving": 1711329879.1911378}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4155707, "arrival": 1711329881.0298886}, "models": {"yolo": {"arrival": 1711329880.2730882, "serving": 1711329881.0193796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4294415, "arrival": 1711329879.5534315}, "models": {"yolo": {"arrival": 1711329879.2020795, "serving": 1711329879.5192595}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.601629034+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4160519}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4386156, "arrival": 1711329876.9679322}, "models": {"yolo": {"arrival": 1711329876.6452632, "serving": 1711329876.9246252}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4318643, "arrival": 1711329879.5543475}, "models": {"yolo": {"arrival": 1711329879.2020795, "serving": 1711329879.5192595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4158733, "arrival": 1711329879.255454}, "models": {"yolo": {"arrival": 1711329878.9026186, "serving": 1711329879.1912286}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4293633, "arrival": 1711329876.4864993}, "models": {"yolo": {"arrival": 1711329875.8856041, "serving": 1711329876.4616015}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4389205, "arrival": 1711329880.2952218}, "models": {"yolo": {"arrival": 1711329879.9766026, "serving": 1711329880.259904}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.51015476+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4319258}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4388757, "arrival": 1711329881.8230424}, "models": {"yolo": {"arrival": 1711329881.5690866, "serving": 1711329881.8048663}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.518076782+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4625795}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.518055081+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4389503}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4157732, "arrival": 1711329876.4847577}, "models": {"yolo": {"arrival": 1711329875.8856041, "serving": 1711329876.4616015}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4385076, "arrival": 1711329881.578315}, "models": {"yolo": {"arrival": 1711329881.3992157, "serving": 1711329881.5568352}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.517785308+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4377532}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.517946752+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.438461}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4387357, "arrival": 1711329876.9683259}, "models": {"yolo": {"arrival": 1711329876.6452632, "serving": 1711329876.9246252}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4385562, "arrival": 1711329879.9890573}, "models": {"yolo": {"arrival": 1711329879.5259907, "serving": 1711329879.9703066}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.601889768+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.437551}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4381955, "arrival": 1711329879.5479035}, "models": {"yolo": {"arrival": 1711329879.1983132, "serving": 1711329879.5136154}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4382107, "arrival": 1711329876.9607341}, "models": {"yolo": {"arrival": 1711329876.6452632, "serving": 1711329876.9246252}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4382257, "arrival": 1711329881.5757608}, "models": {"yolo": {"arrival": 1711329881.3992157, "serving": 1711329881.5568352}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.858581934+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4319055}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.601758728+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4318392}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.922364705+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4385707}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.438676, "arrival": 1711329880.2780755}, "models": {"yolo": {"arrival": 1711329879.9766026, "serving": 1711329880.259904}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.416121, "arrival": 1711329879.2557673}, "models": {"yolo": {"arrival": 1711329878.9026186, "serving": 1711329879.1912286}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4383516, "arrival": 1711329881.5762784}, "models": {"yolo": {"arrival": 1711329881.3992157, "serving": 1711329881.5568352}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.517999875+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.438706}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.601507811+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.415602}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4383368, "arrival": 1711329876.9662485}, "models": {"yolo": {"arrival": 1711329876.6452632, "serving": 1711329876.9246252}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4156744, "arrival": 1711329879.2447689}, "models": {"yolo": {"arrival": 1711329878.9026186, "serving": 1711329879.1912286}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4380198, "arrival": 1711329879.979955}, "models": {"yolo": {"arrival": 1711329879.5259907, "serving": 1711329879.9703066}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.459539, "arrival": 1711329880.282905}, "models": {"yolo": {"arrival": 1711329879.9808578, "serving": 1711329880.2538095}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.437703, "arrival": 1711329879.5635056}, "models": {"yolo": {"arrival": 1711329879.2020795, "serving": 1711329879.5192595}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4384286, "arrival": 1711329879.9884202}, "models": {"yolo": {"arrival": 1711329879.5259907, "serving": 1711329879.9703066}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.607540919+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4381185}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.922436417+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.438814}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.51013122+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.431522}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4386299, "arrival": 1711329881.5788836}, "models": {"yolo": {"arrival": 1711329881.3992157, "serving": 1711329881.5568352}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4382594, "arrival": 1711329880.0220165}, "models": {"yolo": {"arrival": 1711329879.528513, "serving": 1711329879.9700882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4319785, "arrival": 1711329876.6523693}, "models": {"yolo": {"arrival": 1711329876.4717913, "serving": 1711329876.6326892}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.601717163+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4314356}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.43886, "arrival": 1711329877.21068}, "models": {"yolo": {"arrival": 1711329876.9375963, "serving": 1711329877.1708195}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4312322, "arrival": 1711329879.5437233}, "models": {"yolo": {"arrival": 1711329879.2006438, "serving": 1711329879.5138516}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:29.601675078+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.429418}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4374251, "arrival": 1711329879.562932}, "models": {"yolo": {"arrival": 1711329879.2020795, "serving": 1711329879.5192595}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.518020176+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4388301}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.922492778+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4595795}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4383824, "arrival": 1711329880.0230608}, "models": {"yolo": {"arrival": 1711329879.528513, "serving": 1711329879.9700882}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.858725382+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.73 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4376016}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4374418, "arrival": 1711329879.545393}, "models": {"yolo": {"arrival": 1711329879.2006438, "serving": 1711329879.5138516}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.51019123+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4373353}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4318852, "arrival": 1711329879.544569}, "models": {"yolo": {"arrival": 1711329879.2006438, "serving": 1711329879.5138516}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4314127, "arrival": 1711329881.0387592}, "models": {"yolo": {"arrival": 1711329880.2730882, "serving": 1711329881.0193796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4373708, "arrival": 1711329876.6530492}, "models": {"yolo": {"arrival": 1711329876.4717913, "serving": 1711329876.6326892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4380999, "arrival": 1711329881.5751789}, "models": {"yolo": {"arrival": 1711329881.3992157, "serving": 1711329881.5568352}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4159994, "arrival": 1711329881.0329154}, "models": {"yolo": {"arrival": 1711329880.2730882, "serving": 1711329881.0193796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4386008, "arrival": 1711329880.0111547}, "models": {"yolo": {"arrival": 1711329879.522773, "serving": 1711329879.9696193}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4389062, "arrival": 1711329880.280134}, "models": {"yolo": {"arrival": 1711329879.9808578, "serving": 1711329880.2538095}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4375317, "arrival": 1711329881.4101017}, "models": {"yolo": {"arrival": 1711329881.0280886, "serving": 1711329881.3902273}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4159703, "arrival": 1711329876.486129}, "models": {"yolo": {"arrival": 1711329875.8856041, "serving": 1711329876.4616015}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4377868, "arrival": 1711329876.9581165}, "models": {"yolo": {"arrival": 1711329876.6452632, "serving": 1711329876.9246252}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.922313478+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4382913}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.517915267+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.438306}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.437965, "arrival": 1711329881.4117076}, "models": {"yolo": {"arrival": 1711329881.0280886, "serving": 1711329881.3902273}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4313903, "arrival": 1711329876.486794}, "models": {"yolo": {"arrival": 1711329875.8856041, "serving": 1711329876.4616015}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4322267, "arrival": 1711329881.0414693}, "models": {"yolo": {"arrival": 1711329880.2730882, "serving": 1711329881.0193796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4388454, "arrival": 1711329880.0131254}, "models": {"yolo": {"arrival": 1711329879.522773, "serving": 1711329879.9696193}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4313664, "arrival": 1711329879.2426543}, "models": {"yolo": {"arrival": 1711329878.9024606, "serving": 1711329879.18991}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4385407, "arrival": 1711329880.0253563}, "models": {"yolo": {"arrival": 1711329879.528513, "serving": 1711329879.9700882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.431564, "arrival": 1711329876.6514363}, "models": {"yolo": {"arrival": 1711329876.4717913, "serving": 1711329876.6326892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.438721, "arrival": 1711329880.0121582}, "models": {"yolo": {"arrival": 1711329879.522773, "serving": 1711329879.9696193}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.607685272+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4388912}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.922464583+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4389353}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.517971009+00:00\"}\"\n>", "times": {"request": {"sending": 1711329869.4385855}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.4373536, "arrival": 1711329879.248236}, "models": {"yolo": {"arrival": 1711329878.9024606, "serving": 1711329879.18991}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.517878542+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4381802}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:29.601846136+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 18.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329869.4374075}}, "outputs": []}, {"times": {"request": {"sending": 1711329869.438493, "arrival": 1711329876.9673905}, "models": {"yolo": {"arrival": 1711329876.6452632, "serving": 1711329876.9246252}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4375143, "arrival": 1711329876.6536963}, "models": {"yolo": {"arrival": 1711329876.4717913, "serving": 1711329876.6326892}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4375849, "arrival": 1711329879.5459921}, "models": {"yolo": {"arrival": 1711329879.2006438, "serving": 1711329879.5138516}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4293923, "arrival": 1711329881.0376604}, "models": {"yolo": {"arrival": 1711329880.2730882, "serving": 1711329881.0193796}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329869.4382753, "arrival": 1711329879.9877605}, "models": {"yolo": {"arrival": 1711329879.5259907, "serving": 1711329879.9703066}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329870.4159296, "arrival": 1711329881.8335772}, "models": {"yolo": {"arrival": 1711329881.5690866, "serving": 1711329881.8048663}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4161005, "arrival": 1711329880.2657068}, "models": {"yolo": {"arrival": 1711329879.9769464, "serving": 1711329880.2529194}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.433655, "arrival": 1711329881.2621439}, "models": {"yolo": {"arrival": 1711329880.8515086, "serving": 1711329881.2502594}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4157634, "arrival": 1711329880.3014953}, "models": {"yolo": {"arrival": 1711329879.9766026, "serving": 1711329880.259904}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.922658714+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.4333732}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.416017, "arrival": 1711329880.3022406}, "models": {"yolo": {"arrival": 1711329879.9766026, "serving": 1711329880.259904}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.619644, "arrival": 1711329881.569764}, "models": {"yolo": {"arrival": 1711329881.2586017, "serving": 1711329881.545589}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4161284, "arrival": 1711329877.2137358}, "models": {"yolo": {"arrival": 1711329876.9375963, "serving": 1711329877.1708195}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.6203067, "arrival": 1711329881.7995021}, "models": {"yolo": {"arrival": 1711329881.5541813, "serving": 1711329881.7826624}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4158704, "arrival": 1711329880.2639399}, "models": {"yolo": {"arrival": 1711329879.9769464, "serving": 1711329880.2529194}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.41567, "arrival": 1711329880.2835958}, "models": {"yolo": {"arrival": 1711329879.9808578, "serving": 1711329880.2538095}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.007769679+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.6202142}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.730682835+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4535615}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4332867, "arrival": 1711329881.8356907}, "models": {"yolo": {"arrival": 1711329881.5690866, "serving": 1711329881.8048663}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.922702794+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.433694}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.453593, "arrival": 1711329877.9186878}, "models": {"yolo": {"arrival": 1711329877.4752676, "serving": 1711329877.8997042}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4164755, "arrival": 1711329880.8640199}, "models": {"yolo": {"arrival": 1711329880.263235, "serving": 1711329880.842692}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.922593518+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.4165308}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.922524041+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.415804}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4159884, "arrival": 1711329880.2841535}, "models": {"yolo": {"arrival": 1711329879.9808578, "serving": 1711329880.2538095}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.6198983, "arrival": 1711329881.572425}, "models": {"yolo": {"arrival": 1711329881.2586017, "serving": 1711329881.545589}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4159586, "arrival": 1711329870.805753}, "models": {"yolo": {"arrival": 1711329870.4483318, "serving": 1711329870.7937944}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.453393, "arrival": 1711329882.4930456}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.006691731+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4535422}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.922614598+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4168165}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4530687, "arrival": 1711329882.0993845}, "models": {"yolo": {"arrival": 1711329881.8189707, "serving": 1711329882.06022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.923196111+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4528365}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.453244, "arrival": 1711329881.265972}, "models": {"yolo": {"arrival": 1711329880.8515086, "serving": 1711329881.2502594}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.728089273+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.4337227}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4337714, "arrival": 1711329882.0987666}, "models": {"yolo": {"arrival": 1711329881.8189707, "serving": 1711329882.06022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4168737, "arrival": 1711329880.2847412}, "models": {"yolo": {"arrival": 1711329879.9769464, "serving": 1711329880.2529194}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.922572435+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4162717}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.6198668, "arrival": 1711329882.3257887}, "models": {"yolo": {"arrival": 1711329882.072867, "serving": 1711329882.2956703}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4333515, "arrival": 1711329882.4839625}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.6199124, "arrival": 1711329882.5046444}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4532108, "arrival": 1711329882.0999658}, "models": {"yolo": {"arrival": 1711329881.8189707, "serving": 1711329882.06022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4166596, "arrival": 1711329877.487947}, "models": {"yolo": {"arrival": 1711329877.183118, "serving": 1711329877.460159}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4163275, "arrival": 1711329880.273504}, "models": {"yolo": {"arrival": 1711329879.9769464, "serving": 1711329880.2529194}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.518097677+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.4158385}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.6197877, "arrival": 1711329882.5038276}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4159007, "arrival": 1711329877.2131443}, "models": {"yolo": {"arrival": 1711329876.9375963, "serving": 1711329877.1708195}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4167888, "arrival": 1711329882.4684393}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4163642, "arrival": 1711329877.2143312}, "models": {"yolo": {"arrival": 1711329876.9375963, "serving": 1711329877.1708195}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.542525842+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.6196282}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.6203651, "arrival": 1711329882.346213}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4165032, "arrival": 1711329882.4677076}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.518118781+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.4160733}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.546482206+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.619883}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.43333, "arrival": 1711329880.8678982}, "models": {"yolo": {"arrival": 1711329880.263235, "serving": 1711329880.842692}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4533448, "arrival": 1711329882.1005557}, "models": {"yolo": {"arrival": 1711329881.8189707, "serving": 1711329882.06022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.922636171+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4331973}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4335144, "arrival": 1711329882.48469}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.730756708+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.6195586}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.489156023+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.6202295}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.007710868+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.619928}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.727953474+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4165602}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.728040032+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.433393}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4161863, "arrival": 1711329870.8064322}, "models": {"yolo": {"arrival": 1711329870.4483318, "serving": 1711329870.7937944}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.534604526+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.4334729}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.433739, "arrival": 1711329882.3390796}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.6203213, "arrival": 1711329882.5060947}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4162157, "arrival": 1711329880.861749}, "models": {"yolo": {"arrival": 1711329880.263235, "serving": 1711329880.842692}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.546591215+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.6204073}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4332426, "arrival": 1711329882.3380396}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4162438, "arrival": 1711329882.465452}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.534587476+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.433307}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4532602, "arrival": 1711329882.4924374}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.453126, "arrival": 1711329882.4873655}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.00655049+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4532766}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4335942, "arrival": 1711329877.5054066}, "models": {"yolo": {"arrival": 1711329877.183118, "serving": 1711329877.460159}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.727878214+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.4162993}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.922547151+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.416045}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4530404, "arrival": 1711329877.5067146}, "models": {"yolo": {"arrival": 1711329877.183118, "serving": 1711329877.460159}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.489090694+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.6199431}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4332647, "arrival": 1711329877.4894955}, "models": {"yolo": {"arrival": 1711329877.183118, "serving": 1711329877.460159}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.433413, "arrival": 1711329882.338447}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4338171, "arrival": 1711329882.4860017}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.542452989+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.453227}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4529996, "arrival": 1711329882.3393571}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4331732, "arrival": 1711329882.4809055}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.542489003+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.453496}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.6203933, "arrival": 1711329882.3333228}, "models": {"yolo": {"arrival": 1711329882.072867, "serving": 1711329882.2956703}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.542507361+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.6194723}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4164145, "arrival": 1711329881.8340898}, "models": {"yolo": {"arrival": 1711329881.5690866, "serving": 1711329881.8048663}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4535267, "arrival": 1711329882.4935906}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.6202757, "arrival": 1711329882.3311937}, "models": {"yolo": {"arrival": 1711329882.072867, "serving": 1711329882.2956703}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.453327, "arrival": 1711329877.915059}, "models": {"yolo": {"arrival": 1711329877.4752676, "serving": 1711329877.8997042}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.6193993, "arrival": 1711329882.1018932}, "models": {"yolo": {"arrival": 1711329881.8189707, "serving": 1711329882.06022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.728066402+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.4335551}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.433433, "arrival": 1711329877.4899254}, "models": {"yolo": {"arrival": 1711329877.183118, "serving": 1711329877.460159}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.416158, "arrival": 1711329881.833836}, "models": {"yolo": {"arrival": 1711329881.5690866, "serving": 1711329881.8048663}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.619851, "arrival": 1711329877.9252815}, "models": {"yolo": {"arrival": 1711329877.4752676, "serving": 1711329877.8997042}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4334536, "arrival": 1711329882.0975428}, "models": {"yolo": {"arrival": 1711329881.8189707, "serving": 1711329882.06022}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.546543248+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.6200044}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.730804118+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.6198194}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.6201804, "arrival": 1711329881.7962723}, "models": {"yolo": {"arrival": 1711329881.5541813, "serving": 1711329881.7826624}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.61999, "arrival": 1711329882.326815}, "models": {"yolo": {"arrival": 1711329882.072867, "serving": 1711329882.2956703}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.6198351, "arrival": 1711329882.344985}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.53448916+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.416445}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.534546479+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4167309}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.6197248, "arrival": 1711329877.9241078}, "models": {"yolo": {"arrival": 1711329877.4752676, "serving": 1711329877.8997042}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.923227657+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.453144}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.728201556+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4534261}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.542472524+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.453361}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4535115, "arrival": 1711329881.568084}, "models": {"yolo": {"arrival": 1711329881.2586017, "serving": 1711329881.545589}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.542430046+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.4530895}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4334948, "arrival": 1711329880.8691633}, "models": {"yolo": {"arrival": 1711329880.263235, "serving": 1711329880.842692}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.542336317+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4337866}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.6194942, "arrival": 1711329881.568861}, "models": {"yolo": {"arrival": 1711329881.2586017, "serving": 1711329881.545589}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.433756, "arrival": 1711329877.5063765}, "models": {"yolo": {"arrival": 1711329877.183118, "serving": 1711329877.460159}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.453377, "arrival": 1711329881.5670376}, "models": {"yolo": {"arrival": 1711329881.2586017, "serving": 1711329881.545589}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.728013438+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.4332204}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4336138, "arrival": 1711329882.0981474}, "models": {"yolo": {"arrival": 1711329881.8189707, "serving": 1711329882.06022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4330235, "arrival": 1711329881.835382}, "models": {"yolo": {"arrival": 1711329881.5690866, "serving": 1711329881.8048663}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.6195762, "arrival": 1711329882.344254}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.006648595+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.4534104}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4533095, "arrival": 1711329882.3399081}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4167612, "arrival": 1711329880.8648179}, "models": {"yolo": {"arrival": 1711329880.263235, "serving": 1711329880.842692}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4534593, "arrival": 1711329877.917766}, "models": {"yolo": {"arrival": 1711329877.4752676, "serving": 1711329877.8997042}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.6197412, "arrival": 1711329882.325113}, "models": {"yolo": {"arrival": 1711329882.072867, "serving": 1711329882.2956703}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.416701, "arrival": 1711329881.834337}, "models": {"yolo": {"arrival": 1711329881.5690866, "serving": 1711329881.8048663}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4166079, "arrival": 1711329880.274542}, "models": {"yolo": {"arrival": 1711329879.9769464, "serving": 1711329880.2529194}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.730782488+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.6196923}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.542541586+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.6197565}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.728152915+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4531612}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4535766, "arrival": 1711329882.3416698}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.534566767+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4331167}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.433674, "arrival": 1711329882.4853811}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.6197083, "arrival": 1711329882.344611}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4335747, "arrival": 1711329882.3387833}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 23.64 GiB total capacity; 1.64 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.922680689+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.433534}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4331486, "arrival": 1711329880.8671877}, "models": {"yolo": {"arrival": 1711329880.263235, "serving": 1711329880.842692}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.6199589, "arrival": 1711329882.3453925}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4534795, "arrival": 1711329882.1011486}, "models": {"yolo": {"arrival": 1711329881.8189707, "serving": 1711329882.06022}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.620245, "arrival": 1711329882.3458345}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.433802, "arrival": 1711329881.2646}, "models": {"yolo": {"arrival": 1711329880.8515086, "serving": 1711329881.2502594}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.54656907+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.620292}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.6202595, "arrival": 1711329877.9273195}, "models": {"yolo": {"arrival": 1711329877.4752676, "serving": 1711329877.8997042}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.727982589+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4168458}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.620379, "arrival": 1711329878.1247778}, "models": {"yolo": {"arrival": 1711329877.9096937, "serving": 1711329878.111925}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:30.728174847+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.4532924}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.453108, "arrival": 1711329881.265324}, "models": {"yolo": {"arrival": 1711329880.8515086, "serving": 1711329881.2502594}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.489180936+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.6203501}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4531934, "arrival": 1711329877.5070434}, "models": {"yolo": {"arrival": 1711329877.183118, "serving": 1711329877.460159}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:31.534623926+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.4336343}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4169033, "arrival": 1711329877.4882808}, "models": {"yolo": {"arrival": 1711329877.183118, "serving": 1711329877.460159}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:30.728125512+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 16.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.452951}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.6197724, "arrival": 1711329881.5718637}, "models": {"yolo": {"arrival": 1711329881.2586017, "serving": 1711329881.545589}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.619525, "arrival": 1711329882.4942186}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.619593, "arrival": 1711329877.919372}, "models": {"yolo": {"arrival": 1711329877.4752676, "serving": 1711329877.8997042}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.006731682+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.6195414}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.6196592, "arrival": 1711329882.4950376}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.6201978, "arrival": 1711329882.5053494}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.619611, "arrival": 1711329882.3243482}, "models": {"yolo": {"arrival": 1711329882.072867, "serving": 1711329882.2956703}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329870.4531777, "arrival": 1711329882.3396356}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.006770322+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.6196754}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.4534428, "arrival": 1711329882.341349}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.006810166+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329870.6198041}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.007806992+00:00\"}\"\n>", "times": {"request": {"sending": 1711329870.6203358}}, "outputs": []}, {"times": {"request": {"sending": 1711329870.619975, "arrival": 1711329877.926083}, "models": {"yolo": {"arrival": 1711329877.4752676, "serving": 1711329877.8997042}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329871.4142082, "arrival": 1711329882.5082078}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.4329588+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4456923}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4462454, "arrival": 1711329878.9341478}, "models": {"yolo": {"arrival": 1711329878.4531105, "serving": 1711329878.8927512}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.01252203+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.437218}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4373312, "arrival": 1711329882.1088455}, "models": {"yolo": {"arrival": 1711329881.7939184, "serving": 1711329882.0637972}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4374032, "arrival": 1711329882.3500857}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.50173002+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4467669}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4461815, "arrival": 1711329882.5195127}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4469798, "arrival": 1711329879.2392344}, "models": {"yolo": {"arrival": 1711329878.9014952, "serving": 1711329879.1905897}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.501410209+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4334147}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4137766, "arrival": 1711329882.3465846}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4458046, "arrival": 1711329882.5181077}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.446919, "arrival": 1711329882.5246441}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.012444462+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4142404}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4141102, "arrival": 1711329882.3342803}, "models": {"yolo": {"arrival": 1711329882.072867, "serving": 1711329882.2956703}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4138458, "arrival": 1711329882.3338444}, "models": {"yolo": {"arrival": 1711329882.072867, "serving": 1711329882.2956703}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.442119828+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4464536}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.442213014+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4470563}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4139476, "arrival": 1711329882.5075192}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.501572432+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4457715}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.007858093+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4137056}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4136617, "arrival": 1711329882.506844}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4135764, "arrival": 1711329881.8004863}, "models": {"yolo": {"arrival": 1711329881.5541813, "serving": 1711329881.7826624}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4141762, "arrival": 1711329881.803099}, "models": {"yolo": {"arrival": 1711329881.5541813, "serving": 1711329881.7826624}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.489247441+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4140139}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.442098709+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4463222}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4457874, "arrival": 1711329882.1103237}, "models": {"yolo": {"arrival": 1711329881.7939184, "serving": 1711329882.0637972}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4372547, "arrival": 1711329882.349754}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.012399346+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.413981}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4370847, "arrival": 1711329882.7153563}, "models": {"yolo": {"arrival": 1711329882.3075528, "serving": 1711329882.698381}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.437439, "arrival": 1711329882.7251384}, "models": {"yolo": {"arrival": 1711329882.3075528, "serving": 1711329882.698381}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 82.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 18.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.489202849+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4137423}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.501507614+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4374561}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.501878036+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4803786}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.501457743+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4371562}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.054423225+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4463367}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.43718, "arrival": 1711329882.1084616}, "models": {"yolo": {"arrival": 1711329881.7939184, "serving": 1711329882.0637972}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4374728, "arrival": 1711329882.109223}, "models": {"yolo": {"arrival": 1711329881.7939184, "serving": 1711329882.0637972}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.013469795+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4453597}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.432986955+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.445821}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4457555, "arrival": 1711329882.7271752}, "models": {"yolo": {"arrival": 1711329882.3075528, "serving": 1711329882.698381}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4467366, "arrival": 1711329878.9518125}, "models": {"yolo": {"arrival": 1711329878.4531105, "serving": 1711329878.8927512}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.054221583+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4455771}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.446436, "arrival": 1711329882.5235438}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4455407, "arrival": 1711329882.5147436}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.447041, "arrival": 1711329882.525102}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4463818, "arrival": 1711329883.0005805}, "models": {"yolo": {"arrival": 1711329882.7077677, "serving": 1711329882.9609323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4459934, "arrival": 1711329878.486932}, "models": {"yolo": {"arrival": 1711329878.1229343, "serving": 1711329878.4412324}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.546613754+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4138799}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4138122, "arrival": 1711329878.1256847}, "models": {"yolo": {"arrival": 1711329877.9096937, "serving": 1711329878.111925}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.442170668+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4468133}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.442192061+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4469333}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.414046, "arrival": 1711329882.347706}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.445978, "arrival": 1711329882.3525124}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.437349, "arrival": 1711329882.5137222}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.501634806+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4461517}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4144554, "arrival": 1711329881.8037927}, "models": {"yolo": {"arrival": 1711329881.5541813, "serving": 1711329881.7826624}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.047063177+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4372368}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4459128, "arrival": 1711329882.1106977}, "models": {"yolo": {"arrival": 1711329881.7939184, "serving": 1711329882.0637972}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.054328381+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4459617}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4462907, "arrival": 1711329882.3174627}, "models": {"yolo": {"arrival": 1711329882.075091, "serving": 1711329882.2909462}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.05448872+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4468284}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4465144, "arrival": 1711329883.001171}, "models": {"yolo": {"arrival": 1711329882.7077677, "serving": 1711329882.9609323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4803615, "arrival": 1711329883.2883809}, "models": {"yolo": {"arrival": 1711329882.9704099, "serving": 1711329883.2514005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.012477094+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.414521}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.501340943+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4144197}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4464839, "arrival": 1711329882.3552587}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.054445208+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4464688}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4371996, "arrival": 1711329882.5132582}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4333072, "arrival": 1711329878.1606212}, "models": {"yolo": {"arrival": 1711329877.9096937, "serving": 1711329878.111925}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.012500558+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.433475}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.442072565+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4461997}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4468737, "arrival": 1711329883.2871487}, "models": {"yolo": {"arrival": 1711329882.9704099, "serving": 1711329883.2514005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.442043267+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4460733}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.501483222+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.437312}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.046945722+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4142745}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.50175629+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4468887}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.442142485+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.446688}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.413915, "arrival": 1711329881.801278}, "models": {"yolo": {"arrival": 1711329881.5541813, "serving": 1711329881.7826624}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.414488, "arrival": 1711329882.5099978}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4143748, "arrival": 1711329882.713374}, "models": {"yolo": {"arrival": 1711329882.3075528, "serving": 1711329882.698381}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4457395, "arrival": 1711329878.4864604}, "models": {"yolo": {"arrival": 1711329878.1229343, "serving": 1711329878.4412324}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.047086659+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4373853}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.433512, "arrival": 1711329882.3494847}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.047108878+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4454293}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4335325, "arrival": 1711329878.1609397}, "models": {"yolo": {"arrival": 1711329877.9096937, "serving": 1711329878.111925}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4461052, "arrival": 1711329882.3527706}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4454687, "arrival": 1711329878.485963}, "models": {"yolo": {"arrival": 1711329878.1229343, "serving": 1711329878.4412324}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:31.546635569+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.414142}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4140785, "arrival": 1711329878.1266406}, "models": {"yolo": {"arrival": 1711329877.9096937, "serving": 1711329878.111925}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4454508, "arrival": 1711329882.3503292}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.501613997+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4460254}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4143083, "arrival": 1711329882.3479707}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.441980823+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.445945}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.446782, "arrival": 1711329882.3318248}, "models": {"yolo": {"arrival": 1711329882.075091, "serving": 1711329882.2909462}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.054374061+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.446215}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.446351, "arrival": 1711329882.3549945}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4460552, "arrival": 1711329882.5192034}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4143426, "arrival": 1711329878.1603065}, "models": {"yolo": {"arrival": 1711329877.9096937, "serving": 1711329878.111925}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.047039044+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4334931}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4372938, "arrival": 1711329882.7220016}, "models": {"yolo": {"arrival": 1711329882.3075528, "serving": 1711329882.698381}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4334362, "arrival": 1711329882.108011}, "models": {"yolo": {"arrival": 1711329881.7939184, "serving": 1711329882.0637972}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.501529811+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4455047}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4459276, "arrival": 1711329882.5185778}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4468436, "arrival": 1711329882.355952}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.445723, "arrival": 1711329882.351487}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.445674, "arrival": 1711329882.5151281}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.501593051+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.445897}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.0470114+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4145544}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.445658, "arrival": 1711329882.1099572}, "models": {"yolo": {"arrival": 1711329881.7939184, "serving": 1711329882.0637972}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.501703206+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4465299}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4460404, "arrival": 1711329882.1110497}, "models": {"yolo": {"arrival": 1711329881.7939184, "serving": 1711329882.0637972}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4460099, "arrival": 1711329882.9894361}, "models": {"yolo": {"arrival": 1711329882.7077677, "serving": 1711329882.9609323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4802692, "arrival": 1711329882.3575575}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.501778023+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4470103}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4334562, "arrival": 1711329882.5106754}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 23.64 GiB total capacity; 1.72 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.432889683+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4455595}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4467208, "arrival": 1711329882.355483}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.054305573+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4458363}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.501551333+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4456415}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4464993, "arrival": 1711329878.9405096}, "models": {"yolo": {"arrival": 1711329878.4531105, "serving": 1711329878.8927512}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.445522, "arrival": 1711329882.1095886}, "models": {"yolo": {"arrival": 1711329881.7939184, "serving": 1711329882.0637972}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4467976, "arrival": 1711329882.5243037}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.054278902+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.445708}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4469035, "arrival": 1711329882.332367}, "models": {"yolo": {"arrival": 1711329882.075091, "serving": 1711329882.2909462}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.501683086+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4464037}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4465613, "arrival": 1711329882.5239434}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.05451152+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4469488}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4333913, "arrival": 1711329882.7145236}, "models": {"yolo": {"arrival": 1711329882.3075528, "serving": 1711329882.698381}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4462607, "arrival": 1711329882.9930098}, "models": {"yolo": {"arrival": 1711329882.7077677, "serving": 1711329882.9609323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.437275, "arrival": 1711329878.4854517}, "models": {"yolo": {"arrival": 1711329878.1229343, "serving": 1711329878.4412324}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4461212, "arrival": 1711329878.9335585}, "models": {"yolo": {"arrival": 1711329878.4531105, "serving": 1711329878.8927512}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4374223, "arrival": 1711329878.4856865}, "models": {"yolo": {"arrival": 1711329878.1229343, "serving": 1711329878.4412324}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4455938, "arrival": 1711329882.3512447}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4458818, "arrival": 1711329882.988668}, "models": {"yolo": {"arrival": 1711329882.7077677, "serving": 1711329882.9609323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4456108, "arrival": 1711329878.4861965}, "models": {"yolo": {"arrival": 1711329878.1229343, "serving": 1711329878.4412324}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4469948, "arrival": 1711329883.2880988}, "models": {"yolo": {"arrival": 1711329882.9704099, "serving": 1711329883.2514005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.445626, "arrival": 1711329882.7265558}, "models": {"yolo": {"arrival": 1711329882.3075528, "serving": 1711329882.698381}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4461663, "arrival": 1711329882.3166132}, "models": {"yolo": {"arrival": 1711329882.075091, "serving": 1711329882.2909462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4374902, "arrival": 1711329882.5142224}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.446306, "arrival": 1711329882.519853}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.445852, "arrival": 1711329882.3517003}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4145865, "arrival": 1711329882.348192}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4468586, "arrival": 1711329878.9526186}, "models": {"yolo": {"arrival": 1711329878.4531105, "serving": 1711329878.8927512}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4463665, "arrival": 1711329878.935075}, "models": {"yolo": {"arrival": 1711329878.4531105, "serving": 1711329878.8927512}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.501657004+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.446276}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4454865, "arrival": 1711329882.7258775}, "models": {"yolo": {"arrival": 1711329882.3075528, "serving": 1711329882.698381}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.054466755+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.446705}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.054351867+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329871.4460902}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 2.36 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.054534671+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.447072}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 278.00 MiB (GPU 0; 23.64 GiB total capacity; 1.69 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.012543731+00:00\"}\"\n>", "times": {"request": {"sending": 1711329871.4373667}}, "outputs": []}, {"times": {"request": {"sending": 1711329871.4461365, "arrival": 1711329882.992301}, "models": {"yolo": {"arrival": 1711329882.7077677, "serving": 1711329882.9609323}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.446752, "arrival": 1711329883.2868907}, "models": {"yolo": {"arrival": 1711329882.9704099, "serving": 1711329883.2514005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.44623, "arrival": 1711329882.3547368}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.48034, "arrival": 1711329879.2401378}, "models": {"yolo": {"arrival": 1711329878.9014952, "serving": 1711329879.1905897}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4470263, "arrival": 1711329882.3328075}, "models": {"yolo": {"arrival": 1711329882.075091, "serving": 1711329882.2909462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4465458, "arrival": 1711329882.323107}, "models": {"yolo": {"arrival": 1711329882.075091, "serving": 1711329882.2909462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4458673, "arrival": 1711329878.4867094}, "models": {"yolo": {"arrival": 1711329878.1229343, "serving": 1711329878.4412324}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4464204, "arrival": 1711329882.3181694}, "models": {"yolo": {"arrival": 1711329882.075091, "serving": 1711329882.2909462}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329871.4469647, "arrival": 1711329882.357283}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}], [{"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.629846731+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.450974}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4511187, "arrival": 1711329882.543855}, "models": {"yolo": {"arrival": 1711329882.3301528, "serving": 1711329882.534067}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4169204, "arrival": 1711329883.2956371}, "models": {"yolo": {"arrival": 1711329882.9704099, "serving": 1711329883.2514005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4130158, "arrival": 1711329882.792164}, "models": {"yolo": {"arrival": 1711329882.3021498, "serving": 1711329882.7750201}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4380288, "arrival": 1711329879.5668948}, "models": {"yolo": {"arrival": 1711329879.2008786, "serving": 1711329879.5289116}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.617106773+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4371896}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4382617, "arrival": 1711329880.0143824}, "models": {"yolo": {"arrival": 1711329879.5435073, "serving": 1711329879.971327}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.622031929+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4509573}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.41287, "arrival": 1711329882.358653}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.617867864+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4128323}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4385304, "arrival": 1711329883.0120633}, "models": {"yolo": {"arrival": 1711329882.7874882, "serving": 1711329882.9714508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4129071, "arrival": 1711329879.2406769}, "models": {"yolo": {"arrival": 1711329878.9014952, "serving": 1711329879.1905897}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4375927, "arrival": 1711329883.585299}, "models": {"yolo": {"arrival": 1711329883.264629, "serving": 1711329883.554607}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.617897099+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4131417}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.624424727+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4375381}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.665118962+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4376104}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4170024, "arrival": 1711329882.7931063}, "models": {"yolo": {"arrival": 1711329882.3021498, "serving": 1711329882.7750201}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.412743, "arrival": 1711329882.52545}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4510071, "arrival": 1711329880.016269}, "models": {"yolo": {"arrival": 1711329879.5435073, "serving": 1711329879.971327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4383795, "arrival": 1711329882.531821}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.485215425+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.451218}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.451532, "arrival": 1711329885.678823}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4372394, "arrival": 1711329882.3620756}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.413195, "arrival": 1711329882.3589203}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4511337, "arrival": 1711329880.0249805}, "models": {"yolo": {"arrival": 1711329879.5435073, "serving": 1711329879.971327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4126475, "arrival": 1711329882.7896657}, "models": {"yolo": {"arrival": 1711329882.3021498, "serving": 1711329882.7750201}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4515624, "arrival": 1711329883.2866178}, "models": {"yolo": {"arrival": 1711329882.9815633, "serving": 1711329883.254511}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4382336, "arrival": 1711329882.3664992}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.657584888+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4169648}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.617136047+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4373648}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.612045691+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4130852}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.665236111+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4385118}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.612097391+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4173622}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.62966948+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4376893}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4170396, "arrival": 1711329882.5261178}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.665304301+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.451166}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.657528255+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4129784}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4378972, "arrival": 1711329883.9015062}, "models": {"yolo": {"arrival": 1711329883.56365, "serving": 1711329883.8798084}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.667731914+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4514225}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4175704, "arrival": 1711329882.7960095}, "models": {"yolo": {"arrival": 1711329882.3021498, "serving": 1711329882.7750201}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.611975842+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4127917}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4372609, "arrival": 1711329879.5605288}, "models": {"yolo": {"arrival": 1711329879.2008786, "serving": 1711329879.5289116}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.451516, "arrival": 1711329880.3060024}, "models": {"yolo": {"arrival": 1711329879.9807422, "serving": 1711329880.2610657}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4339774, "arrival": 1711329882.7980795}, "models": {"yolo": {"arrival": 1711329882.3021498, "serving": 1711329882.7750201}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4513288, "arrival": 1711329883.2571182}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.413051, "arrival": 1711329882.5257902}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.620566185+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4379723}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.629804251+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4384372}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4339151, "arrival": 1711329879.5600119}, "models": {"yolo": {"arrival": 1711329879.2008786, "serving": 1711329879.5289116}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4515781, "arrival": 1711329883.261317}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4129431, "arrival": 1711329883.295432}, "models": {"yolo": {"arrival": 1711329882.9704099, "serving": 1711329883.2514005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.437426, "arrival": 1711329879.5609448}, "models": {"yolo": {"arrival": 1711329879.2008786, "serving": 1711329879.5289116}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.489665304+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4514701}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4386077, "arrival": 1711329882.3675272}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4174325, "arrival": 1711329882.3614564}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4339366, "arrival": 1711329883.5663059}, "models": {"yolo": {"arrival": 1711329883.264629, "serving": 1711329883.554607}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4512496, "arrival": 1711329883.0016172}, "models": {"yolo": {"arrival": 1711329882.5427196, "serving": 1711329882.968724}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.665191409+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4380822}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.620622243+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4384165}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.450991, "arrival": 1711329882.543376}, "models": {"yolo": {"arrival": 1711329882.3301528, "serving": 1711329882.534067}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.438475, "arrival": 1711329880.0150166}, "models": {"yolo": {"arrival": 1711329879.5435073, "serving": 1711329879.971327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.438009, "arrival": 1711329882.3661938}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4173276, "arrival": 1711329882.5264764}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4513752, "arrival": 1711329883.0020251}, "models": {"yolo": {"arrival": 1711329882.5427196, "serving": 1711329882.968724}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.612073372+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.417076}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.451024, "arrival": 1711329883.9098985}, "models": {"yolo": {"arrival": 1711329883.56365, "serving": 1711329883.8798084}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.417292, "arrival": 1711329882.7952833}, "models": {"yolo": {"arrival": 1711329882.3021498, "serving": 1711329882.7750201}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4514394, "arrival": 1711329883.2863317}, "models": {"yolo": {"arrival": 1711329882.9815633, "serving": 1711329883.254511}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4338918, "arrival": 1711329882.3617973}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4374442, "arrival": 1711329883.5725694}, "models": {"yolo": {"arrival": 1711329883.264629, "serving": 1711329883.554607}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4512012, "arrival": 1711329882.552132}, "models": {"yolo": {"arrival": 1711329882.479336, "serving": 1711329882.543427}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4174669, "arrival": 1711329879.245616}, "models": {"yolo": {"arrival": 1711329878.9014952, "serving": 1711329879.1905897}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4171498, "arrival": 1711329882.359144}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.624353458+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4338644}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.460598, "arrival": 1711329880.3065748}, "models": {"yolo": {"arrival": 1711329879.9807422, "serving": 1711329880.2610657}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.617182586+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4376695}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.66504832+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4339576}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4509213, "arrival": 1711329883.012562}, "models": {"yolo": {"arrival": 1711329882.7874882, "serving": 1711329882.9714508}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:33.485322627+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4512336}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.43763, "arrival": 1711329882.8012977}, "models": {"yolo": {"arrival": 1711329882.3021498, "serving": 1711329882.7750201}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4514065, "arrival": 1711329885.678233}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4385512, "arrival": 1711329882.5321789}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.48526188+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4511034}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.416845, "arrival": 1711329879.2413158}, "models": {"yolo": {"arrival": 1711329878.9014952, "serving": 1711329879.1905897}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.437481, "arrival": 1711329882.8007722}, "models": {"yolo": {"arrival": 1711329882.3021498, "serving": 1711329882.7750201}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4511492, "arrival": 1711329885.6761892}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4377081, "arrival": 1711329882.3647716}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.417221, "arrival": 1711329883.2958646}, "models": {"yolo": {"arrival": 1711329882.9704099, "serving": 1711329883.2514005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4375565, "arrival": 1711329882.3644207}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4379537, "arrival": 1711329882.5304768}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4382918, "arrival": 1711329883.9077797}, "models": {"yolo": {"arrival": 1711329883.56365, "serving": 1711329883.8798084}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:33.485448527+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4605012}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.629825626+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.438589}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4377263, "arrival": 1711329879.566266}, "models": {"yolo": {"arrival": 1711329879.2008786, "serving": 1711329879.5289116}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.624267733+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.417114}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:33.485363769+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4513595}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4375744, "arrival": 1711329879.5616753}, "models": {"yolo": {"arrival": 1711329879.2008786, "serving": 1711329879.5289116}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.417185, "arrival": 1711329879.2451763}, "models": {"yolo": {"arrival": 1711329878.9014952, "serving": 1711329879.1905897}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4373453, "arrival": 1711329882.5296276}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4509404, "arrival": 1711329882.5323877}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.61715968+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4375196}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4508073, "arrival": 1711329880.0156324}, "models": {"yolo": {"arrival": 1711329879.5435073, "serving": 1711329879.971327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4605768, "arrival": 1711329883.0057626}, "models": {"yolo": {"arrival": 1711329882.5427196, "serving": 1711329882.968724}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.657612103+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4172564}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4508808, "arrival": 1711329883.909227}, "models": {"yolo": {"arrival": 1711329883.56365, "serving": 1711329883.8798084}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4515011, "arrival": 1711329883.0051785}, "models": {"yolo": {"arrival": 1711329882.5427196, "serving": 1711329882.968724}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.665073035+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4373038}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.437651, "arrival": 1711329882.5300474}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.66521371+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4383204}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4176059, "arrival": 1711329882.52907}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.667759606+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.451547}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.489611073+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.451344}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4512658, "arrival": 1711329880.025729}, "models": {"yolo": {"arrival": 1711329879.5435073, "serving": 1711329879.971327}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4381156, "arrival": 1711329883.0073433}, "models": {"yolo": {"arrival": 1711329882.7874882, "serving": 1711329882.9714508}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.665096446+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4374619}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.437116, "arrival": 1711329882.529418}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4513912, "arrival": 1711329880.3054426}, "models": {"yolo": {"arrival": 1711329879.9807422, "serving": 1711329880.2610657}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.620512519+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4378204}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4384935, "arrival": 1711329883.9084957}, "models": {"yolo": {"arrival": 1711329883.56365, "serving": 1711329883.8798084}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.667677929+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4512959}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4510548, "arrival": 1711329883.0129926}, "models": {"yolo": {"arrival": 1711329882.7874882, "serving": 1711329882.9714508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4377449, "arrival": 1711329883.586649}, "models": {"yolo": {"arrival": 1711329883.264629, "serving": 1711329883.554607}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.624403371+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4373858}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4384558, "arrival": 1711329882.3672752}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4513106, "arrival": 1711329883.286068}, "models": {"yolo": {"arrival": 1711329882.9815633, "serving": 1711329883.254511}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.485407472+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4514854}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.66514307+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.437764}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.667783903+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4613256}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4378018, "arrival": 1711329882.5302498}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.624378046+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4372168}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.417501, "arrival": 1711329883.565556}, "models": {"yolo": {"arrival": 1711329883.264629, "serving": 1711329883.554607}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.665282065+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4510396}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4514546, "arrival": 1711329883.257444}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.451071, "arrival": 1711329882.551085}, "models": {"yolo": {"arrival": 1711329882.479336, "serving": 1711329882.543427}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.665258998+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.4509015}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.665167577+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4379153}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.438147, "arrival": 1711329882.5306861}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4379337, "arrival": 1711329883.0068946}, "models": {"yolo": {"arrival": 1711329882.7874882, "serving": 1711329882.9714508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.438349, "arrival": 1711329883.0078006}, "models": {"yolo": {"arrival": 1711329882.7874882, "serving": 1711329882.9714508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.437323, "arrival": 1711329882.7990158}, "models": {"yolo": {"arrival": 1711329882.3021498, "serving": 1711329882.7750201}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.437501, "arrival": 1711329882.5298355}, "models": {"yolo": {"arrival": 1711329880.308656, "serving": 1711329882.448202}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:32.624325608+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.417398}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4374063, "arrival": 1711329882.3623476}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.629728614+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.43784}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.617049866+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4337738}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4612715, "arrival": 1711329885.6897902}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.620595049+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.438176}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.485088374+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.451087}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.629756698+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4379904}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 2.40 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.629781656+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4382048}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.489726531+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329872.451593}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.664992382+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.4175348}}, "outputs": []}, {"times": {"request": {"sending": 1711329872.4377813, "arrival": 1711329883.006376}, "models": {"yolo": {"arrival": 1711329882.7874882, "serving": 1711329882.9714508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4372828, "arrival": 1711329883.5701804}, "models": {"yolo": {"arrival": 1711329883.264629, "serving": 1711329883.554607}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.438047, "arrival": 1711329883.9068298}, "models": {"yolo": {"arrival": 1711329883.56365, "serving": 1711329883.8798084}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4378784, "arrival": 1711329879.5666635}, "models": {"yolo": {"arrival": 1711329879.2008786, "serving": 1711329879.5289116}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4512804, "arrival": 1711329885.6767175}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4511836, "arrival": 1711329883.285786}, "models": {"yolo": {"arrival": 1711329882.9815633, "serving": 1711329883.254511}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329872.4378593, "arrival": 1711329882.365254}, "models": {"yolo": {"arrival": 1711329880.3083375, "serving": 1711329882.2966537}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 244.00 MiB (GPU 0; 23.64 GiB total capacity; 1.77 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:32.620644766+00:00\"}\"\n>", "times": {"request": {"sending": 1711329872.43857}}, "outputs": []}], [{"times": {"request": {"sending": 1711329873.4373717, "arrival": 1711329885.7027917}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4132743, "arrival": 1711329885.6944847}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.414284, "arrival": 1711329880.8950994}, "models": {"yolo": {"arrival": 1711329880.2707727, "serving": 1711329880.882098}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4436967, "arrival": 1711329885.709171}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.609349217+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4140906}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.413089, "arrival": 1711329883.2630188}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.443272, "arrival": 1711329885.7084796}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.444185, "arrival": 1711329885.7134688}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4134734, "arrival": 1711329883.0136733}, "models": {"yolo": {"arrival": 1711329882.5427196, "serving": 1711329882.968724}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.437036, "arrival": 1711329880.8982086}, "models": {"yolo": {"arrival": 1711329880.2707727, "serving": 1711329880.882098}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.150617148+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.437153}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.150767548+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.443634}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.150862452+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4442475}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4437926, "arrival": 1711329883.5952609}, "models": {"yolo": {"arrival": 1711329883.2572975, "serving": 1711329883.5618508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4445333, "arrival": 1711329881.8089037}, "models": {"yolo": {"arrival": 1711329881.5463626, "serving": 1711329881.7887275}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.165057842+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.444141}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4139953, "arrival": 1711329883.0142496}, "models": {"yolo": {"arrival": 1711329882.5427196, "serving": 1711329882.968724}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4129946, "arrival": 1711329883.2931252}, "models": {"yolo": {"arrival": 1711329882.9815633, "serving": 1711329883.254511}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.13151435+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4141877}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4441097, "arrival": 1711329883.556055}, "models": {"yolo": {"arrival": 1711329883.2605462, "serving": 1711329883.541111}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4371908, "arrival": 1711329883.2633755}, "models": {"yolo": {"arrival": 1711329882.9808147, "serving": 1711329883.2451627}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.609512702+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4373894}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.413538, "arrival": 1711329885.6949816}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.609565352+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4432895}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4141555, "arrival": 1711329883.2827668}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4436169, "arrival": 1711329883.2969697}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4441705, "arrival": 1711329881.5606644}, "models": {"yolo": {"arrival": 1711329881.2423134, "serving": 1711329881.5352178}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.609586595+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4434173}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4142516, "arrival": 1711329883.014576}, "models": {"yolo": {"arrival": 1711329882.5427196, "serving": 1711329882.968724}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:33.489789418+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4131336}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4437304, "arrival": 1711329883.9196155}, "models": {"yolo": {"arrival": 1711329883.565928, "serving": 1711329883.8846238}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4444556, "arrival": 1711329885.6626306}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4138658, "arrival": 1711329883.2951941}, "models": {"yolo": {"arrival": 1711329882.9815633, "serving": 1711329883.254511}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4437451, "arrival": 1711329883.2972436}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4136038, "arrival": 1711329883.29497}, "models": {"yolo": {"arrival": 1711329882.9815633, "serving": 1711329883.254511}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:33.609631334+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4437132}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4140272, "arrival": 1711329880.8928332}, "models": {"yolo": {"arrival": 1711329880.2707727, "serving": 1711329880.882098}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.164904081+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4430523}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.413342, "arrival": 1711329883.2947493}, "models": {"yolo": {"arrival": 1711329882.9815633, "serving": 1711329883.254511}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4372623, "arrival": 1711329883.5890963}, "models": {"yolo": {"arrival": 1711329883.267031, "serving": 1711329883.558023}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.150674636+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4429772}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4137654, "arrival": 1711329880.3160994}, "models": {"yolo": {"arrival": 1711329879.9807422, "serving": 1711329880.2610657}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.150813658+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.443887}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4336882, "arrival": 1711329885.7017403}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:33.489578698+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4134402}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4443958, "arrival": 1711329883.597606}, "models": {"yolo": {"arrival": 1711329883.2572975, "serving": 1711329883.5618508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4444714, "arrival": 1711329883.9274166}, "models": {"yolo": {"arrival": 1711329883.5518067, "serving": 1711329883.8939111}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4443069, "arrival": 1711329885.7137113}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.165102675+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.444382}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.48950591+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 2.42 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4131703}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4336684, "arrival": 1711329880.895923}, "models": {"yolo": {"arrival": 1711329880.2707727, "serving": 1711329880.882098}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4431129, "arrival": 1711329881.2487528}, "models": {"yolo": {"arrival": 1711329880.8902347, "serving": 1711329881.2358506}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4440923, "arrival": 1711329883.929486}, "models": {"yolo": {"arrival": 1711329883.565928, "serving": 1711329883.8846238}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4132063, "arrival": 1711329883.013371}, "models": {"yolo": {"arrival": 1711329882.5427196, "serving": 1711329882.968724}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4439187, "arrival": 1711329883.5957077}, "models": {"yolo": {"arrival": 1711329883.2572975, "serving": 1711329883.5618508}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.165036383+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4439032}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.598563513+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4138298}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4137337, "arrival": 1711329883.0139709}, "models": {"yolo": {"arrival": 1711329882.5427196, "serving": 1711329882.968724}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4138985, "arrival": 1711329883.2819614}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.444292, "arrival": 1711329881.8075323}, "models": {"yolo": {"arrival": 1711329881.5463626, "serving": 1711329881.7887275}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.150884993+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4443676}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4438236, "arrival": 1711329885.7126327}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.413375, "arrival": 1711329883.263623}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.16469238+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4137008}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4439511, "arrival": 1711329885.7132158}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.131485947+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4139304}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4442773, "arrival": 1711329883.5970933}, "models": {"yolo": {"arrival": 1711329883.2572975, "serving": 1711329883.5618508}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.489852369+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 23.64 GiB total capacity; 1.79 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4134076}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.598453306+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4133072}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4132397, "arrival": 1711329880.3073478}, "models": {"yolo": {"arrival": 1711329879.9807422, "serving": 1711329880.2610657}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.164870282+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.437318}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.437337, "arrival": 1711329883.2648852}, "models": {"yolo": {"arrival": 1711329882.9808147, "serving": 1711329883.2451627}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:33.609411915+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4143484}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4438717, "arrival": 1711329883.5538225}, "models": {"yolo": {"arrival": 1711329883.2605462, "serving": 1711329883.541111}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4140592, "arrival": 1711329885.6970513}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4441557, "arrival": 1711329883.5961058}, "models": {"yolo": {"arrival": 1711329883.2572975, "serving": 1711329883.5618508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.413505, "arrival": 1711329880.3150184}, "models": {"yolo": {"arrival": 1711329879.9807422, "serving": 1711329880.2610657}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.433648, "arrival": 1711329883.2547336}, "models": {"yolo": {"arrival": 1711329882.9808147, "serving": 1711329883.2451627}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4435139, "arrival": 1711329883.5899045}, "models": {"yolo": {"arrival": 1711329883.2572975, "serving": 1711329883.5618508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4137979, "arrival": 1711329885.69665}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:33.598533427+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4135702}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4434385, "arrival": 1711329883.9187763}, "models": {"yolo": {"arrival": 1711329883.565928, "serving": 1711329883.8846238}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.437013, "arrival": 1711329883.2566361}, "models": {"yolo": {"arrival": 1711329882.9808147, "serving": 1711329883.2451627}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4136364, "arrival": 1711329883.2638545}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.150838895+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4441254}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.165014271+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4437768}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.609721199+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4443214}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.609653927+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.443839}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.609488588+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.437243}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.609699593+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4442008}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.131416785+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4136684}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4439352, "arrival": 1711329881.5599923}, "models": {"yolo": {"arrival": 1711329881.2423134, "serving": 1711329881.5352178}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4435394, "arrival": 1711329881.554659}, "models": {"yolo": {"arrival": 1711329881.2423134, "serving": 1711329881.5352178}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4372244, "arrival": 1711329885.7024643}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4432561, "arrival": 1711329881.2508924}, "models": {"yolo": {"arrival": 1711329880.8902347, "serving": 1711329881.2358506}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.150723377+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4433377}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.164824967+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4369378}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4444106, "arrival": 1711329881.8083093}, "models": {"yolo": {"arrival": 1711329881.5463626, "serving": 1711329881.7887275}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4433084, "arrival": 1711329883.9183173}, "models": {"yolo": {"arrival": 1711329883.565928, "serving": 1711329883.8846238}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.16492562+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4432225}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.150744748+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4434812}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.433515, "arrival": 1711329883.2832103}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4141233, "arrival": 1711329883.5872335}, "models": {"yolo": {"arrival": 1711329883.267031, "serving": 1711329883.558023}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.150521368+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4335997}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:33.609608419+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4435732}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4436657, "arrival": 1711329883.5903468}, "models": {"yolo": {"arrival": 1711329883.2572975, "serving": 1711329883.5618508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4374092, "arrival": 1711329883.5947077}, "models": {"yolo": {"arrival": 1711329883.267031, "serving": 1711329883.558023}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.164779333+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.414219}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4431891, "arrival": 1711329883.293578}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4143162, "arrival": 1711329885.7009406}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4444249, "arrival": 1711329885.715218}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.437355, "arrival": 1711329880.9012244}, "models": {"yolo": {"arrival": 1711329880.2707727, "serving": 1711329880.882098}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.164968202+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4434974}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.609439343+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.433708}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4433231, "arrival": 1711329883.2938025}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4431725, "arrival": 1711329883.5965154}, "models": {"yolo": {"arrival": 1711329883.267031, "serving": 1711329883.558023}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4370995, "arrival": 1711329883.5886903}, "models": {"yolo": {"arrival": 1711329883.267031, "serving": 1711329883.558023}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:33.609677541+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4440737}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.164752016+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4139628}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.164947297+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4433537}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4445174, "arrival": 1711329883.9200084}, "models": {"yolo": {"arrival": 1711329883.5739543, "serving": 1711329883.890282}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4435914, "arrival": 1711329883.919186}, "models": {"yolo": {"arrival": 1711329883.565928, "serving": 1711329883.8846238}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4445484, "arrival": 1711329885.7173524}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:33.60953451+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4431524}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4443529, "arrival": 1711329883.5582578}, "models": {"yolo": {"arrival": 1711329883.2605462, "serving": 1711329883.541111}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4372816, "arrival": 1711329883.2925794}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4433699, "arrival": 1711329883.589496}, "models": {"yolo": {"arrival": 1711329883.2572975, "serving": 1711329883.5618508}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4434645, "arrival": 1711329883.2940216}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4337301, "arrival": 1711329883.5882883}, "models": {"yolo": {"arrival": 1711329883.267031, "serving": 1711329883.558023}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4371307, "arrival": 1711329883.284021}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.150701296+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.443205}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.164847838+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4371717}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4372082, "arrival": 1711329880.9004657}, "models": {"yolo": {"arrival": 1711329880.2707727, "serving": 1711329880.882098}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4433854, "arrival": 1711329881.2516687}, "models": {"yolo": {"arrival": 1711329880.8902347, "serving": 1711329881.2358506}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4431334, "arrival": 1711329885.7079737}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.443681, "arrival": 1711329881.556536}, "models": {"yolo": {"arrival": 1711329881.2423134, "serving": 1711329881.5352178}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4435573, "arrival": 1711329885.7089465}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.150789302+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.443761}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.164803891+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4336257}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.44444, "arrival": 1711329874.6388137}, "models": {"yolo": {"arrival": 1711329874.4561203, "serving": 1711329874.6159256}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4434006, "arrival": 1711329885.7087152}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4430869, "arrival": 1711329883.2651923}, "models": {"yolo": {"arrival": 1711329882.9808147, "serving": 1711329883.2451627}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.150909027+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4444861}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4432395, "arrival": 1711329883.2654784}, "models": {"yolo": {"arrival": 1711329882.9808147, "serving": 1711329883.2451627}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.165124393+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.444502}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.150588371+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4337666}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.4442167, "arrival": 1711329883.9297764}, "models": {"yolo": {"arrival": 1711329883.565928, "serving": 1711329883.8846238}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4438088, "arrival": 1711329881.5591888}, "models": {"yolo": {"arrival": 1711329881.2423134, "serving": 1711329881.5352178}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.165080326+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4442627}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.443855, "arrival": 1711329883.9288907}, "models": {"yolo": {"arrival": 1711329883.565928, "serving": 1711329883.8846238}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.164992059+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329873.4436505}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.150642107+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4372995}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:33.609466373+00:00\"}\"\n>", "times": {"request": {"sending": 1711329873.4370775}}, "outputs": []}, {"times": {"request": {"sending": 1711329873.433748, "arrival": 1711329883.2836244}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.414381, "arrival": 1711329883.5877924}, "models": {"yolo": {"arrival": 1711329883.267031, "serving": 1711329883.558023}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4443376, "arrival": 1711329885.6581519}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4651043, "arrival": 1711329874.6397762}, "models": {"yolo": {"arrival": 1711329874.4561203, "serving": 1711329874.6159256}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4370568, "arrival": 1711329885.7021048}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4442322, "arrival": 1711329883.5569098}, "models": {"yolo": {"arrival": 1711329883.2605462, "serving": 1711329883.541111}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329873.4374263, "arrival": 1711329883.2933512}, "models": {"yolo": {"arrival": 1711329882.5593286, "serving": 1711329883.244285}}}, "model_name": "yolo", "outputs": []}], [{"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:35.507652274+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4430382}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4423685, "arrival": 1711329885.7276008}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4430075, "arrival": 1711329882.2981074}, "models": {"yolo": {"arrival": 1711329882.0689938, "serving": 1711329882.2838352}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4428692, "arrival": 1711329884.7236981}, "models": {"yolo": {"arrival": 1711329883.9018366, "serving": 1711329884.7038193}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.614844634+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4255319}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4657855, "arrival": 1711329885.714418}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4658072, "arrival": 1711329885.9385014}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4258912, "arrival": 1711329883.9285822}, "models": {"yolo": {"arrival": 1711329883.5518067, "serving": 1711329883.8939111}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.443325, "arrival": 1711329885.9140534}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.615063464+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4255035}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4419534, "arrival": 1711329885.726202}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4432352, "arrival": 1711329884.7271893}, "models": {"yolo": {"arrival": 1711329883.9018366, "serving": 1711329884.7038193}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:35.507504538+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4422526}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.443299, "arrival": 1711329885.710092}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4260175, "arrival": 1711329885.7183154}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.443055, "arrival": 1711329885.7096345}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4424183, "arrival": 1711329885.703788}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4253862, "arrival": 1711329885.6642377}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.665309182+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4436967}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4257936, "arrival": 1711329881.8148067}, "models": {"yolo": {"arrival": 1711329881.5463626, "serving": 1711329881.7887275}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4254704, "arrival": 1711329883.9279442}, "models": {"yolo": {"arrival": 1711329883.5518067, "serving": 1711329883.8939111}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4424822, "arrival": 1711329884.719485}, "models": {"yolo": {"arrival": 1711329883.9018366, "serving": 1711329884.7038193}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.665224547+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.443346}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.615009364+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4259439}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4422867, "arrival": 1711329884.5267513}, "models": {"yolo": {"arrival": 1711329883.9021597, "serving": 1711329884.5059261}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.665181616+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.443206}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4369462, "arrival": 1711329885.6934738}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:35.50754296+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4423852}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4420824, "arrival": 1711329882.0899115}, "models": {"yolo": {"arrival": 1711329881.801001, "serving": 1711329882.0589905}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.633746831+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4648204}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4261713, "arrival": 1711329883.929198}, "models": {"yolo": {"arrival": 1711329883.5739543, "serving": 1711329883.890282}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4391809, "arrival": 1711329882.0690944}, "models": {"yolo": {"arrival": 1711329881.801001, "serving": 1711329882.0589905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.441995, "arrival": 1711329885.7013748}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4430692, "arrival": 1711329885.9108605}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.425587, "arrival": 1711329881.809484}, "models": {"yolo": {"arrival": 1711329881.5463626, "serving": 1711329881.7887275}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.442014, "arrival": 1711329883.9343228}, "models": {"yolo": {"arrival": 1711329883.5518067, "serving": 1711329883.8939111}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4256124, "arrival": 1711329885.7176335}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4436498, "arrival": 1711329885.713948}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4429924, "arrival": 1711329884.724468}, "models": {"yolo": {"arrival": 1711329883.9018366, "serving": 1711329884.7038193}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:35.507679946+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4431603}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.442219, "arrival": 1711329882.0967913}, "models": {"yolo": {"arrival": 1711329881.801001, "serving": 1711329882.0589905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4256415, "arrival": 1711329874.6404629}, "models": {"yolo": {"arrival": 1711329874.4561203, "serving": 1711329874.6159256}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.443386, "arrival": 1711329885.0036912}, "models": {"yolo": {"arrival": 1711329884.7126179, "serving": 1711329884.9901967}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.633260334+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4391422}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4434903, "arrival": 1711329885.9363034}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4429462, "arrival": 1711329884.5295157}, "models": {"yolo": {"arrival": 1711329883.9021597, "serving": 1711329884.5059261}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.633631521+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4431}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.664822835+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.442168}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4421349, "arrival": 1711329885.7031364}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.633654896+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4432201}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.665137075+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.443085}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4633894, "arrival": 1711329885.9378483}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4256682, "arrival": 1711329885.6649632}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4435549, "arrival": 1711329885.0062654}, "models": {"yolo": {"arrival": 1711329884.7126179, "serving": 1711329884.9901967}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.425867, "arrival": 1711329885.6924884}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4391978, "arrival": 1711329885.725954}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.63333548+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4421852}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4369678, "arrival": 1711329883.933776}, "models": {"yolo": {"arrival": 1711329883.5518067, "serving": 1711329883.8939111}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4368033, "arrival": 1711329881.8161705}, "models": {"yolo": {"arrival": 1711329881.5463626, "serving": 1711329881.7887275}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4255602, "arrival": 1711329883.9203994}, "models": {"yolo": {"arrival": 1711329883.5739543, "serving": 1711329883.890282}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4421508, "arrival": 1711329884.5231245}, "models": {"yolo": {"arrival": 1711329883.9021597, "serving": 1711329884.5059261}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.614956523+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4257452}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.633719968+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4437208}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.442514, "arrival": 1711329885.728056}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4257693, "arrival": 1711329883.9215147}, "models": {"yolo": {"arrival": 1711329883.5739543, "serving": 1711329883.890282}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:35.514246859+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4436283}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.633191736+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.426147}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4368927, "arrival": 1711329885.7252939}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.615116577+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4257174}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4658904, "arrival": 1711329885.740122}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4258173, "arrival": 1711329885.718083}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.46333, "arrival": 1711329885.7392218}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4658246, "arrival": 1711329875.5718496}, "models": {"yolo": {"arrival": 1711329875.437559, "serving": 1711329875.560203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4422703, "arrival": 1711329885.7034616}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4437504, "arrival": 1711329885.0070622}, "models": {"yolo": {"arrival": 1711329884.7126179, "serving": 1711329884.9901967}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4658427, "arrival": 1711329875.5727596}, "models": {"yolo": {"arrival": 1711329875.4398026, "serving": 1711329875.5604513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4420998, "arrival": 1711329885.7264555}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.63331408+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4420478}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4424975, "arrival": 1711329882.1032722}, "models": {"yolo": {"arrival": 1711329881.801001, "serving": 1711329882.0589905}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.633427202+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4390786}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4392996, "arrival": 1711329883.9316883}, "models": {"yolo": {"arrival": 1711329883.5739543, "serving": 1711329883.890282}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.442564, "arrival": 1711329884.5282636}, "models": {"yolo": {"arrival": 1711329883.9021597, "serving": 1711329884.5059261}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.425693, "arrival": 1711329883.9282677}, "models": {"yolo": {"arrival": 1711329883.5518067, "serving": 1711329883.8939111}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.442548, "arrival": 1711329885.7041051}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.665092551+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4429617}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.633357504+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4423192}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.443146, "arrival": 1711329885.7342796}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.633382414+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.442467}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4428084, "arrival": 1711329885.7082188}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:35.514274961+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4633527}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4418929, "arrival": 1711329882.0889966}, "models": {"yolo": {"arrival": 1711329881.801001, "serving": 1711329882.0589905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.442236, "arrival": 1711329885.7266958}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.633676937+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4433663}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4431303, "arrival": 1711329882.3007548}, "models": {"yolo": {"arrival": 1711329882.0689938, "serving": 1711329882.2838352}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4432528, "arrival": 1711329882.301635}, "models": {"yolo": {"arrival": 1711329882.0689938, "serving": 1711329882.2838352}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.442824, "arrival": 1711329884.528899}, "models": {"yolo": {"arrival": 1711329883.9021597, "serving": 1711329884.5059261}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4428992, "arrival": 1711329885.7294214}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4429307, "arrival": 1711329885.709406}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.633697856+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4435334}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.633563829+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.442598}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4259932, "arrival": 1711329881.8156028}, "models": {"yolo": {"arrival": 1711329881.5463626, "serving": 1711329881.7887275}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4259686, "arrival": 1711329883.921849}, "models": {"yolo": {"arrival": 1711329883.5739543, "serving": 1711329883.890282}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.443267, "arrival": 1711329885.734569}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.665351408+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4634075}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4658592, "arrival": 1711329885.3343852}, "models": {"yolo": {"arrival": 1711329885.0001268, "serving": 1711329885.3204618}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4426453, "arrival": 1711329885.7291343}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4258416, "arrival": 1711329874.641111}, "models": {"yolo": {"arrival": 1711329874.4561203, "serving": 1711329874.6159256}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.63358623+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4428537}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.443175, "arrival": 1711329885.709858}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4260933, "arrival": 1711329883.9334683}, "models": {"yolo": {"arrival": 1711329883.5518067, "serving": 1711329883.8939111}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:35.507568334+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4425294}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4633715, "arrival": 1711329885.714178}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:35.507592881+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.442789}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.633292005+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4392822}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4423347, "arrival": 1711329884.7186453}, "models": {"yolo": {"arrival": 1711329883.9018366, "serving": 1711329884.7038193}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4632416, "arrival": 1711329882.3050416}, "models": {"yolo": {"arrival": 1711329882.0689938, "serving": 1711329882.2838352}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4436011, "arrival": 1711329885.7352417}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.615349451+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4259157}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4431152, "arrival": 1711329884.7262714}, "models": {"yolo": {"arrival": 1711329883.9018366, "serving": 1711329884.7038193}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4391634, "arrival": 1711329883.9313524}, "models": {"yolo": {"arrival": 1711329883.5739543, "serving": 1711329883.890282}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4260688, "arrival": 1711329885.692979}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.44319, "arrival": 1711329885.9131572}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4648943, "arrival": 1711329882.6350152}, "models": {"yolo": {"arrival": 1711329882.295004, "serving": 1711329882.6216426}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4426148, "arrival": 1711329884.7201521}, "models": {"yolo": {"arrival": 1711329883.9018366, "serving": 1711329884.7038193}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.615406803+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4261227}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:35.501258078+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.441975}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.66491467+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4423032}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.633608892+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 2.43 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4429772}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4422023, "arrival": 1711329884.7158628}, "models": {"yolo": {"arrival": 1711329883.9018366, "serving": 1711329884.7038193}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4658747, "arrival": 1711329882.6377373}, "models": {"yolo": {"arrival": 1711329882.295004, "serving": 1711329882.6216426}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4434266, "arrival": 1711329885.7348156}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4424348, "arrival": 1711329884.5276191}, "models": {"yolo": {"arrival": 1711329883.9021597, "serving": 1711329884.5059261}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.633521524+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4420314}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4657254, "arrival": 1711329885.7398512}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:35.514139357+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4432807}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4420652, "arrival": 1711329883.9319673}, "models": {"yolo": {"arrival": 1711329883.5739543, "serving": 1711329883.890282}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.633470314+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4392655}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:35.501087613+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4260428}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:35.501162148+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4369197}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.439248, "arrival": 1711329883.9340477}, "models": {"yolo": {"arrival": 1711329883.5518067, "serving": 1711329883.8939111}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4423513, "arrival": 1711329882.1026816}, "models": {"yolo": {"arrival": 1711329881.801001, "serving": 1711329882.0589905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.44347, "arrival": 1711329885.7129483}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4430232, "arrival": 1711329885.7296329}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.665049715+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4428387}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4434068, "arrival": 1711329882.3023205}, "models": {"yolo": {"arrival": 1711329882.0689938, "serving": 1711329882.2838352}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:35.501217722+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.439214}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4436731, "arrival": 1711329885.9371965}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:35.514217503+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4434488}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.439231, "arrival": 1711329885.693979}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:34.665267712+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4435127}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:35.514304283+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.465767}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.464875, "arrival": 1711329885.0077312}, "models": {"yolo": {"arrival": 1711329884.7126179, "serving": 1711329884.9901967}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:35.514326637+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4659078}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.664961533+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4424517}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:34.665007451+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.4425783}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:35.507619164+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329874.4429138}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4426303, "arrival": 1711329882.1038265}, "models": {"yolo": {"arrival": 1711329881.801001, "serving": 1711329882.0589905}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:35.507414737+00:00\"}\"\n>", "times": {"request": {"sending": 1711329874.442116}}, "outputs": []}, {"times": {"request": {"sending": 1711329874.4428847, "arrival": 1711329882.1043773}, "models": {"yolo": {"arrival": 1711329881.801001, "serving": 1711329882.0589905}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329874.4435751, "arrival": 1711329882.3042626}, "models": {"yolo": {"arrival": 1711329882.0689938, "serving": 1711329882.2838352}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329875.4117074, "arrival": 1711329885.7146888}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.381948999+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.431127}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.41191, "arrival": 1711329875.5734332}, "models": {"yolo": {"arrival": 1711329875.4398026, "serving": 1711329875.5604513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.439582, "arrival": 1711329885.461909}, "models": {"yolo": {"arrival": 1711329885.3323495, "serving": 1711329885.4481397}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4401696, "arrival": 1711329886.1289344}, "models": {"yolo": {"arrival": 1711329885.8532617, "serving": 1711329886.0889397}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4306061, "arrival": 1711329882.6445043}, "models": {"yolo": {"arrival": 1711329882.295004, "serving": 1711329882.6216426}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.501265841+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4125392}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4400759, "arrival": 1711329885.731959}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.382288143+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4408023}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4122653, "arrival": 1711329885.3373885}, "models": {"yolo": {"arrival": 1711329885.0001268, "serving": 1711329885.3204618}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4396193, "arrival": 1711329886.1207402}, "models": {"yolo": {"arrival": 1711329885.8532617, "serving": 1711329886.0889397}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4306757, "arrival": 1711329885.7256808}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4118133, "arrival": 1711329885.943694}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4398923, "arrival": 1711329882.9934502}, "models": {"yolo": {"arrival": 1711329882.6325028, "serving": 1711329882.956716}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.3819111+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4309697}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4281209, "arrival": 1711329885.9464204}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.411866, "arrival": 1711329875.5767167}, "models": {"yolo": {"arrival": 1711329875.437559, "serving": 1711329875.560203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4121134, "arrival": 1711329885.7178686}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.440279, "arrival": 1711329883.285214}, "models": {"yolo": {"arrival": 1711329882.9677424, "serving": 1711329883.2504315}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.521592111+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4307368}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.381708405+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4128973}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4120338, "arrival": 1711329885.7403889}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4279811, "arrival": 1711329885.8556447}, "models": {"yolo": {"arrival": 1711329885.673653, "serving": 1711329885.8428328}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4309132, "arrival": 1711329885.3478127}, "models": {"yolo": {"arrival": 1711329885.0001268, "serving": 1711329885.3204618}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4119935, "arrival": 1711329882.6386683}, "models": {"yolo": {"arrival": 1711329882.295004, "serving": 1711329882.6216426}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4410064, "arrival": 1711329885.674539}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4400098, "arrival": 1711329885.4701304}, "models": {"yolo": {"arrival": 1711329885.3323495, "serving": 1711329885.4481397}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.382172693+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4403112}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.501311469+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4130242}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4409597, "arrival": 1711329885.9619818}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.526343591+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4401226}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.381984171+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.439638}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4311087, "arrival": 1711329886.120186}, "models": {"yolo": {"arrival": 1711329885.8532617, "serving": 1711329886.0889397}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.501342773+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4307177}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.382358943+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4737113}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4397619, "arrival": 1711329886.1212883}, "models": {"yolo": {"arrival": 1711329885.8532617, "serving": 1711329886.0889397}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.501384016+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4394767}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4403958, "arrival": 1711329885.6635368}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4127498, "arrival": 1711329882.6430438}, "models": {"yolo": {"arrival": 1711329882.295004, "serving": 1711329882.6216426}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4398112, "arrival": 1711329885.958192}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.439944, "arrival": 1711329885.7316911}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4404256, "arrival": 1711329886.45864}, "models": {"yolo": {"arrival": 1711329886.1020095, "serving": 1711329886.4164464}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.505508397+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4409754}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4408176, "arrival": 1711329885.735709}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:35.514348754+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4120734}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4280999, "arrival": 1711329885.7189665}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4123044, "arrival": 1711329882.6394758}, "models": {"yolo": {"arrival": 1711329882.295004, "serving": 1711329882.6216426}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.501410953+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4398274}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.381787617+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4280722}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4759347, "arrival": 1711329883.5618818}, "models": {"yolo": {"arrival": 1711329883.2613337, "serving": 1711329883.5520303}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.411952, "arrival": 1711329885.336554}, "models": {"yolo": {"arrival": 1711329885.0001268, "serving": 1711329885.3204618}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4131446, "arrival": 1711329882.6438367}, "models": {"yolo": {"arrival": 1711329882.295004, "serving": 1711329882.6216426}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4305823, "arrival": 1711329885.34584}, "models": {"yolo": {"arrival": 1711329885.0001268, "serving": 1711329885.3204618}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4124203, "arrival": 1711329885.718536}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:35.514416119+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4123812}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4409146, "arrival": 1711329886.4642684}, "models": {"yolo": {"arrival": 1711329886.1020095, "serving": 1711329886.4164464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4308574, "arrival": 1711329885.9528356}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.526307681+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4398534}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4759157, "arrival": 1711329885.6751122}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4404771, "arrival": 1711329885.9611182}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.440898, "arrival": 1711329883.2945046}, "models": {"yolo": {"arrival": 1711329882.9677424, "serving": 1711329883.2504315}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.412482, "arrival": 1711329885.9451373}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.412152, "arrival": 1711329885.9444642}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4397953, "arrival": 1711329885.7313867}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.526271103+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4395597}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4737406, "arrival": 1711329885.7367141}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4396007, "arrival": 1711329882.9870641}, "models": {"yolo": {"arrival": 1711329882.6325028, "serving": 1711329882.956716}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.501424278+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4399757}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4396772, "arrival": 1711329885.9540648}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.526360638+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4402473}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4128366, "arrival": 1711329885.7433496}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.412343, "arrival": 1711329885.7406602}, "models": {"yolo": {"arrival": 1711329883.9340613, "serving": 1711329885.6364005}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.50135755+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4308755}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4122276, "arrival": 1711329875.5759869}, "models": {"yolo": {"arrival": 1711329875.4398026, "serving": 1711329875.5604513}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4121902, "arrival": 1711329875.5773382}, "models": {"yolo": {"arrival": 1711329875.437559, "serving": 1711329875.560203}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4306285, "arrival": 1711329885.8578978}, "models": {"yolo": {"arrival": 1711329885.673653, "serving": 1711329885.8428328}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4129844, "arrival": 1711329885.9457886}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4307566, "arrival": 1711329885.3465111}, "models": {"yolo": {"arrival": 1711329885.0001268, "serving": 1711329885.3204618}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.382437841+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4759674}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.501469133+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.440363}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.430838, "arrival": 1711329885.7269173}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4736204, "arrival": 1711329886.466302}, "models": {"yolo": {"arrival": 1711329886.1020095, "serving": 1711329886.4164464}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.501371428+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4310303}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4399605, "arrival": 1711329885.9585612}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4311671, "arrival": 1711329885.9536579}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4131052, "arrival": 1711329885.3450055}, "models": {"yolo": {"arrival": 1711329885.0001268, "serving": 1711329885.3204618}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.382324613+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.44093}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.526249816+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.431051}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.533393558+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4409904}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.430992, "arrival": 1711329885.7271304}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.533248655+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4403784}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.382062335+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4399252}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.501328438+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.428141}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.533373742+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4408672}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4410207, "arrival": 1711329883.5610359}, "models": {"yolo": {"arrival": 1711329883.2613337, "serving": 1711329883.5520303}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.501397666+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4396946}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.430776, "arrival": 1711329882.9687972}, "models": {"yolo": {"arrival": 1711329882.6325028, "serving": 1711329882.956716}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4406605, "arrival": 1711329886.4590564}, "models": {"yolo": {"arrival": 1711329886.1020095, "serving": 1711329886.4164464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.43107, "arrival": 1711329885.459691}, "models": {"yolo": {"arrival": 1711329885.3323495, "serving": 1711329885.4481397}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.431147, "arrival": 1711329885.7273417}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4306972, "arrival": 1711329885.9517984}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.38209728+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.440058}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.381874122+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4308152}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.382134143+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4401846}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.412665, "arrival": 1711329885.3420117}, "models": {"yolo": {"arrival": 1711329885.0001268, "serving": 1711329885.3204618}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.440201, "arrival": 1711329885.732193}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.439746, "arrival": 1711329882.9879458}, "models": {"yolo": {"arrival": 1711329882.6325028, "serving": 1711329882.956716}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.412942, "arrival": 1711329885.7187552}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4406416, "arrival": 1711329883.2928805}, "models": {"yolo": {"arrival": 1711329882.9677424, "serving": 1711329883.2504315}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.533351773+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4407415}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.440411, "arrival": 1711329883.2855337}, "models": {"yolo": {"arrival": 1711329882.9677424, "serving": 1711329883.2504315}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.533322217+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.440507}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4402156, "arrival": 1711329885.9591281}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4310117, "arrival": 1711329885.9532495}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.382024793+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4397776}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.440295, "arrival": 1711329886.4581826}, "models": {"yolo": {"arrival": 1711329886.1020095, "serving": 1711329886.4164464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4403467, "arrival": 1711329885.9607942}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.501454362+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4402313}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.505535253+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.475876}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.516229842+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.412604}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.521573416+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4305158}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4403312, "arrival": 1711329885.7324033}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4407864, "arrival": 1711329886.4594948}, "models": {"yolo": {"arrival": 1711329886.1020095, "serving": 1711329886.4164464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4401383, "arrival": 1711329885.470856}, "models": {"yolo": {"arrival": 1711329885.3323495, "serving": 1711329885.4481397}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4407086, "arrival": 1711329885.9614239}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.440042, "arrival": 1711329886.128384}, "models": {"yolo": {"arrival": 1711329885.8532617, "serving": 1711329886.0889397}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4408336, "arrival": 1711329885.9617012}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4399085, "arrival": 1711329886.1277304}, "models": {"yolo": {"arrival": 1711329885.8532617, "serving": 1711329886.0889397}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4398758, "arrival": 1711329885.4692092}, "models": {"yolo": {"arrival": 1711329885.3323495, "serving": 1711329885.4481397}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4309504, "arrival": 1711329886.119533}, "models": {"yolo": {"arrival": 1711329885.8532617, "serving": 1711329886.0889397}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.501497841+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4407268}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.521531663+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4130652}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.501438899+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4401073}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4758008, "arrival": 1711329885.963844}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4397287, "arrival": 1711329885.4627144}, "models": {"yolo": {"arrival": 1711329885.3323495, "serving": 1711329885.4481397}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.382253302+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4406776}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.505445406+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.44085}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4408827, "arrival": 1711329885.6736698}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.440264, "arrival": 1711329885.471484}, "models": {"yolo": {"arrival": 1711329885.3323495, "serving": 1711329885.4481397}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.526202437+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4308946}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.501484265+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4404924}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4400253, "arrival": 1711329883.2844255}, "models": {"yolo": {"arrival": 1711329882.9677424, "serving": 1711329883.2504315}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4409442, "arrival": 1711329885.7364373}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.381833859+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.430652}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.526289437+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4397118}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4406233, "arrival": 1711329885.6656158}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4406924, "arrival": 1711329885.7354667}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4309316, "arrival": 1711329882.9817812}, "models": {"yolo": {"arrival": 1711329882.6325028, "serving": 1711329882.956716}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4759514, "arrival": 1711329886.4692535}, "models": {"yolo": {"arrival": 1711329886.1020095, "serving": 1711329886.4164464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.440154, "arrival": 1711329883.2848322}, "models": {"yolo": {"arrival": 1711329882.9677424, "serving": 1711329883.2504315}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4407566, "arrival": 1711329885.6662304}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4407716, "arrival": 1711329883.2942407}, "models": {"yolo": {"arrival": 1711329882.9677424, "serving": 1711329883.2504315}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4307954, "arrival": 1711329885.858734}, "models": {"yolo": {"arrival": 1711329885.673653, "serving": 1711329885.8428328}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.533417327+00:00\"}\"\n>", "times": {"request": {"sending": 1711329875.4758978}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.440092, "arrival": 1711329885.9588444}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.526326818+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.439992}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4396596, "arrival": 1711329885.7278411}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329875.4310892, "arrival": 1711329882.9860344}, "models": {"yolo": {"arrival": 1711329882.6325028, "serving": 1711329882.956716}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.382207879+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 16.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329875.4404404}}, "outputs": []}, {"times": {"request": {"sending": 1711329875.4404557, "arrival": 1711329885.735031}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}], [{"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.043742097+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4102058}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.409742, "arrival": 1711329886.4696283}, "models": {"yolo": {"arrival": 1711329886.1020095, "serving": 1711329886.4164464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4094617, "arrival": 1711329885.739573}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.303081963+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.433071}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.053489976+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4679217}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4243386, "arrival": 1711329886.700944}, "models": {"yolo": {"arrival": 1711329886.427636, "serving": 1711329886.6567686}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.424484, "arrival": 1711329886.7072883}, "models": {"yolo": {"arrival": 1711329886.427636, "serving": 1711329886.6567686}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4244661, "arrival": 1711329883.912556}, "models": {"yolo": {"arrival": 1711329883.5635133, "serving": 1711329883.8795137}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4330862, "arrival": 1711329886.1040463}, "models": {"yolo": {"arrival": 1711329885.923815, "serving": 1711329886.0880368}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.610352879+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4097807}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.793936949+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4321709}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.05344718+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4331744}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4324498, "arrival": 1711329885.9697556}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.303036571+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4328306}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.533438073+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4096248}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4098601, "arrival": 1711329885.9644558}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.79386777+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4245038}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.050357265+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4327147}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4101303, "arrival": 1711329885.7418585}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.505581274+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4098988}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.303124949+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4679434}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4678848, "arrival": 1711329886.4564087}, "models": {"yolo": {"arrival": 1711329886.1060982, "serving": 1711329886.4116585}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.050319501+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.432593}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.432249, "arrival": 1711329885.9486952}, "models": {"yolo": {"arrival": 1711329885.6536586, "serving": 1711329885.914711}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.29621599+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4319704}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4323118, "arrival": 1711329886.1245294}, "models": {"yolo": {"arrival": 1711329885.8620853, "serving": 1711329886.0941093}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.296238099+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.432106}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.64 GiB total capacity; 2.41 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.536613098+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4099374}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4331448, "arrival": 1711329886.1335516}, "models": {"yolo": {"arrival": 1711329885.8620853, "serving": 1711329886.0941093}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.410652, "arrival": 1711329883.572027}, "models": {"yolo": {"arrival": 1711329883.2613337, "serving": 1711329883.5520303}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.049987378+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4242587}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4326377, "arrival": 1711329884.5796585}, "models": {"yolo": {"arrival": 1711329883.8901043, "serving": 1711329884.5620673}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.40954, "arrival": 1711329885.9641733}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4777625, "arrival": 1711329877.8514318}, "models": {"yolo": {"arrival": 1711329877.448739, "serving": 1711329877.8335843}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 326.00 MiB (GPU 0; 23.64 GiB total capacity; 1.75 GiB already allocated; 18.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.505559467+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4095838}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4323277, "arrival": 1711329885.9695482}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.424376, "arrival": 1711329885.8712206}, "models": {"yolo": {"arrival": 1711329885.6654189, "serving": 1711329885.8505056}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4328475, "arrival": 1711329885.9577925}, "models": {"yolo": {"arrival": 1711329885.6536586, "serving": 1711329885.914711}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.288274359+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4105737}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.424153, "arrival": 1711329885.7438362}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4746063, "arrival": 1711329884.9167235}, "models": {"yolo": {"arrival": 1711329884.5699725, "serving": 1711329884.8999894}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4679782, "arrival": 1711329884.9159493}, "models": {"yolo": {"arrival": 1711329884.5699725, "serving": 1711329884.8999894}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.410014, "arrival": 1711329883.5709486}, "models": {"yolo": {"arrival": 1711329883.2613337, "serving": 1711329883.5520303}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.433101, "arrival": 1711329884.5845158}, "models": {"yolo": {"arrival": 1711329883.8901043, "serving": 1711329884.5620673}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4330268, "arrival": 1711329886.1327777}, "models": {"yolo": {"arrival": 1711329885.8620853, "serving": 1711329886.0941093}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.796766865+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.432668}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4100525, "arrival": 1711329886.469923}, "models": {"yolo": {"arrival": 1711329886.1020095, "serving": 1711329886.4164464}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4328792, "arrival": 1711329887.0006275}, "models": {"yolo": {"arrival": 1711329886.6677008, "serving": 1711329886.9602368}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4319894, "arrival": 1711329885.9476588}, "models": {"yolo": {"arrival": 1711329885.6536586, "serving": 1711329885.914711}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4320054, "arrival": 1711329883.91736}, "models": {"yolo": {"arrival": 1711329883.5635133, "serving": 1711329883.8795137}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.424522, "arrival": 1711329885.8737679}, "models": {"yolo": {"arrival": 1711329885.6654189, "serving": 1711329885.8505056}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.468029, "arrival": 1711329886.4568806}, "models": {"yolo": {"arrival": 1711329886.1060982, "serving": 1711329886.4116585}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4330115, "arrival": 1711329877.8485677}, "models": {"yolo": {"arrival": 1711329877.448739, "serving": 1711329877.8335843}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4322023, "arrival": 1711329885.9693387}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4323878, "arrival": 1711329884.5781589}, "models": {"yolo": {"arrival": 1711329883.8901043, "serving": 1711329884.5620673}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4679604, "arrival": 1711329886.1133547}, "models": {"yolo": {"arrival": 1711329885.923815, "serving": 1711329886.0880368}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.409664, "arrival": 1711329885.6756477}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.786715291+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4209058}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4331594, "arrival": 1711329885.9732883}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4103222, "arrival": 1711329883.571494}, "models": {"yolo": {"arrival": 1711329883.2613337, "serving": 1711329883.5520303}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.296169748+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4244301}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4318705, "arrival": 1711329885.9676797}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.296192843+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.424576}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4677489, "arrival": 1711329884.912804}, "models": {"yolo": {"arrival": 1711329884.5699725, "serving": 1711329884.8999894}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4097037, "arrival": 1711329883.5632486}, "models": {"yolo": {"arrival": 1711329883.2613337, "serving": 1711329883.5520303}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.288203484+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4102447}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.796809782+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4328945}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.424595, "arrival": 1711329885.9470518}, "models": {"yolo": {"arrival": 1711329885.6536586, "serving": 1711329885.914711}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4246655, "arrival": 1711329885.8745382}, "models": {"yolo": {"arrival": 1711329885.6654189, "serving": 1711329885.8505056}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4321222, "arrival": 1711329885.948292}, "models": {"yolo": {"arrival": 1711329885.6536586, "serving": 1711329885.914711}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4329233, "arrival": 1711329885.9728427}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.432623, "arrival": 1711329885.95739}, "models": {"yolo": {"arrival": 1711329885.6536586, "serving": 1711329885.914711}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.477633, "arrival": 1711329887.2424572}, "models": {"yolo": {"arrival": 1711329886.9702418, "serving": 1711329887.2055912}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4101682, "arrival": 1711329885.9647396}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.302989367+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4324827}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.043806419+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.410535}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4246297, "arrival": 1711329886.7083}, "models": {"yolo": {"arrival": 1711329886.427636, "serving": 1711329886.6567686}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.303103949+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4331894}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4678395, "arrival": 1711329887.2415714}, "models": {"yolo": {"arrival": 1711329886.9702418, "serving": 1711329887.2055912}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.432057, "arrival": 1711329885.8751857}, "models": {"yolo": {"arrival": 1711329885.6654189, "serving": 1711329885.8505056}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4098215, "arrival": 1711329885.7409346}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4102833, "arrival": 1711329885.690322}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4210956, "arrival": 1711329883.910601}, "models": {"yolo": {"arrival": 1711329883.5635133, "serving": 1711329883.8795137}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4680128, "arrival": 1711329877.8510075}, "models": {"yolo": {"arrival": 1711329877.448739, "serving": 1711329877.8335843}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.050168893+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4320903}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.050205754+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.432218}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.410496, "arrival": 1711329885.9650161}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.432514, "arrival": 1711329884.5789437}, "models": {"yolo": {"arrival": 1711329883.8901043, "serving": 1711329884.5620673}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.793914299+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.432041}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4242342, "arrival": 1711329885.9668512}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.424612, "arrival": 1711329883.9130368}, "models": {"yolo": {"arrival": 1711329883.5635133, "serving": 1711329883.8795137}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4106905, "arrival": 1711329886.6999066}, "models": {"yolo": {"arrival": 1711329886.427636, "serving": 1711329886.6567686}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.793892542+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4246485}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4099758, "arrival": 1711329885.6890347}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.786633398+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4104156}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4330416, "arrival": 1711329885.973062}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4329677, "arrival": 1711329886.1018972}, "models": {"yolo": {"arrival": 1711329885.923815, "serving": 1711329886.0880368}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4328637, "arrival": 1711329884.5826063}, "models": {"yolo": {"arrival": 1711329883.8901043, "serving": 1711329884.5620673}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.432138, "arrival": 1711329883.9178834}, "models": {"yolo": {"arrival": 1711329883.5635133, "serving": 1711329883.8795137}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.043833326+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4210367}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4329967, "arrival": 1711329887.01208}, "models": {"yolo": {"arrival": 1711329886.6677008, "serving": 1711329886.9602368}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.432434, "arrival": 1711329886.125094}, "models": {"yolo": {"arrival": 1711329885.8620853, "serving": 1711329886.0941093}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4320734, "arrival": 1711329885.9690502}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.610471163+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4100912}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.796618831+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4322968}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4104567, "arrival": 1711329885.7422092}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.432653, "arrival": 1711329886.9999535}, "models": {"yolo": {"arrival": 1711329886.6677008, "serving": 1711329886.9602368}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4103618, "arrival": 1711329886.6993344}, "models": {"yolo": {"arrival": 1711329886.427636, "serving": 1711329886.6567686}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4320223, "arrival": 1711329886.9810214}, "models": {"yolo": {"arrival": 1711329886.6677008, "serving": 1711329886.9602368}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.050130262+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4319482}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.432683, "arrival": 1711329886.1314278}, "models": {"yolo": {"arrival": 1711329885.8620853, "serving": 1711329886.0941093}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.303146289+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4745698}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.050090018+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4245584}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4321866, "arrival": 1711329886.1238518}, "models": {"yolo": {"arrival": 1711329885.8620853, "serving": 1711329886.0941093}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4678643, "arrival": 1711329877.8493621}, "models": {"yolo": {"arrival": 1711329877.448739, "serving": 1711329877.8335843}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4325597, "arrival": 1711329886.1256466}, "models": {"yolo": {"arrival": 1711329885.8620853, "serving": 1711329886.0941093}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.303014454+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4326074}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.424394, "arrival": 1711329885.967125}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4325778, "arrival": 1711329885.9723353}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4332063, "arrival": 1711329886.1049137}, "models": {"yolo": {"arrival": 1711329885.923815, "serving": 1711329886.0880368}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.793840158+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4243581}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4209905, "arrival": 1711329885.7436173}, "models": {"yolo": {"arrival": 1711329883.942869, "serving": 1711329885.6360707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.421077, "arrival": 1711329885.6914408}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.050049102+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.424412}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4243004, "arrival": 1711329885.691971}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4324992, "arrival": 1711329885.9568994}, "models": {"yolo": {"arrival": 1711329885.6536586, "serving": 1711329885.914711}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4243195, "arrival": 1711329883.9110768}, "models": {"yolo": {"arrival": 1711329883.5635133, "serving": 1711329883.8795137}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4679034, "arrival": 1711329885.974614}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.796722307+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4325442}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.302908615+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4322338}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4329088, "arrival": 1711329886.132083}, "models": {"yolo": {"arrival": 1711329885.8620853, "serving": 1711329886.0941093}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:36.793786412+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4211357}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4679952, "arrival": 1711329887.2419987}, "models": {"yolo": {"arrival": 1711329886.9702418, "serving": 1711329887.2055912}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.303058933+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4329534}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4106126, "arrival": 1711329885.69089}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.053400802+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.433056}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.432373, "arrival": 1711329885.9523578}, "models": {"yolo": {"arrival": 1711329885.6536586, "serving": 1711329885.914711}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4244478, "arrival": 1711329885.7076564}, "models": {"yolo": {"arrival": 1711329885.4610713, "serving": 1711329885.6397882}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4322648, "arrival": 1711329884.575515}, "models": {"yolo": {"arrival": 1711329883.8901043, "serving": 1711329884.5620673}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.424539, "arrival": 1711329885.9673946}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.302963303+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4323578}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.296081136+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4210577}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4331305, "arrival": 1711329877.8489769}, "models": {"yolo": {"arrival": 1711329877.448739, "serving": 1711329877.8335843}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.050243508+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4323428}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 23.64 GiB total capacity; 2.46 GiB already allocated; 20.44 MiB free; 2.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:36.796678251+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.4324193}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.432698, "arrival": 1711329885.972598}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4322803, "arrival": 1711329886.982311}, "models": {"yolo": {"arrival": 1711329886.6677008, "serving": 1711329886.9602368}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.432153, "arrival": 1711329886.9816382}, "models": {"yolo": {"arrival": 1711329886.6677008, "serving": 1711329886.9602368}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.053335358+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.432938}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.474462, "arrival": 1711329885.9748669}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4745877, "arrival": 1711329886.11797}, "models": {"yolo": {"arrival": 1711329885.923815, "serving": 1711329886.0880368}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4210153, "arrival": 1711329885.9665353}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4331157, "arrival": 1711329887.24115}, "models": {"yolo": {"arrival": 1711329886.9702418, "serving": 1711329887.2055912}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.050282665+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4324663}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4325294, "arrival": 1711329886.9992273}, "models": {"yolo": {"arrival": 1711329886.6677008, "serving": 1711329886.9602368}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.80 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.053532972+00:00\"}\"\n>", "times": {"request": {"sending": 1711329876.474543}}, "outputs": []}, {"times": {"request": {"sending": 1711329876.4329822, "arrival": 1711329884.583293}, "models": {"yolo": {"arrival": 1711329883.8901043, "serving": 1711329884.5620673}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.4211154, "arrival": 1711329886.700434}, "models": {"yolo": {"arrival": 1711329886.427636, "serving": 1711329886.6567686}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329876.432403, "arrival": 1711329886.9982991}, "models": {"yolo": {"arrival": 1711329886.6677008, "serving": 1711329886.9602368}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.296139421+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.45 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329876.4242797}}, "outputs": []}], [{"times": {"request": {"sending": 1711329877.4146776, "arrival": 1711329887.249581}, "models": {"yolo": {"arrival": 1711329886.9702418, "serving": 1711329887.2055912}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4151156, "arrival": 1711329886.4654474}, "models": {"yolo": {"arrival": 1711329886.1060982, "serving": 1711329886.4116585}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4144673, "arrival": 1711329886.4573095}, "models": {"yolo": {"arrival": 1711329886.1060982, "serving": 1711329886.4116585}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4375918, "arrival": 1711329887.6797237}, "models": {"yolo": {"arrival": 1711329887.4481108, "serving": 1711329887.6640031}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4291825, "arrival": 1711329886.4494991}, "models": {"yolo": {"arrival": 1711329886.0973375, "serving": 1711329886.4101894}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4348884, "arrival": 1711329887.4713156}, "models": {"yolo": {"arrival": 1711329887.2178755, "serving": 1711329887.4368525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.43794, "arrival": 1711329887.0046654}, "models": {"yolo": {"arrival": 1711329886.6654196, "serving": 1711329886.9603753}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.408811849+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.438329}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.488178882+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.456284}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4145417, "arrival": 1711329885.9750881}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.414823, "arrival": 1711329886.4444585}, "models": {"yolo": {"arrival": 1711329886.0973375, "serving": 1711329886.4101894}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.43762, "arrival": 1711329886.7015207}, "models": {"yolo": {"arrival": 1711329886.421701, "serving": 1711329886.6552758}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4347196, "arrival": 1711329886.4513927}, "models": {"yolo": {"arrival": 1711329886.0973375, "serving": 1711329886.4101894}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4382482, "arrival": 1711329879.2587736}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.641185187+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4348412}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4348073, "arrival": 1711329885.9811933}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.392147141+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4295535}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4293103, "arrival": 1711329885.9783554}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4376733, "arrival": 1711329886.6875243}, "models": {"yolo": {"arrival": 1711329886.4203646, "serving": 1711329886.6551952}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.415017, "arrival": 1711329886.4454215}, "models": {"yolo": {"arrival": 1711329886.0973375, "serving": 1711329886.4101894}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.429434, "arrival": 1711329879.2486196}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4349043, "arrival": 1711329879.2550883}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.644969785+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4377656}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4381018, "arrival": 1711329886.9739273}, "models": {"yolo": {"arrival": 1711329886.6644616, "serving": 1711329886.9545953}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4562647, "arrival": 1711329886.465156}, "models": {"yolo": {"arrival": 1711329886.0842993, "serving": 1711329886.422888}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.408723952+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4379003}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4148948, "arrival": 1711329877.852149}, "models": {"yolo": {"arrival": 1711329877.448739, "serving": 1711329877.8335843}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.619345523+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4145744}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.634010043+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4146013}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.408618917+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4374669}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4381669, "arrival": 1711329886.4645653}, "models": {"yolo": {"arrival": 1711329886.0842993, "serving": 1711329886.422888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4150417, "arrival": 1711329885.208943}, "models": {"yolo": {"arrival": 1711329884.9048853, "serving": 1711329885.1926758}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.427163, "arrival": 1711329879.2055244}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.437699, "arrival": 1711329887.6810896}, "models": {"yolo": {"arrival": 1711329887.4481108, "serving": 1711329887.6640031}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4383423, "arrival": 1711329887.9331596}, "models": {"yolo": {"arrival": 1711329887.676527, "serving": 1711329887.9052076}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.415139, "arrival": 1711329885.9778922}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.619418136+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4147754}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4377794, "arrival": 1711329886.6882424}, "models": {"yolo": {"arrival": 1711329886.4203646, "serving": 1711329886.6551952}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.488136769+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4381797}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4376326, "arrival": 1711329885.9819255}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4147513, "arrival": 1711329885.975888}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.629091721+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.434825}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.438262, "arrival": 1711329887.0133939}, "models": {"yolo": {"arrival": 1711329886.6654196, "serving": 1711329886.9603753}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4377122, "arrival": 1711329879.257383}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.493771481+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4380882}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.414945, "arrival": 1711329885.9761248}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4593406, "arrival": 1711329879.2593064}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4148707, "arrival": 1711329887.2516701}, "models": {"yolo": {"arrival": 1711329886.9702418, "serving": 1711329887.2055912}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4152331, "arrival": 1711329885.209819}, "models": {"yolo": {"arrival": 1711329884.9048853, "serving": 1711329885.1926758}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.641161321+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4346898}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.636836701+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4147995}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.45624, "arrival": 1711329887.0138643}, "models": {"yolo": {"arrival": 1711329886.6654196, "serving": 1711329886.9603753}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4382348, "arrival": 1711329887.9324963}, "models": {"yolo": {"arrival": 1711329887.676527, "serving": 1711329887.9052076}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4349377, "arrival": 1711329885.9814646}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.619444628+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.414968}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4147015, "arrival": 1711329877.8517954}, "models": {"yolo": {"arrival": 1711329877.448739, "serving": 1711329877.8335843}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.40867417+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.437686}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4593606, "arrival": 1711329887.01427}, "models": {"yolo": {"arrival": 1711329886.6654196, "serving": 1711329886.9603753}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4146528, "arrival": 1711329884.917388}, "models": {"yolo": {"arrival": 1711329884.5699725, "serving": 1711329884.8999894}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.629160356+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4376452}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4380333, "arrival": 1711329879.2582238}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4146276, "arrival": 1711329886.1188562}, "models": {"yolo": {"arrival": 1711329885.923815, "serving": 1711329886.0880368}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.438154, "arrival": 1711329887.0125878}, "models": {"yolo": {"arrival": 1711329886.6654196, "serving": 1711329886.9603753}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.629042666+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4294944}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.628939352+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4290822}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4378195, "arrival": 1711329879.257681}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.434626, "arrival": 1711329885.9793513}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4147277, "arrival": 1711329886.457729}, "models": {"yolo": {"arrival": 1711329886.1060982, "serving": 1711329886.4116585}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.619471842+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4151626}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.408644526+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4375787}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.427217, "arrival": 1711329885.9781392}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.408768878+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4381156}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4149203, "arrival": 1711329886.463967}, "models": {"yolo": {"arrival": 1711329886.1060982, "serving": 1711329886.4116585}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.493916503+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4594128}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.437525, "arrival": 1711329885.9816966}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.414847, "arrival": 1711329885.206261}, "models": {"yolo": {"arrival": 1711329884.9048853, "serving": 1711329885.1926758}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4295743, "arrival": 1711329887.469941}, "models": {"yolo": {"arrival": 1711329887.2178755, "serving": 1711329887.4368525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4271932, "arrival": 1711329886.4657345}, "models": {"yolo": {"arrival": 1711329886.1060982, "serving": 1711329886.4116585}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4378064, "arrival": 1711329887.7014}, "models": {"yolo": {"arrival": 1711329887.4481108, "serving": 1711329887.6640031}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4150903, "arrival": 1711329879.2045698}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4593189, "arrival": 1711329887.933834}, "models": {"yolo": {"arrival": 1711329887.676527, "serving": 1711329887.9052076}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4270713, "arrival": 1711329887.449217}, "models": {"yolo": {"arrival": 1711329887.2178755, "serving": 1711329887.4368525}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4375658, "arrival": 1711329886.6856904}, "models": {"yolo": {"arrival": 1711329886.4203646, "serving": 1711329886.6551952}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.456152, "arrival": 1711329879.2590396}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4375117, "arrival": 1711329886.6985364}, "models": {"yolo": {"arrival": 1711329886.421701, "serving": 1711329886.6552758}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.392054099+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.429393}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4380484, "arrival": 1711329887.0052712}, "models": {"yolo": {"arrival": 1711329886.6654196, "serving": 1711329886.9603753}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.644866603+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4374335}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.434856, "arrival": 1711329886.4558308}, "models": {"yolo": {"arrival": 1711329886.0973375, "serving": 1711329886.4101894}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4377272, "arrival": 1711329887.003426}, "models": {"yolo": {"arrival": 1711329886.6654196, "serving": 1711329886.9603753}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4379933, "arrival": 1711329886.7051332}, "models": {"yolo": {"arrival": 1711329886.4203646, "serving": 1711329886.6551952}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.641115721+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4293532}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4150665, "arrival": 1711329887.4480069}, "models": {"yolo": {"arrival": 1711329887.2178755, "serving": 1711329887.4368525}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.408534343+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4347377}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4374819, "arrival": 1711329887.678709}, "models": {"yolo": {"arrival": 1711329887.4481108, "serving": 1711329887.6640031}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.62918238+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4377522}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4630084, "arrival": 1711329887.0014098}, "models": {"yolo": {"arrival": 1711329886.6644616, "serving": 1711329886.9545953}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.488112309+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4380744}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4379535, "arrival": 1711329886.0887146}, "models": {"yolo": {"arrival": 1711329885.9270782, "serving": 1711329886.073496}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.493848501+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.438302}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4379256, "arrival": 1711329879.2579582}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.408590991+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4348724}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.408701851+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4377928}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4382064, "arrival": 1711329886.9788492}, "models": {"yolo": {"arrival": 1711329886.6644616, "serving": 1711329886.9545953}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.408746144+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4380069}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.641137966+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4295151}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.64105992+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.415186}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4295924, "arrival": 1711329879.2489965}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4293737, "arrival": 1711329886.4501293}, "models": {"yolo": {"arrival": 1711329886.0973375, "serving": 1711329886.4101894}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4292061, "arrival": 1711329885.2105296}, "models": {"yolo": {"arrival": 1711329884.9048853, "serving": 1711329885.1926758}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.493732655+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4379797}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.493882974+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4563038}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.434772, "arrival": 1711329879.249368}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4152093, "arrival": 1711329886.4487462}, "models": {"yolo": {"arrival": 1711329886.0973375, "serving": 1711329886.4101894}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.640999224+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4149926}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.459379, "arrival": 1711329886.4702146}, "models": {"yolo": {"arrival": 1711329886.0842993, "serving": 1711329886.422888}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.641090507+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4291556}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4374964, "arrival": 1711329879.2565415}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.488200748+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4593961}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.488016802+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4378624}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4295342, "arrival": 1711329886.45079}, "models": {"yolo": {"arrival": 1711329886.0973375, "serving": 1711329886.4101894}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4294133, "arrival": 1711329887.4567246}, "models": {"yolo": {"arrival": 1711329887.2178755, "serving": 1711329887.4368525}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.408790509+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.438222}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.644943514+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.437659}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.438275, "arrival": 1711329886.464864}, "models": {"yolo": {"arrival": 1711329886.0842993, "serving": 1711329886.422888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4292831, "arrival": 1711329886.466019}, "models": {"yolo": {"arrival": 1711329886.1060982, "serving": 1711329886.4116585}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4381285, "arrival": 1711329887.7033637}, "models": {"yolo": {"arrival": 1711329887.4481108, "serving": 1711329887.6640031}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.434756, "arrival": 1711329887.4706872}, "models": {"yolo": {"arrival": 1711329887.2178755, "serving": 1711329887.4368525}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.64 GiB total capacity; 2.44 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.644918027+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.437552}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.488079776+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4379666}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4374514, "arrival": 1711329886.6850567}, "models": {"yolo": {"arrival": 1711329886.4203646, "serving": 1711329886.6551952}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4378352, "arrival": 1711329887.0040636}, "models": {"yolo": {"arrival": 1711329886.6654196, "serving": 1711329886.9603753}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.629005539+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4293315}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.408855491+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4631107}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4294753, "arrival": 1711329885.979088}, "models": {"yolo": {"arrival": 1711329884.553596, "serving": 1711329885.894614}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.43774, "arrival": 1711329886.0853755}, "models": {"yolo": {"arrival": 1711329885.9270782, "serving": 1711329886.073496}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.629138621+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4375389}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4349222, "arrival": 1711329886.6844053}, "models": {"yolo": {"arrival": 1711329886.421701, "serving": 1711329886.6552758}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.429228, "arrival": 1711329887.4536004}, "models": {"yolo": {"arrival": 1711329887.2178755, "serving": 1711329887.4368525}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.408833867+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4592357}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4631395, "arrival": 1711329887.9344885}, "models": {"yolo": {"arrival": 1711329887.676527, "serving": 1711329887.9052076}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4345438, "arrival": 1711329886.6831026}, "models": {"yolo": {"arrival": 1711329886.421701, "serving": 1711329886.6552758}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4563236, "arrival": 1711329886.9803903}, "models": {"yolo": {"arrival": 1711329886.6644616, "serving": 1711329886.9545953}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:37.629069175+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4346585}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4347904, "arrival": 1711329886.6837642}, "models": {"yolo": {"arrival": 1711329886.421701, "serving": 1711329886.6552758}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4379132, "arrival": 1711329887.7020757}, "models": {"yolo": {"arrival": 1711329887.4481108, "serving": 1711329887.6640031}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4378498, "arrival": 1711329886.0877314}, "models": {"yolo": {"arrival": 1711329885.9270782, "serving": 1711329886.073496}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.493666688+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4378753}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.437888, "arrival": 1711329886.7044203}, "models": {"yolo": {"arrival": 1711329886.4203646, "serving": 1711329886.6551952}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.488158043+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4382882}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:37.629117024+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329877.4373827}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4376054, "arrival": 1711329879.2571135}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4381409, "arrival": 1711329879.2585044}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4292493, "arrival": 1711329879.206243}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4294558, "arrival": 1711329886.682132}, "models": {"yolo": {"arrival": 1711329886.421701, "serving": 1711329886.6552758}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4380608, "arrival": 1711329886.4635828}, "models": {"yolo": {"arrival": 1711329886.0842993, "serving": 1711329886.422888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.4383152, "arrival": 1711329886.9797378}, "models": {"yolo": {"arrival": 1711329886.6644616, "serving": 1711329886.9545953}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329877.43802, "arrival": 1711329887.7027476}, "models": {"yolo": {"arrival": 1711329887.4481108, "serving": 1711329887.6640031}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.493808391+00:00\"}\"\n>", "times": {"request": {"sending": 1711329877.4381928}}, "outputs": []}, {"times": {"request": {"sending": 1711329877.4631646, "arrival": 1711329879.2595685}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329878.4302564, "arrival": 1711329886.4717407}, "models": {"yolo": {"arrival": 1711329886.0842993, "serving": 1711329886.422888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.412739, "arrival": 1711329887.2501256}, "models": {"yolo": {"arrival": 1711329886.9746883, "serving": 1711329887.20963}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.48825932+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4131792}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.423598198+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4399316}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4133177, "arrival": 1711329887.9368658}, "models": {"yolo": {"arrival": 1711329887.676527, "serving": 1711329887.9052076}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.779125949+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4137661}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4325552, "arrival": 1711329887.253374}, "models": {"yolo": {"arrival": 1711329886.9746883, "serving": 1711329887.20963}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.793885647+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4394517}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4398232, "arrival": 1711329880.3192434}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4405255, "arrival": 1711329887.7077785}, "models": {"yolo": {"arrival": 1711329887.4509058, "serving": 1711329887.6679254}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4130726, "arrival": 1711329879.2598553}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.421598248+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.439787}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.440544, "arrival": 1711329887.015054}, "models": {"yolo": {"arrival": 1711329886.672621, "serving": 1711329886.9620326}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4129648, "arrival": 1711329887.0020888}, "models": {"yolo": {"arrival": 1711329886.6644616, "serving": 1711329886.9545953}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4750524, "arrival": 1711329880.324158}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4128354, "arrival": 1711329886.470549}, "models": {"yolo": {"arrival": 1711329886.0842993, "serving": 1711329886.422888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.440344, "arrival": 1711329887.690866}, "models": {"yolo": {"arrival": 1711329887.4537232, "serving": 1711329887.6651714}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.45812296+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4749942}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.686652083+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4325955}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.779060945+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4134886}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.794087097+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.440872}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4745944, "arrival": 1711329887.6985688}, "models": {"yolo": {"arrival": 1711329887.4537232, "serving": 1711329887.6651714}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4324734, "arrival": 1711329887.2397864}, "models": {"yolo": {"arrival": 1711329886.9631321, "serving": 1711329887.2050898}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4136968, "arrival": 1711329886.4714303}, "models": {"yolo": {"arrival": 1711329886.0842993, "serving": 1711329886.422888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4131098, "arrival": 1711329887.2505379}, "models": {"yolo": {"arrival": 1711329886.9746883, "serving": 1711329887.20963}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.408956274+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.413836}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4324126, "arrival": 1711329886.7021117}, "models": {"yolo": {"arrival": 1711329886.436055, "serving": 1711329886.662524}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4131446, "arrival": 1711329886.4708467}, "models": {"yolo": {"arrival": 1711329886.0842993, "serving": 1711329886.422888}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.686576189+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4137318}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4136617, "arrival": 1711329887.2512937}, "models": {"yolo": {"arrival": 1711329886.9746883, "serving": 1711329887.20963}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.408905004+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.413283}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.686603413+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4302938}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4398053, "arrival": 1711329888.3912482}, "models": {"yolo": {"arrival": 1711329888.1581888, "serving": 1711329888.3772707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4393747, "arrival": 1711329880.3171115}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.687937739+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4394329}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4400945, "arrival": 1711329888.394729}, "models": {"yolo": {"arrival": 1711329888.1581888, "serving": 1711329888.3772707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.793954362+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4398944}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.692951851+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4408543}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4405067, "arrival": 1711329880.3219595}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 1.83 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.488228357+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.412885}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.439396, "arrival": 1711329887.475347}, "models": {"yolo": {"arrival": 1711329887.2196586, "serving": 1711329887.440916}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.441887485+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4407647}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.793865179+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4393}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4325755, "arrival": 1711329886.7026596}, "models": {"yolo": {"arrival": 1711329886.436055, "serving": 1711329886.662524}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.493950322+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4129264}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4326348, "arrival": 1711329887.2402651}, "models": {"yolo": {"arrival": 1711329886.9631321, "serving": 1711329887.2050898}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.440486, "arrival": 1711329888.4000576}, "models": {"yolo": {"arrival": 1711329888.1581888, "serving": 1711329888.3772707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.408878048+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4130015}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.428199203+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4402175}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.793732349+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4324539}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.439223, "arrival": 1711329880.3167777}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4136257, "arrival": 1711329880.3096533}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4408007, "arrival": 1711329880.3237739}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.686675469+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4387357}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.779153958+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4303257}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4406939, "arrival": 1711329887.0154347}, "models": {"yolo": {"arrival": 1711329886.672621, "serving": 1711329886.9620326}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4396753, "arrival": 1711329880.3188596}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4399495, "arrival": 1711329888.3937447}, "models": {"yolo": {"arrival": 1711329888.1581888, "serving": 1711329888.3772707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4392045, "arrival": 1711329888.1898944}, "models": {"yolo": {"arrival": 1711329887.9180605, "serving": 1711329888.1437252}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.686697972+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4391289}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4138713, "arrival": 1711329887.9375896}, "models": {"yolo": {"arrival": 1711329887.676527, "serving": 1711329887.9052076}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4386604, "arrival": 1711329880.315393}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.794021744+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.440326}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.686629+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4324334}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4301462, "arrival": 1711329887.2527857}, "models": {"yolo": {"arrival": 1711329886.9746883, "serving": 1711329887.20963}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4396203, "arrival": 1711329887.4655616}, "models": {"yolo": {"arrival": 1711329887.2139206, "serving": 1711329887.440208}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4399858, "arrival": 1711329887.4800847}, "models": {"yolo": {"arrival": 1711329887.2196586, "serving": 1711329887.440916}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.410642505+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4326563}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.436833754+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.44062}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.433757353+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.440364}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4408374, "arrival": 1711329887.242907}, "models": {"yolo": {"arrival": 1711329886.9698284, "serving": 1711329887.206755}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.692882582+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4403079}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4390697, "arrival": 1711329880.3164387}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.413387, "arrival": 1711329887.2509236}, "models": {"yolo": {"arrival": 1711329886.9746883, "serving": 1711329887.20963}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4402359, "arrival": 1711329888.3970754}, "models": {"yolo": {"arrival": 1711329888.1581888, "serving": 1711329888.3772707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4303555, "arrival": 1711329887.2391527}, "models": {"yolo": {"arrival": 1711329886.9631321, "serving": 1711329887.2050898}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4391098, "arrival": 1711329886.7036774}, "models": {"yolo": {"arrival": 1711329886.436055, "serving": 1711329886.662524}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4401298, "arrival": 1711329887.7068086}, "models": {"yolo": {"arrival": 1711329887.4509058, "serving": 1711329887.6679254}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.440747, "arrival": 1711329887.697747}, "models": {"yolo": {"arrival": 1711329887.4537232, "serving": 1711329887.6651714}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.440059, "arrival": 1711329887.4678385}, "models": {"yolo": {"arrival": 1711329887.2139206, "serving": 1711329887.440208}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.440291, "arrival": 1711329887.0146704}, "models": {"yolo": {"arrival": 1711329886.672621, "serving": 1711329886.9620326}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.793978076+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.44004}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.413037, "arrival": 1711329887.936483}, "models": {"yolo": {"arrival": 1711329887.676527, "serving": 1711329887.9052076}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.410701074+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4387975}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.692905322+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4405627}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4135914, "arrival": 1711329887.9372535}, "models": {"yolo": {"arrival": 1711329887.676527, "serving": 1711329887.9052076}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4399126, "arrival": 1711329887.467207}, "models": {"yolo": {"arrival": 1711329887.2139206, "serving": 1711329887.440208}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.687960095+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.439583}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4135227, "arrival": 1711329887.2307646}, "models": {"yolo": {"arrival": 1711329886.9631321, "serving": 1711329887.2050898}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.408933795+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.413557}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4323654, "arrival": 1711329880.3105395}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4387774, "arrival": 1711329887.2407255}, "models": {"yolo": {"arrival": 1711329886.9631321, "serving": 1711329887.2050898}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4397132, "arrival": 1711329887.0064766}, "models": {"yolo": {"arrival": 1711329886.672621, "serving": 1711329886.9620326}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4385667, "arrival": 1711329888.1841855}, "models": {"yolo": {"arrival": 1711329887.9180605, "serving": 1711329888.1437252}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4132483, "arrival": 1711329887.0027986}, "models": {"yolo": {"arrival": 1711329886.6644616, "serving": 1711329886.9545953}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.416522196+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4394884}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4395454, "arrival": 1711329887.4779336}, "models": {"yolo": {"arrival": 1711329887.2196586, "serving": 1711329887.440916}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4406753, "arrival": 1711329887.7082982}, "models": {"yolo": {"arrival": 1711329887.4509058, "serving": 1711329887.6679254}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4408188, "arrival": 1711329887.708812}, "models": {"yolo": {"arrival": 1711329887.4509058, "serving": 1711329887.6679254}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.493984631+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.413214}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.421539218+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4396393}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.413421, "arrival": 1711329886.4711366}, "models": {"yolo": {"arrival": 1711329886.0842993, "serving": 1711329886.422888}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.439507, "arrival": 1711329888.1905677}, "models": {"yolo": {"arrival": 1711329887.9180605, "serving": 1711329888.1437252}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.440602, "arrival": 1711329887.6967075}, "models": {"yolo": {"arrival": 1711329887.4537232, "serving": 1711329887.6651714}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.692829952+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.440022}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.413907, "arrival": 1711329880.3100796}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4750328, "arrival": 1711329888.4028745}, "models": {"yolo": {"arrival": 1711329888.1581888, "serving": 1711329888.3772707}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.794066144+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4407294}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4395645, "arrival": 1711329887.0058599}, "models": {"yolo": {"arrival": 1711329886.672621, "serving": 1711329886.9620326}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4133518, "arrival": 1711329879.2601237}, "models": {"yolo": {"arrival": 1711329878.447979, "serving": 1711329879.1876383}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4393191, "arrival": 1711329887.4582233}, "models": {"yolo": {"arrival": 1711329887.2139206, "serving": 1711329887.440208}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4325335, "arrival": 1711329880.3108954}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.692772741+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4398766}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.68650944+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4134543}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4392624, "arrival": 1711329886.707829}, "models": {"yolo": {"arrival": 1711329886.436055, "serving": 1711329886.662524}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.43869, "arrival": 1711329887.4736857}, "models": {"yolo": {"arrival": 1711329887.2196586, "serving": 1711329887.440916}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4391656, "arrival": 1711329887.4575658}, "models": {"yolo": {"arrival": 1711329887.2139206, "serving": 1711329887.440208}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.427238364+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4400764}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4407823, "arrival": 1711329888.4012063}, "models": {"yolo": {"arrival": 1711329888.1581888, "serving": 1711329888.3772707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4401124, "arrival": 1711329880.3209422}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.793929131+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.43975}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4390912, "arrival": 1711329887.474299}, "models": {"yolo": {"arrival": 1711329887.2196586, "serving": 1711329887.440916}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.440638, "arrival": 1711329888.4006646}, "models": {"yolo": {"arrival": 1711329888.1581888, "serving": 1711329888.3772707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4394147, "arrival": 1711329886.708765}, "models": {"yolo": {"arrival": 1711329886.436055, "serving": 1711329886.662524}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4401476, "arrival": 1711329887.0129902}, "models": {"yolo": {"arrival": 1711329886.672621, "serving": 1711329886.9620326}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4396574, "arrival": 1711329888.190909}, "models": {"yolo": {"arrival": 1711329887.9180605, "serving": 1711329888.1437252}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.439244, "arrival": 1711329887.474843}, "models": {"yolo": {"arrival": 1711329887.2196586, "serving": 1711329887.440916}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4401991, "arrival": 1711329887.4795778}, "models": {"yolo": {"arrival": 1711329887.2139206, "serving": 1711329887.440208}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.687981605+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4397311}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4398403, "arrival": 1711329887.4790726}, "models": {"yolo": {"arrival": 1711329887.2196586, "serving": 1711329887.440916}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.793908424+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.439602}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4393551, "arrival": 1711329888.1902194}, "models": {"yolo": {"arrival": 1711329887.9180605, "serving": 1711329888.1437252}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.432513, "arrival": 1711329888.183657}, "models": {"yolo": {"arrival": 1711329887.9180605, "serving": 1711329888.1437252}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.794000157+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4401817}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.687902206+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4392812}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.411041297+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4391856}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.40898011+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4322662}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4397678, "arrival": 1711329887.4665027}, "models": {"yolo": {"arrival": 1711329887.2139206, "serving": 1711329887.440208}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4402728, "arrival": 1711329887.7072809}, "models": {"yolo": {"arrival": 1711329887.4509058, "serving": 1711329887.6679254}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.415668404+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4393375}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4138007, "arrival": 1711329887.2382588}, "models": {"yolo": {"arrival": 1711329886.9631321, "serving": 1711329887.2050898}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.793841646+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4391472}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.432391, "arrival": 1711329887.253088}, "models": {"yolo": {"arrival": 1711329886.9746883, "serving": 1711329887.20963}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.793791961+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4326158}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.439525, "arrival": 1711329880.317442}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.692857148+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4401648}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4400039, "arrival": 1711329887.008645}, "models": {"yolo": {"arrival": 1711329886.672621, "serving": 1711329886.9620326}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.43947, "arrival": 1711329887.458883}, "models": {"yolo": {"arrival": 1711329887.2139206, "serving": 1711329887.440208}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4323392, "arrival": 1711329888.183046}, "models": {"yolo": {"arrival": 1711329887.9180605, "serving": 1711329888.1437252}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4396958, "arrival": 1711329887.4785476}, "models": {"yolo": {"arrival": 1711329887.2196586, "serving": 1711329887.440916}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.440254, "arrival": 1711329880.3211873}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.439048, "arrival": 1711329888.1846995}, "models": {"yolo": {"arrival": 1711329887.9180605, "serving": 1711329888.1437252}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4406571, "arrival": 1711329880.3234313}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.794044791+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4405825}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4399683, "arrival": 1711329880.3206127}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 1.84 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:38.692929288+00:00\"}\"\n>", "times": {"request": {"sending": 1711329878.4407113}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:38.793818503+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4387567}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.409002987+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329878.4324932}}, "outputs": []}, {"times": {"request": {"sending": 1711329878.4387136, "arrival": 1711329886.7031665}, "models": {"yolo": {"arrival": 1711329886.436055, "serving": 1711329886.662524}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329878.4398587, "arrival": 1711329887.0081818}, "models": {"yolo": {"arrival": 1711329886.672621, "serving": 1711329886.9620326}}}, "model_name": "yolo", "outputs": []}], [{"times": {"request": {"sending": 1711329879.4657059, "arrival": 1711329888.7807648}, "models": {"yolo": {"arrival": 1711329888.6331873, "serving": 1711329888.752617}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.459921431+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4214458}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4768286, "arrival": 1711329888.782813}, "models": {"yolo": {"arrival": 1711329888.6331873, "serving": 1711329888.752617}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4210987, "arrival": 1711329887.7092772}, "models": {"yolo": {"arrival": 1711329887.4509058, "serving": 1711329887.6679254}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.63827447+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4216635}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.640417404+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4220583}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4657533, "arrival": 1711329888.4655802}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657197439+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.422791}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4219723, "arrival": 1711329887.2441778}, "models": {"yolo": {"arrival": 1711329886.9698284, "serving": 1711329887.206755}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.527380256+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4510338}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4217498, "arrival": 1711329887.7001095}, "models": {"yolo": {"arrival": 1711329887.4537232, "serving": 1711329887.6651714}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4244828, "arrival": 1711329887.9301972}, "models": {"yolo": {"arrival": 1711329887.678186, "serving": 1711329887.9016068}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.446485, "arrival": 1711329888.770139}, "models": {"yolo": {"arrival": 1711329888.6331873, "serving": 1711329888.752617}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4226046, "arrival": 1711329888.5031288}, "models": {"yolo": {"arrival": 1711329888.390053, "serving": 1711329888.489625}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4215324, "arrival": 1711329880.324488}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.640337191+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4213598}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4226484, "arrival": 1711329880.3263164}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4225118, "arrival": 1711329887.918523}, "models": {"yolo": {"arrival": 1711329887.678186, "serving": 1711329887.9016068}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.638304664+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4220152}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4212663, "arrival": 1711329887.2433515}, "models": {"yolo": {"arrival": 1711329886.9698284, "serving": 1711329887.206755}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:40.685486815+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4656582}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4236302, "arrival": 1711329887.9243097}, "models": {"yolo": {"arrival": 1711329887.678186, "serving": 1711329887.9016068}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4227436, "arrival": 1711329887.2524056}, "models": {"yolo": {"arrival": 1711329886.9698284, "serving": 1711329887.206755}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4233997, "arrival": 1711329881.819792}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.451097, "arrival": 1711329888.4618523}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.638198891+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.421315}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657076548+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4235866}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.657539303+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4464364}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4214032, "arrival": 1711329887.6993535}, "models": {"yolo": {"arrival": 1711329887.4537232, "serving": 1711329887.6651714}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4237616, "arrival": 1711329881.820345}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.468193133+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4217925}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4218843, "arrival": 1711329880.3256648}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.451082, "arrival": 1711329888.1887367}, "models": {"yolo": {"arrival": 1711329887.9074543, "serving": 1711329888.1393538}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.65724307+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4235375}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.451145, "arrival": 1711329888.1714892}, "models": {"yolo": {"arrival": 1711329887.9121192, "serving": 1711329888.1395671}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.424176, "arrival": 1711329887.914373}, "models": {"yolo": {"arrival": 1711329887.677639, "serving": 1711329887.8974257}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4654548, "arrival": 1711329881.83332}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4232621, "arrival": 1711329887.919945}, "models": {"yolo": {"arrival": 1711329887.678186, "serving": 1711329887.9016068}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.422197, "arrival": 1711329888.5023463}, "models": {"yolo": {"arrival": 1711329888.390053, "serving": 1711329888.489625}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4216204, "arrival": 1711329887.2437654}, "models": {"yolo": {"arrival": 1711329886.9698284, "serving": 1711329887.206755}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4231212, "arrival": 1711329888.4401283}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657172661+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4223862}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4245603, "arrival": 1711329881.8271446}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.422342, "arrival": 1711329887.2520418}, "models": {"yolo": {"arrival": 1711329886.9698284, "serving": 1711329887.206755}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4214895, "arrival": 1711329888.4034567}, "models": {"yolo": {"arrival": 1711329888.1581888, "serving": 1711329888.3772707}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.421841, "arrival": 1711329888.5002663}, "models": {"yolo": {"arrival": 1711329888.390053, "serving": 1711329888.489625}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.450836, "arrival": 1711329888.4591064}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4245348, "arrival": 1711329888.5170212}, "models": {"yolo": {"arrival": 1711329888.390053, "serving": 1711329888.489625}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4234934, "arrival": 1711329888.441145}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4242194, "arrival": 1711329888.4502907}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657314813+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4246347}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4465358, "arrival": 1711329888.4583445}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4215772, "arrival": 1711329887.709712}, "models": {"yolo": {"arrival": 1711329887.4509058, "serving": 1711329887.6679254}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4245863, "arrival": 1711329887.9176655}, "models": {"yolo": {"arrival": 1711329887.677639, "serving": 1711329887.8974257}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4507766, "arrival": 1711329888.7710533}, "models": {"yolo": {"arrival": 1711329888.6331873, "serving": 1711329888.752617}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4656734, "arrival": 1711329888.1774006}, "models": {"yolo": {"arrival": 1711329887.9121192, "serving": 1711329888.1395671}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4657369, "arrival": 1711329888.963566}, "models": {"yolo": {"arrival": 1711329888.9089723, "serving": 1711329888.9528775}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.423718, "arrival": 1711329888.51571}, "models": {"yolo": {"arrival": 1711329888.390053, "serving": 1711329888.489625}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.422293, "arrival": 1711329887.9036903}, "models": {"yolo": {"arrival": 1711329887.677639, "serving": 1711329887.8974257}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.498574419+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4247086}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.450889, "arrival": 1711329888.1605358}, "models": {"yolo": {"arrival": 1711329887.9121192, "serving": 1711329888.1395671}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:40.685499037+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4657853}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4240804, "arrival": 1711329888.5163834}, "models": {"yolo": {"arrival": 1711329888.390053, "serving": 1711329888.489625}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4653351, "arrival": 1711329885.8807676}, "models": {"yolo": {"arrival": 1711329885.6857216, "serving": 1711329885.8597426}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4509711, "arrival": 1711329888.4612525}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657496214+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.465509}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657475627+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4512382}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4512696, "arrival": 1711329888.1760137}, "models": {"yolo": {"arrival": 1711329887.9121192, "serving": 1711329888.1395671}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4219286, "arrival": 1711329887.7100992}, "models": {"yolo": {"arrival": 1711329887.4509058, "serving": 1711329887.6679254}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4221017, "arrival": 1711329887.7007644}, "models": {"yolo": {"arrival": 1711329887.4537232, "serving": 1711329887.6651714}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.47367211+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4229336}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657290767+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.424424}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4464538, "arrival": 1711329888.1570907}, "models": {"yolo": {"arrival": 1711329887.9121192, "serving": 1711329888.1395671}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657266888+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.423902}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4226978, "arrival": 1711329887.9058485}, "models": {"yolo": {"arrival": 1711329887.677639, "serving": 1711329887.8974257}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.657100927+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4239454}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.657124266+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4244556}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.507295378+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4462883}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.491688569+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4240324}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657022044+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.422839}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4230766, "arrival": 1711329887.9067252}, "models": {"yolo": {"arrival": 1711329887.677639, "serving": 1711329887.8974257}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4230275, "arrival": 1711329881.8190777}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4511907, "arrival": 1711329881.8329408}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.656958996+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.422464}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4657211, "arrival": 1711329881.8367229}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657584035+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4508712}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.657149332+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4246595}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.657452476+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4511123}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.657649234+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4512553}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657605344+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4510021}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4655585, "arrival": 1711329885.8815007}, "models": {"yolo": {"arrival": 1711329885.6857216, "serving": 1711329885.8597426}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.65738547+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4465525}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.451206, "arrival": 1711329888.1891937}, "models": {"yolo": {"arrival": 1711329887.9074543, "serving": 1711329888.1393538}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.424125, "arrival": 1711329881.8208776}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4654741, "arrival": 1711329888.189531}, "models": {"yolo": {"arrival": 1711329887.9074543, "serving": 1711329888.1393538}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.64039098+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4217067}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.65756232+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4465685}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.423989, "arrival": 1711329887.9253185}, "models": {"yolo": {"arrival": 1711329887.678186, "serving": 1711329887.9016068}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.465641, "arrival": 1711329880.850105}, "models": {"yolo": {"arrival": 1711329880.6445153, "serving": 1711329880.8425705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4509208, "arrival": 1711329888.7720075}, "models": {"yolo": {"arrival": 1711329888.6331873, "serving": 1711329888.752617}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.6573376+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4462154}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4246106, "arrival": 1711329888.4528248}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4462643, "arrival": 1711329887.9317575}, "models": {"yolo": {"arrival": 1711329887.678186, "serving": 1711329887.9016068}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.423858, "arrival": 1711329888.4419904}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4509556, "arrival": 1711329888.18832}, "models": {"yolo": {"arrival": 1711329887.9074543, "serving": 1711329888.1393538}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4460492, "arrival": 1711329888.5176446}, "models": {"yolo": {"arrival": 1711329888.390053, "serving": 1711329888.489625}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.46831736+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4221444}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4463642, "arrival": 1711329888.1788366}, "models": {"yolo": {"arrival": 1711329887.9074543, "serving": 1711329888.1393538}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.487938917+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4236739}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4246843, "arrival": 1711329887.931056}, "models": {"yolo": {"arrival": 1711329887.678186, "serving": 1711329887.9016068}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4654915, "arrival": 1711329888.464462}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.465816, "arrival": 1711329885.8828592}, "models": {"yolo": {"arrival": 1711329885.6857216, "serving": 1711329885.8597426}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657626695+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4511287}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4769137, "arrival": 1711329881.8370001}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4233549, "arrival": 1711329888.514978}, "models": {"yolo": {"arrival": 1711329888.390053, "serving": 1711329888.489625}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4461644, "arrival": 1711329888.1780016}, "models": {"yolo": {"arrival": 1711329887.9074543, "serving": 1711329888.1393538}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.422884, "arrival": 1711329887.9192562}, "models": {"yolo": {"arrival": 1711329887.678186, "serving": 1711329887.9016068}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.657220469+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4231694}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.46938652+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.422556}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657050253+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4232137}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.482051179+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4233084}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.422244, "arrival": 1711329880.3260062}, "models": {"yolo": {"arrival": 1711329879.4719148, "serving": 1711329880.2698314}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4234498, "arrival": 1711329887.912798}, "models": {"yolo": {"arrival": 1711329887.677639, "serving": 1711329887.8974257}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.512083668+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4464688}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4657698, "arrival": 1711329880.8511}, "models": {"yolo": {"arrival": 1711329880.6445153, "serving": 1711329880.8425705}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4229784, "arrival": 1711329888.511808}, "models": {"yolo": {"arrival": 1711329888.390053, "serving": 1711329888.489625}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:45.494288879+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4245098}}, "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657517599+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 52.00 MiB (GPU 0; 23.64 GiB total capacity; 2.48 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4462411}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4511604, "arrival": 1711329885.877936}, "models": {"yolo": {"arrival": 1711329885.6857216, "serving": 1711329885.8597426}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4463134, "arrival": 1711329888.7671077}, "models": {"yolo": {"arrival": 1711329888.6331873, "serving": 1711329888.752617}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4238126, "arrival": 1711329887.9136992}, "models": {"yolo": {"arrival": 1711329887.677639, "serving": 1711329887.8974257}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4511752, "arrival": 1711329888.778201}, "models": {"yolo": {"arrival": 1711329888.6331873, "serving": 1711329888.752617}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4507976, "arrival": 1711329881.8288648}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4510176, "arrival": 1711329888.1705384}, "models": {"yolo": {"arrival": 1711329887.9121192, "serving": 1711329888.1395671}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.46569, "arrival": 1711329885.8822105}, "models": {"yolo": {"arrival": 1711329885.6857216, "serving": 1711329885.8597426}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.451221, "arrival": 1711329888.4623778}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4655766, "arrival": 1711329888.7798562}, "models": {"yolo": {"arrival": 1711329888.6331873, "serving": 1711329888.752617}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4465835, "arrival": 1711329888.1594853}, "models": {"yolo": {"arrival": 1711329887.9121192, "serving": 1711329888.1395671}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4510646, "arrival": 1711329881.832457}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.516286524+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.450707}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4656074, "arrival": 1711329888.9620442}, "models": {"yolo": {"arrival": 1711329888.9089723, "serving": 1711329888.9528775}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4654322, "arrival": 1711329888.7789123}, "models": {"yolo": {"arrival": 1711329888.6331873, "serving": 1711329888.752617}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.64 GiB total capacity; 2.47 GiB already allocated; 20.44 MiB free; 2.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:40.685449841+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4655252}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4461348, "arrival": 1711329881.8276734}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4655917, "arrival": 1711329881.8359413}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4461904, "arrival": 1711329888.453537}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4655423, "arrival": 1711329888.1767938}, "models": {"yolo": {"arrival": 1711329887.9121192, "serving": 1711329888.1395671}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:39.65736149+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4464195}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4658005, "arrival": 1711329888.95905}, "models": {"yolo": {"arrival": 1711329888.9133854, "serving": 1711329888.9528587}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.45105, "arrival": 1711329888.7773824}, "models": {"yolo": {"arrival": 1711329888.6331873, "serving": 1711329888.752617}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657408329+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4508538}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4465015, "arrival": 1711329881.8284585}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4508185, "arrival": 1711329888.1804552}, "models": {"yolo": {"arrival": 1711329887.9074543, "serving": 1711329888.1393538}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {grpc_message:\"CUDA out of memory. Tried to allocate 814.00 MiB (GPU 0; 23.64 GiB total capacity; 2.00 GiB already allocated; 472.44 MiB free; 2.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\", grpc_status:13, created_time:\"2024-03-25T01:24:45.523092795+00:00\"}\"\n>", "times": {"request": {"sending": 1711329879.4509046}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4656243, "arrival": 1711329888.465038}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}, {"failed": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:127.0.0.1:32000 {created_time:\"2024-03-25T01:24:39.657429693+00:00\", grpc_status:13, grpc_message:\"CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 23.64 GiB total capacity; 1.74 GiB already allocated; 20.44 MiB free; 2.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\"}\"\n>", "times": {"request": {"sending": 1711329879.4509861}}, "outputs": []}, {"times": {"request": {"sending": 1711329879.4465194, "arrival": 1711329888.1796784}, "models": {"yolo": {"arrival": 1711329887.9074543, "serving": 1711329888.1393538}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.446338, "arrival": 1711329881.8280702}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4509366, "arrival": 1711329881.8292367}, "models": {"yolo": {"arrival": 1711329881.2166321, "serving": 1711329881.7929454}}}, "model_name": "yolo", "outputs": []}, {"times": {"request": {"sending": 1711329879.4463956, "arrival": 1711329888.4542665}, "models": {"yolo": {"arrival": 1711329888.1577098, "serving": 1711329888.426908}}}, "model_name": "yolo", "outputs": []}]], "start_time_experiment": 1711329820.393225, "end_time_experiment": 1711329888.9670722}